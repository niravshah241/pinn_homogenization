{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Dacbu4C9T8"
      },
      "source": [
        "Import the required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0VU67p7DB00"
      },
      "outputs": [],
      "source": [
        "import torch # Pytorch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyj3JFzgDWLY"
      },
      "source": [
        "Select the device for performing the computations. ```torch.cuda.is_available()``` is ```True``` if graphic card supporting ```CUDA``` is available. If $n$ graphics cards are available, graphic card $m$, $0 \\leq m \\leq (n-1)$, can be used by \"cuda:$m$\".\n",
        "Select the default data type for ```torch.Tensor``` such as ```torch.float32``` (32-bit floating point), ```torch.float64``` (64-bit floating point) or ```torch.float16``` (16-bit floating point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQrx_eOc0Mcq"
      },
      "outputs": [],
      "source": [
        "# Select device and Data type\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "dtype = torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcWWqZv9IO4z"
      },
      "source": [
        "**Problem setup**\n",
        "\n",
        "Input, output and other matrices/vectors are defined in terms of ```torch.Tensor```. A ```torch.Tensor``` can be allocated to CPU or to GPU by specifying ```device``` argument. Data type can be specified by specifying argument ```dtype```. ```.numpy()``` returns the tensor as NumPy ```ndarray```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLBS-aqkeL3M",
        "outputId": "cec43112-a11e-4c9a-94c8-dff38133877e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input - Shape: torch.Size([1000, 2]), Device: cuda:0, Data type: torch.float32\n",
            "Output - Shape: torch.Size([1000, 3]), Device cuda:0, Data type: torch.float32\n",
            "Input in NumPy: \n",
            " [[ 0.26556277 -0.19017243]\n",
            " [ 0.54225695  0.813377  ]\n",
            " [-0.65788615 -0.86627245]\n",
            " ...\n",
            " [ 0.3622831  -0.5096818 ]\n",
            " [-0.5690993   0.10198534]\n",
            " [ 0.10045409 -0.17743206]]\n"
          ]
        }
      ],
      "source": [
        "num_samples = 1000 # Number of samples\n",
        "sample_dim = 2 # Input dimensions\n",
        "label_dim = 3 # Output dimensions\n",
        "hidden_dim = 10 # Hidden layer dimensions / Number of neurons\n",
        "\n",
        "# Input parameters\n",
        "x = torch.rand(num_samples, sample_dim).to(device=device).to(dtype=dtype) * 2 - 1\n",
        "\n",
        "# True Weight and Bias parameter\n",
        "w1 = torch.rand(label_dim, sample_dim).to(device=device).to(dtype=dtype)\n",
        "b1 = torch.rand(label_dim, 1).to(device=device).to(dtype=dtype)\n",
        "\n",
        "# Output values\n",
        "y = torch.sin(torch.matmul(w1, x.T) + b1).T\n",
        "\n",
        "# w1 = torch.rand(hidden_dim, sample_dim).to(device=device)\n",
        "# b1 = torch.rand(hidden_dim, 1).to(device=device)\n",
        "\n",
        "# w2 = torch.rand(label_dim, hidden_dim).to(device=device)\n",
        "# b2 = torch.rand(label_dim, 1).to(device=device)\n",
        "\n",
        "# y = torch.sin(torch.matmul(w2, torch.sin(torch.matmul(w1, x.T) + b1)) + b2).T\n",
        "\n",
        "print(f\"Input - Shape: {x.shape}, Device: {x.device}, Data type: {x.dtype}\")\n",
        "print(f\"Output - Shape: {y.shape}, Device {y.device}, Data type: {y.dtype}\")\n",
        "if x.device == \"cpu\":\n",
        "  print(f\"Input in NumPy: \\n {x.numpy()}\")\n",
        "else:\n",
        "  print(f\"Input in NumPy: \\n {x.cpu().numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhfrhGryRX3N"
      },
      "source": [
        "We now define the Artificial Neural Network (ANN). ```torch.nn.Module``` is base class for ANN modules. Class ```net``` should should implement ```forward``` method, which implements forward pass of the ANN. Weights and biases of the ANN can be read from ```.parameters()```. ANN can be moved to a device by specifying ```device``` and data type of weights and biases can be specified by ```dtype```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWxVIo2ZlR5V",
        "outputId": "ce124366-ffe0-48f0-ea72-0a417a508382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN parameter Shape: torch.Size([3, 2]), Device cuda:0, Data type: torch.float32\n",
            "NN parameter Shape: torch.Size([3]), Device cuda:0, Data type: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Artificial Neural Network\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, sample_dim, label_dim, activation_function):\n",
        "    super().__init__()\n",
        "    self.Linear1 = torch.nn.Linear(sample_dim, label_dim)\n",
        "    self.activation_function = activation_function\n",
        "\n",
        "  def forward(self, x_para):\n",
        "    output_val = self.activation_function(self.Linear1(x_para))\n",
        "    return output_val\n",
        "\n",
        "net = Net(sample_dim, label_dim, torch.sin).to(device=device).to(dtype=dtype)\n",
        "\n",
        "# class Net(torch.nn.Module):\n",
        "#   def __init__(self, sample_dim, hidden_dim, label_dim, activation_function):\n",
        "#     super().__init__()\n",
        "#     self.Linear1 = torch.nn.Linear(sample_dim, hidden_dim)\n",
        "#     self.Linear2 = torch.nn.Linear(hidden_dim, label_dim)\n",
        "#     self.activation_function = activation_function\n",
        "\n",
        "#   def forward(self, x_para):\n",
        "#     hidden_val = self.activation_function(self.Linear1(x_para))\n",
        "#     output_val = self.activation_function(self.Linear2((hidden_val)))\n",
        "#     return output_val\n",
        "\n",
        "# net = Net(sample_dim, hidden_dim, label_dim, torch.sin).to(device=device)\n",
        "\n",
        "for param in net.parameters():\n",
        "  print(f\"NN parameter Shape: {param.shape}, Device {param.device}, Data type: {param.dtype}\")\n",
        "\n",
        "y_pred = net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El8pBDQvTB_T"
      },
      "source": [
        "**Data processing**\n",
        "\n",
        "```Dataset``` stores the input-output dataset. In the below eaxmple, we implement custom dataset ```CustomDataset```, which must implement three functions: ```__init__```,  ```__len__``` and ```__getitem__```. ``` __len__``` function returns the number of samples in our dataset. ```__getitem__``` function loads and returns a sample from the dataset at the given index ```index```. Additionally, some methods ```some_transformation``` can be implemented, for example for scaling of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9VZuI1VpuI8"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, input_data, output_data):\n",
        "    self.input_data = self.some_transformation(input_data)\n",
        "    self.output_data = self.some_transformation(output_data)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_data.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_data[index, :], self.output_data[index, :]\n",
        "\n",
        "  def some_transformation(self, input_data):\n",
        "    return input_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYw6PbVfZfSz"
      },
      "source": [
        "```DataLoader``` wraps an iterable around ```Dataset``` for easy access. During ANN training, data can be passed in minibatches with size specified by ```batch_size```, can be shuffled at every epoch by ```shuffle=True``` and speed up data retrieval.\n",
        "In the below example, we use ~80% of total data for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVuFL7zpx7pV",
        "outputId": "aaaaa014-81c7-40fb-c2dd-b05318b02a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 800\n",
            "Number of training batches: 2\n",
            "Training: Batch 1, Batch size (Input, Output) ((torch.Size([400, 2]), torch.Size([400, 3])))\n",
            "Training: Batch 2, Batch size (Input, Output) ((torch.Size([400, 2]), torch.Size([400, 3])))\n"
          ]
        }
      ],
      "source": [
        "num_train_data = int(0.8 * x.shape[0]) # 80% data for training\n",
        "batch_size_train = 400 # Training data batch size\n",
        "\n",
        "customDataset_train = CustomDataset(x[:num_train_data, :], y[:num_train_data, :])\n",
        "customDataLoader_train = torch.utils.data.DataLoader(customDataset_train, batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "print(f\"Number of training samples: {len(customDataLoader_train.dataset)}\")\n",
        "print(f\"Number of training batches: {len(customDataLoader_train)}\")\n",
        "\n",
        "for batch, (input_batch, output_batch) in enumerate(customDataLoader_train):\n",
        "  print(f\"Training: Batch {batch+1}, Batch size (Input, Output) ({input_batch.shape, output_batch.shape})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAGUUCBrarhB"
      },
      "source": [
        "Remaining ~20% data will be used for validation. Notice that validation data is passed in a single batch as no weight/bias update takes place during validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H2qY9VGyBLv",
        "outputId": "dfc9817d-b465-4d56-d57d-3315c7937185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation samples: 200\n",
            "Number of validation batches: 1\n",
            "Validation: Batch 1, Batch size (Input, Output) ((torch.Size([200, 2]), torch.Size([200, 3])))\n"
          ]
        }
      ],
      "source": [
        "num_val_data = x.shape[0] - num_train_data # Remaining data for validation\n",
        "batch_size_val = num_val_data # Validation data batch size\n",
        "customDataset_val = CustomDataset(x[num_train_data:, :], y[num_train_data:, :])\n",
        "customDataLoader_val = torch.utils.data.DataLoader(customDataset_val, batch_size=batch_size_val, shuffle=False)\n",
        "\n",
        "print(f\"Number of validation samples: {len(customDataLoader_val.dataset)}\")\n",
        "print(f\"Number of validation batches: {len(customDataLoader_val)}\")\n",
        "\n",
        "for batch, (input_batch, output_batch) in enumerate(customDataLoader_val):\n",
        "  print(f\"Validation: Batch {batch+1}, Batch size (Input, Output) ({input_batch.shape, output_batch.shape})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmhfK7iBbCVn"
      },
      "source": [
        "**Training and Validation**\n",
        "\n",
        "Training process is initialised after data processing. The stopping criteria used in below implementation is ```max_epochs``` and early stopping. The optimisation algorithm ```Adam``` is specified for ```optimiser```. The error between true value and ANN prediction is specified as ```MSELoss```.\n",
        "\n",
        "During training, first minibatch of data is loaded, forward pass of the ANN is performed, the error between ANN prediction and True value is measured, weight and bias are updated by backpropagation. ```optimiser.zero_grad()``` sets gradients to zero to avoid accumulating gradient from the previous backward passes.\n",
        "\n",
        "During validation, first validation data is loaded, forward pass of the ANN is performed and error between ANN prediction and True value is measured. We save, ```best_model``` , the ANN parameters with minimum validation loss. If the validation loss increases in subsequent epoch, the training is stopped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2M4u_kNmjuVj",
        "outputId": "a719ea27-6f6a-450b-c857-917315b8c144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training: Batch 2/2, Loss: 8.638636791147292e-05\n",
            "Validation: Batch 1/1, Loss: 8.300418267026544e-05\n",
            "Epoch: 1479/10000\n",
            "Training: Batch 1/2, Loss: 8.551470818929374e-05\n",
            "Training: Batch 2/2, Loss: 8.635553240310401e-05\n",
            "Validation: Batch 1/1, Loss: 8.234253618866205e-05\n",
            "Epoch: 1480/10000\n",
            "Training: Batch 1/2, Loss: 8.37205516290851e-05\n",
            "Training: Batch 2/2, Loss: 8.678393351146951e-05\n",
            "Validation: Batch 1/1, Loss: 8.168554631993175e-05\n",
            "Epoch: 1481/10000\n",
            "Training: Batch 1/2, Loss: 8.603231253800914e-05\n",
            "Training: Batch 2/2, Loss: 8.313208672916517e-05\n",
            "Validation: Batch 1/1, Loss: 8.103204891085625e-05\n",
            "Epoch: 1482/10000\n",
            "Training: Batch 1/2, Loss: 8.66686095832847e-05\n",
            "Training: Batch 2/2, Loss: 8.11583231552504e-05\n",
            "Validation: Batch 1/1, Loss: 8.03832954261452e-05\n",
            "Epoch: 1483/10000\n",
            "Training: Batch 1/2, Loss: 8.602825255366042e-05\n",
            "Training: Batch 2/2, Loss: 8.046616858337075e-05\n",
            "Validation: Batch 1/1, Loss: 7.973952597239986e-05\n",
            "Epoch: 1484/10000\n",
            "Training: Batch 1/2, Loss: 8.18480912130326e-05\n",
            "Training: Batch 2/2, Loss: 8.331210847245529e-05\n",
            "Validation: Batch 1/1, Loss: 7.910193380666897e-05\n",
            "Epoch: 1485/10000\n",
            "Training: Batch 1/2, Loss: 7.997071224963292e-05\n",
            "Training: Batch 2/2, Loss: 8.387520210817456e-05\n",
            "Validation: Batch 1/1, Loss: 7.847006054362282e-05\n",
            "Epoch: 1486/10000\n",
            "Training: Batch 1/2, Loss: 8.235806308221072e-05\n",
            "Training: Batch 2/2, Loss: 8.019868255360052e-05\n",
            "Validation: Batch 1/1, Loss: 7.78417379478924e-05\n",
            "Epoch: 1487/10000\n",
            "Training: Batch 1/2, Loss: 8.15392195363529e-05\n",
            "Training: Batch 2/2, Loss: 7.972565799718723e-05\n",
            "Validation: Batch 1/1, Loss: 7.721796282567084e-05\n",
            "Epoch: 1488/10000\n",
            "Training: Batch 1/2, Loss: 8.030747267184779e-05\n",
            "Training: Batch 2/2, Loss: 7.967284909682348e-05\n",
            "Validation: Batch 1/1, Loss: 7.659916445845738e-05\n",
            "Epoch: 1489/10000\n",
            "Training: Batch 1/2, Loss: 7.71764389355667e-05\n",
            "Training: Batch 2/2, Loss: 8.152370719471946e-05\n",
            "Validation: Batch 1/1, Loss: 7.5985946750734e-05\n",
            "Epoch: 1490/10000\n",
            "Training: Batch 1/2, Loss: 7.793230179231614e-05\n",
            "Training: Batch 2/2, Loss: 7.951247971504927e-05\n",
            "Validation: Batch 1/1, Loss: 7.537662895629182e-05\n",
            "Epoch: 1491/10000\n",
            "Training: Batch 1/2, Loss: 7.533417374361306e-05\n",
            "Training: Batch 2/2, Loss: 8.085169974947348e-05\n",
            "Validation: Batch 1/1, Loss: 7.477249164367095e-05\n",
            "Epoch: 1492/10000\n",
            "Training: Batch 1/2, Loss: 7.669912156416103e-05\n",
            "Training: Batch 2/2, Loss: 7.824962813174352e-05\n",
            "Validation: Batch 1/1, Loss: 7.417202141368762e-05\n",
            "Epoch: 1493/10000\n",
            "Training: Batch 1/2, Loss: 7.57563320803456e-05\n",
            "Training: Batch 2/2, Loss: 7.79580368543975e-05\n",
            "Validation: Batch 1/1, Loss: 7.357566937571391e-05\n",
            "Epoch: 1494/10000\n",
            "Training: Batch 1/2, Loss: 7.54271968617104e-05\n",
            "Training: Batch 2/2, Loss: 7.706274482188746e-05\n",
            "Validation: Batch 1/1, Loss: 7.298393757082522e-05\n",
            "Epoch: 1495/10000\n",
            "Training: Batch 1/2, Loss: 7.42646079743281e-05\n",
            "Training: Batch 2/2, Loss: 7.700671267230064e-05\n",
            "Validation: Batch 1/1, Loss: 7.239720434881747e-05\n",
            "Epoch: 1496/10000\n",
            "Training: Batch 1/2, Loss: 7.614009518874809e-05\n",
            "Training: Batch 2/2, Loss: 7.393294799840078e-05\n",
            "Validation: Batch 1/1, Loss: 7.181419641710818e-05\n",
            "Epoch: 1497/10000\n",
            "Training: Batch 1/2, Loss: 7.668929902138188e-05\n",
            "Training: Batch 2/2, Loss: 7.21883334335871e-05\n",
            "Validation: Batch 1/1, Loss: 7.123596878955141e-05\n",
            "Epoch: 1498/10000\n",
            "Training: Batch 1/2, Loss: 7.403246127068996e-05\n",
            "Training: Batch 2/2, Loss: 7.364772318396717e-05\n",
            "Validation: Batch 1/1, Loss: 7.066270336508751e-05\n",
            "Epoch: 1499/10000\n",
            "Training: Batch 1/2, Loss: 7.127414573915303e-05\n",
            "Training: Batch 2/2, Loss: 7.521927182096988e-05\n",
            "Validation: Batch 1/1, Loss: 7.009416003711522e-05\n",
            "Epoch: 1500/10000\n",
            "Training: Batch 1/2, Loss: 7.328737410716712e-05\n",
            "Training: Batch 2/2, Loss: 7.204507710412145e-05\n",
            "Validation: Batch 1/1, Loss: 6.95296548656188e-05\n",
            "Epoch: 1501/10000\n",
            "Training: Batch 1/2, Loss: 6.969026435399428e-05\n",
            "Training: Batch 2/2, Loss: 7.446906965924427e-05\n",
            "Validation: Batch 1/1, Loss: 6.896955164847896e-05\n",
            "Epoch: 1502/10000\n",
            "Training: Batch 1/2, Loss: 7.05586644471623e-05\n",
            "Training: Batch 2/2, Loss: 7.245384767884389e-05\n",
            "Validation: Batch 1/1, Loss: 6.84137485222891e-05\n",
            "Epoch: 1503/10000\n",
            "Training: Batch 1/2, Loss: 7.19926756573841e-05\n",
            "Training: Batch 2/2, Loss: 6.988068344071507e-05\n",
            "Validation: Batch 1/1, Loss: 6.786083395127207e-05\n",
            "Epoch: 1504/10000\n",
            "Training: Batch 1/2, Loss: 7.173965423135087e-05\n",
            "Training: Batch 2/2, Loss: 6.89974331180565e-05\n",
            "Validation: Batch 1/1, Loss: 6.731217581545934e-05\n",
            "Epoch: 1505/10000\n",
            "Training: Batch 1/2, Loss: 7.227024616440758e-05\n",
            "Training: Batch 2/2, Loss: 6.734191993018612e-05\n",
            "Validation: Batch 1/1, Loss: 6.676712655462325e-05\n",
            "Epoch: 1506/10000\n",
            "Training: Batch 1/2, Loss: 6.852785008959472e-05\n",
            "Training: Batch 2/2, Loss: 6.99523079674691e-05\n",
            "Validation: Batch 1/1, Loss: 6.622775981668383e-05\n",
            "Epoch: 1507/10000\n",
            "Training: Batch 1/2, Loss: 6.94397822371684e-05\n",
            "Training: Batch 2/2, Loss: 6.793501961510628e-05\n",
            "Validation: Batch 1/1, Loss: 6.56921838526614e-05\n",
            "Epoch: 1508/10000\n",
            "Training: Batch 1/2, Loss: 6.947004294488579e-05\n",
            "Training: Batch 2/2, Loss: 6.680338992737234e-05\n",
            "Validation: Batch 1/1, Loss: 6.516016583191231e-05\n",
            "Epoch: 1509/10000\n",
            "Training: Batch 1/2, Loss: 6.837391993030906e-05\n",
            "Training: Batch 2/2, Loss: 6.680298974970356e-05\n",
            "Validation: Batch 1/1, Loss: 6.463201134465635e-05\n",
            "Epoch: 1510/10000\n",
            "Training: Batch 1/2, Loss: 6.964558269828558e-05\n",
            "Training: Batch 2/2, Loss: 6.445045437430963e-05\n",
            "Validation: Batch 1/1, Loss: 6.410744390450418e-05\n",
            "Epoch: 1511/10000\n",
            "Training: Batch 1/2, Loss: 6.811234197812155e-05\n",
            "Training: Batch 2/2, Loss: 6.490254600066692e-05\n",
            "Validation: Batch 1/1, Loss: 6.358833343256265e-05\n",
            "Epoch: 1512/10000\n",
            "Training: Batch 1/2, Loss: 6.773602945031598e-05\n",
            "Training: Batch 2/2, Loss: 6.420807767426595e-05\n",
            "Validation: Batch 1/1, Loss: 6.307283183559775e-05\n",
            "Epoch: 1513/10000\n",
            "Training: Batch 1/2, Loss: 6.609058618778363e-05\n",
            "Training: Batch 2/2, Loss: 6.478760769823566e-05\n",
            "Validation: Batch 1/1, Loss: 6.256181950448081e-05\n",
            "Epoch: 1514/10000\n",
            "Training: Batch 1/2, Loss: 6.443593883886933e-05\n",
            "Training: Batch 2/2, Loss: 6.538612797157839e-05\n",
            "Validation: Batch 1/1, Loss: 6.205479439813644e-05\n",
            "Epoch: 1515/10000\n",
            "Training: Batch 1/2, Loss: 6.533705163747072e-05\n",
            "Training: Batch 2/2, Loss: 6.344430585158989e-05\n",
            "Validation: Batch 1/1, Loss: 6.155143637442961e-05\n",
            "Epoch: 1516/10000\n",
            "Training: Batch 1/2, Loss: 6.45498585072346e-05\n",
            "Training: Batch 2/2, Loss: 6.319347448879853e-05\n",
            "Validation: Batch 1/1, Loss: 6.105173088144511e-05\n",
            "Epoch: 1517/10000\n",
            "Training: Batch 1/2, Loss: 6.182376091601327e-05\n",
            "Training: Batch 2/2, Loss: 6.488281360361725e-05\n",
            "Validation: Batch 1/1, Loss: 6.0556991229532287e-05\n",
            "Epoch: 1518/10000\n",
            "Training: Batch 1/2, Loss: 6.185269012348726e-05\n",
            "Training: Batch 2/2, Loss: 6.383432628354058e-05\n",
            "Validation: Batch 1/1, Loss: 6.00646817474626e-05\n",
            "Epoch: 1519/10000\n",
            "Training: Batch 1/2, Loss: 6.516496796393767e-05\n",
            "Training: Batch 2/2, Loss: 5.952202991466038e-05\n",
            "Validation: Batch 1/1, Loss: 5.957505345577374e-05\n",
            "Epoch: 1520/10000\n",
            "Training: Batch 1/2, Loss: 6.315251084743068e-05\n",
            "Training: Batch 2/2, Loss: 6.052047683624551e-05\n",
            "Validation: Batch 1/1, Loss: 5.908943057875149e-05\n",
            "Epoch: 1521/10000\n",
            "Training: Batch 1/2, Loss: 6.229416612768546e-05\n",
            "Training: Batch 2/2, Loss: 6.037771890987642e-05\n",
            "Validation: Batch 1/1, Loss: 5.860824239789508e-05\n",
            "Epoch: 1522/10000\n",
            "Training: Batch 1/2, Loss: 6.291656609391794e-05\n",
            "Training: Batch 2/2, Loss: 5.8767542213900015e-05\n",
            "Validation: Batch 1/1, Loss: 5.813031384604983e-05\n",
            "Epoch: 1523/10000\n",
            "Training: Batch 1/2, Loss: 6.007379124639556e-05\n",
            "Training: Batch 2/2, Loss: 6.0617963754339144e-05\n",
            "Validation: Batch 1/1, Loss: 5.765703826909885e-05\n",
            "Epoch: 1524/10000\n",
            "Training: Batch 1/2, Loss: 6.005885734339245e-05\n",
            "Training: Batch 2/2, Loss: 5.96590616623871e-05\n",
            "Validation: Batch 1/1, Loss: 5.718641114071943e-05\n",
            "Epoch: 1525/10000\n",
            "Training: Batch 1/2, Loss: 5.85739835514687e-05\n",
            "Training: Batch 2/2, Loss: 6.017032501404174e-05\n",
            "Validation: Batch 1/1, Loss: 5.672098632203415e-05\n",
            "Epoch: 1526/10000\n",
            "Training: Batch 1/2, Loss: 5.808662899653427e-05\n",
            "Training: Batch 2/2, Loss: 5.9696209063986316e-05\n",
            "Validation: Batch 1/1, Loss: 5.625812991638668e-05\n",
            "Epoch: 1527/10000\n",
            "Training: Batch 1/2, Loss: 5.874655835214071e-05\n",
            "Training: Batch 2/2, Loss: 5.808615605928935e-05\n",
            "Validation: Batch 1/1, Loss: 5.57983476028312e-05\n",
            "Epoch: 1528/10000\n",
            "Training: Batch 1/2, Loss: 5.727876123273745e-05\n",
            "Training: Batch 2/2, Loss: 5.860300370841287e-05\n",
            "Validation: Batch 1/1, Loss: 5.534295269171707e-05\n",
            "Epoch: 1529/10000\n",
            "Training: Batch 1/2, Loss: 5.648845399264246e-05\n",
            "Training: Batch 2/2, Loss: 5.845298437634483e-05\n",
            "Validation: Batch 1/1, Loss: 5.489073373610154e-05\n",
            "Epoch: 1530/10000\n",
            "Training: Batch 1/2, Loss: 5.67823153687641e-05\n",
            "Training: Batch 2/2, Loss: 5.7229772210121155e-05\n",
            "Validation: Batch 1/1, Loss: 5.4441778047475964e-05\n",
            "Epoch: 1531/10000\n",
            "Training: Batch 1/2, Loss: 5.94747943978291e-05\n",
            "Training: Batch 2/2, Loss: 5.362364390748553e-05\n",
            "Validation: Batch 1/1, Loss: 5.399459405452944e-05\n",
            "Epoch: 1532/10000\n",
            "Training: Batch 1/2, Loss: 5.2935654821339995e-05\n",
            "Training: Batch 2/2, Loss: 5.9219415561528876e-05\n",
            "Validation: Batch 1/1, Loss: 5.3554136684397236e-05\n",
            "Epoch: 1533/10000\n",
            "Training: Batch 1/2, Loss: 5.5604614317417145e-05\n",
            "Training: Batch 2/2, Loss: 5.565156607190147e-05\n",
            "Validation: Batch 1/1, Loss: 5.3115698392502964e-05\n",
            "Epoch: 1534/10000\n",
            "Training: Batch 1/2, Loss: 5.448257070383988e-05\n",
            "Training: Batch 2/2, Loss: 5.5865803005872294e-05\n",
            "Validation: Batch 1/1, Loss: 5.2680621593026444e-05\n",
            "Epoch: 1535/10000\n",
            "Training: Batch 1/2, Loss: 5.458200030261651e-05\n",
            "Training: Batch 2/2, Loss: 5.487120506586507e-05\n",
            "Validation: Batch 1/1, Loss: 5.224844426265918e-05\n",
            "Epoch: 1536/10000\n",
            "Training: Batch 1/2, Loss: 5.240352402324788e-05\n",
            "Training: Batch 2/2, Loss: 5.61518500035163e-05\n",
            "Validation: Batch 1/1, Loss: 5.182045424589887e-05\n",
            "Epoch: 1537/10000\n",
            "Training: Batch 1/2, Loss: 5.2770948968827724e-05\n",
            "Training: Batch 2/2, Loss: 5.490423791343346e-05\n",
            "Validation: Batch 1/1, Loss: 5.139570566825569e-05\n",
            "Epoch: 1538/10000\n",
            "Training: Batch 1/2, Loss: 5.606675040326081e-05\n",
            "Training: Batch 2/2, Loss: 5.074337241239846e-05\n",
            "Validation: Batch 1/1, Loss: 5.097223038319498e-05\n",
            "Epoch: 1539/10000\n",
            "Training: Batch 1/2, Loss: 5.176203194423579e-05\n",
            "Training: Batch 2/2, Loss: 5.416287240223028e-05\n",
            "Validation: Batch 1/1, Loss: 5.055434667156078e-05\n",
            "Epoch: 1540/10000\n",
            "Training: Batch 1/2, Loss: 5.290988337947056e-05\n",
            "Training: Batch 2/2, Loss: 5.215430064708926e-05\n",
            "Validation: Batch 1/1, Loss: 5.013874397263862e-05\n",
            "Epoch: 1541/10000\n",
            "Training: Batch 1/2, Loss: 5.4311145504470915e-05\n",
            "Training: Batch 2/2, Loss: 4.99020898132585e-05\n",
            "Validation: Batch 1/1, Loss: 4.972507667844184e-05\n",
            "Epoch: 1542/10000\n",
            "Training: Batch 1/2, Loss: 5.066830635769293e-05\n",
            "Training: Batch 2/2, Loss: 5.268126187729649e-05\n",
            "Validation: Batch 1/1, Loss: 4.931593139190227e-05\n",
            "Epoch: 1543/10000\n",
            "Training: Batch 1/2, Loss: 4.960502337780781e-05\n",
            "Training: Batch 2/2, Loss: 5.289780892780982e-05\n",
            "Validation: Batch 1/1, Loss: 4.8910380428424105e-05\n",
            "Epoch: 1544/10000\n",
            "Training: Batch 1/2, Loss: 5.1115093810949475e-05\n",
            "Training: Batch 2/2, Loss: 5.0556540372781456e-05\n",
            "Validation: Batch 1/1, Loss: 4.8507354222238064e-05\n",
            "Epoch: 1545/10000\n",
            "Training: Batch 1/2, Loss: 5.070860424893908e-05\n",
            "Training: Batch 2/2, Loss: 5.0131759053329006e-05\n",
            "Validation: Batch 1/1, Loss: 4.81074457638897e-05\n",
            "Epoch: 1546/10000\n",
            "Training: Batch 1/2, Loss: 5.1662238547578454e-05\n",
            "Training: Batch 2/2, Loss: 4.835648724110797e-05\n",
            "Validation: Batch 1/1, Loss: 4.7710367653053254e-05\n",
            "Epoch: 1547/10000\n",
            "Training: Batch 1/2, Loss: 5.131969737703912e-05\n",
            "Training: Batch 2/2, Loss: 4.787920261151157e-05\n",
            "Validation: Batch 1/1, Loss: 4.7315817937487736e-05\n",
            "Epoch: 1548/10000\n",
            "Training: Batch 1/2, Loss: 4.939394784742035e-05\n",
            "Training: Batch 2/2, Loss: 4.898521365248598e-05\n",
            "Validation: Batch 1/1, Loss: 4.692543370765634e-05\n",
            "Epoch: 1549/10000\n",
            "Training: Batch 1/2, Loss: 4.875172089668922e-05\n",
            "Training: Batch 2/2, Loss: 4.881970744463615e-05\n",
            "Validation: Batch 1/1, Loss: 4.653795622289181e-05\n",
            "Epoch: 1550/10000\n",
            "Training: Batch 1/2, Loss: 4.6849672798998654e-05\n",
            "Training: Batch 2/2, Loss: 4.991625246475451e-05\n",
            "Validation: Batch 1/1, Loss: 4.615445504896343e-05\n",
            "Epoch: 1551/10000\n",
            "Training: Batch 1/2, Loss: 4.644227010430768e-05\n",
            "Training: Batch 2/2, Loss: 4.953039388055913e-05\n",
            "Validation: Batch 1/1, Loss: 4.5773460442433134e-05\n",
            "Epoch: 1552/10000\n",
            "Training: Batch 1/2, Loss: 4.751989399665035e-05\n",
            "Training: Batch 2/2, Loss: 4.7670313506387174e-05\n",
            "Validation: Batch 1/1, Loss: 4.539468864095397e-05\n",
            "Epoch: 1553/10000\n",
            "Training: Batch 1/2, Loss: 4.720119250123389e-05\n",
            "Training: Batch 2/2, Loss: 4.720669676316902e-05\n",
            "Validation: Batch 1/1, Loss: 4.5019103708909824e-05\n",
            "Epoch: 1554/10000\n",
            "Training: Batch 1/2, Loss: 4.60385053884238e-05\n",
            "Training: Batch 2/2, Loss: 4.759015791933052e-05\n",
            "Validation: Batch 1/1, Loss: 4.464656376512721e-05\n",
            "Epoch: 1555/10000\n",
            "Training: Batch 1/2, Loss: 4.734099275083281e-05\n",
            "Training: Batch 2/2, Loss: 4.552346945274621e-05\n",
            "Validation: Batch 1/1, Loss: 4.4275770051172e-05\n",
            "Epoch: 1556/10000\n",
            "Training: Batch 1/2, Loss: 4.634366268874146e-05\n",
            "Training: Batch 2/2, Loss: 4.575352431857027e-05\n",
            "Validation: Batch 1/1, Loss: 4.3908316001761705e-05\n",
            "Epoch: 1557/10000\n",
            "Training: Batch 1/2, Loss: 4.7082477976800874e-05\n",
            "Training: Batch 2/2, Loss: 4.425954830367118e-05\n",
            "Validation: Batch 1/1, Loss: 4.3543590436456725e-05\n",
            "Epoch: 1558/10000\n",
            "Training: Batch 1/2, Loss: 4.5334090827964246e-05\n",
            "Training: Batch 2/2, Loss: 4.524940231931396e-05\n",
            "Validation: Batch 1/1, Loss: 4.318273568060249e-05\n",
            "Epoch: 1559/10000\n",
            "Training: Batch 1/2, Loss: 4.645591616281308e-05\n",
            "Training: Batch 2/2, Loss: 4.338651706348173e-05\n",
            "Validation: Batch 1/1, Loss: 4.2823732655961066e-05\n",
            "Epoch: 1560/10000\n",
            "Training: Batch 1/2, Loss: 4.5316181058296934e-05\n",
            "Training: Batch 2/2, Loss: 4.378127778181806e-05\n",
            "Validation: Batch 1/1, Loss: 4.246781827532686e-05\n",
            "Epoch: 1561/10000\n",
            "Training: Batch 1/2, Loss: 4.410932524478994e-05\n",
            "Training: Batch 2/2, Loss: 4.4249685743125156e-05\n",
            "Validation: Batch 1/1, Loss: 4.211506529827602e-05\n",
            "Epoch: 1562/10000\n",
            "Training: Batch 1/2, Loss: 4.4219803385203704e-05\n",
            "Training: Batch 2/2, Loss: 4.3411557271610945e-05\n",
            "Validation: Batch 1/1, Loss: 4.17651936004404e-05\n",
            "Epoch: 1563/10000\n",
            "Training: Batch 1/2, Loss: 4.4709027861244977e-05\n",
            "Training: Batch 2/2, Loss: 4.220137998345308e-05\n",
            "Validation: Batch 1/1, Loss: 4.141751924180426e-05\n",
            "Epoch: 1564/10000\n",
            "Training: Batch 1/2, Loss: 4.29824176535476e-05\n",
            "Training: Batch 2/2, Loss: 4.3204458052059636e-05\n",
            "Validation: Batch 1/1, Loss: 4.107270433451049e-05\n",
            "Epoch: 1565/10000\n",
            "Training: Batch 1/2, Loss: 4.2327126720920205e-05\n",
            "Training: Batch 2/2, Loss: 4.314567559049465e-05\n",
            "Validation: Batch 1/1, Loss: 4.0730927139520645e-05\n",
            "Epoch: 1566/10000\n",
            "Training: Batch 1/2, Loss: 4.335381163400598e-05\n",
            "Training: Batch 2/2, Loss: 4.1417133616050705e-05\n",
            "Validation: Batch 1/1, Loss: 4.039108898723498e-05\n",
            "Epoch: 1567/10000\n",
            "Training: Batch 1/2, Loss: 4.152000110480003e-05\n",
            "Training: Batch 2/2, Loss: 4.254321538610384e-05\n",
            "Validation: Batch 1/1, Loss: 4.005444861832075e-05\n",
            "Epoch: 1568/10000\n",
            "Training: Batch 1/2, Loss: 4.417526361066848e-05\n",
            "Training: Batch 2/2, Loss: 3.920184826711193e-05\n",
            "Validation: Batch 1/1, Loss: 3.9719299820717424e-05\n",
            "Epoch: 1569/10000\n",
            "Training: Batch 1/2, Loss: 4.13262787333224e-05\n",
            "Training: Batch 2/2, Loss: 4.134904520469718e-05\n",
            "Validation: Batch 1/1, Loss: 3.938793088309467e-05\n",
            "Epoch: 1570/10000\n",
            "Training: Batch 1/2, Loss: 4.1639006667537615e-05\n",
            "Training: Batch 2/2, Loss: 4.035346137243323e-05\n",
            "Validation: Batch 1/1, Loss: 3.9058624679455534e-05\n",
            "Epoch: 1571/10000\n",
            "Training: Batch 1/2, Loss: 3.971777914557606e-05\n",
            "Training: Batch 2/2, Loss: 4.1587969462852925e-05\n",
            "Validation: Batch 1/1, Loss: 3.8732774555683136e-05\n",
            "Epoch: 1572/10000\n",
            "Training: Batch 1/2, Loss: 4.0792208892526105e-05\n",
            "Training: Batch 2/2, Loss: 3.984398063039407e-05\n",
            "Validation: Batch 1/1, Loss: 3.840862700599246e-05\n",
            "Epoch: 1573/10000\n",
            "Training: Batch 1/2, Loss: 4.123513645026833e-05\n",
            "Training: Batch 2/2, Loss: 3.873334208037704e-05\n",
            "Validation: Batch 1/1, Loss: 3.808721521636471e-05\n",
            "Epoch: 1574/10000\n",
            "Training: Batch 1/2, Loss: 4.0414397517452016e-05\n",
            "Training: Batch 2/2, Loss: 3.888765422743745e-05\n",
            "Validation: Batch 1/1, Loss: 3.776839002966881e-05\n",
            "Epoch: 1575/10000\n",
            "Training: Batch 1/2, Loss: 3.9922819269122556e-05\n",
            "Training: Batch 2/2, Loss: 3.8717953430023044e-05\n",
            "Validation: Batch 1/1, Loss: 3.745189678738825e-05\n",
            "Epoch: 1576/10000\n",
            "Training: Batch 1/2, Loss: 3.8960966776357964e-05\n",
            "Training: Batch 2/2, Loss: 3.902335447492078e-05\n",
            "Validation: Batch 1/1, Loss: 3.713829210028052e-05\n",
            "Epoch: 1577/10000\n",
            "Training: Batch 1/2, Loss: 3.8600632251473144e-05\n",
            "Training: Batch 2/2, Loss: 3.873366949846968e-05\n",
            "Validation: Batch 1/1, Loss: 3.6827164876740426e-05\n",
            "Epoch: 1578/10000\n",
            "Training: Batch 1/2, Loss: 3.86368265026249e-05\n",
            "Training: Batch 2/2, Loss: 3.8053975004004315e-05\n",
            "Validation: Batch 1/1, Loss: 3.651815131888725e-05\n",
            "Epoch: 1579/10000\n",
            "Training: Batch 1/2, Loss: 3.8513331674039364e-05\n",
            "Training: Batch 2/2, Loss: 3.7538964534178376e-05\n",
            "Validation: Batch 1/1, Loss: 3.621158248279244e-05\n",
            "Epoch: 1580/10000\n",
            "Training: Batch 1/2, Loss: 3.8721169403288513e-05\n",
            "Training: Batch 2/2, Loss: 3.669806756079197e-05\n",
            "Validation: Batch 1/1, Loss: 3.590713095036335e-05\n",
            "Epoch: 1581/10000\n",
            "Training: Batch 1/2, Loss: 3.697181091411039e-05\n",
            "Training: Batch 2/2, Loss: 3.781260966206901e-05\n",
            "Validation: Batch 1/1, Loss: 3.5605735320132226e-05\n",
            "Epoch: 1582/10000\n",
            "Training: Batch 1/2, Loss: 3.679673682199791e-05\n",
            "Training: Batch 2/2, Loss: 3.7363966839620844e-05\n",
            "Validation: Batch 1/1, Loss: 3.5306456993566826e-05\n",
            "Epoch: 1583/10000\n",
            "Training: Batch 1/2, Loss: 3.635770917753689e-05\n",
            "Training: Batch 2/2, Loss: 3.718268999364227e-05\n",
            "Validation: Batch 1/1, Loss: 3.5009670682484284e-05\n",
            "Epoch: 1584/10000\n",
            "Training: Batch 1/2, Loss: 3.621411451604217e-05\n",
            "Training: Batch 2/2, Loss: 3.6712342989631e-05\n",
            "Validation: Batch 1/1, Loss: 3.471489981166087e-05\n",
            "Epoch: 1585/10000\n",
            "Training: Batch 1/2, Loss: 3.581834971555509e-05\n",
            "Training: Batch 2/2, Loss: 3.649751306511462e-05\n",
            "Validation: Batch 1/1, Loss: 3.442255183472298e-05\n",
            "Epoch: 1586/10000\n",
            "Training: Batch 1/2, Loss: 3.490476592560299e-05\n",
            "Training: Batch 2/2, Loss: 3.6803347029490396e-05\n",
            "Validation: Batch 1/1, Loss: 3.41328232025262e-05\n",
            "Epoch: 1587/10000\n",
            "Training: Batch 1/2, Loss: 3.637386180344038e-05\n",
            "Training: Batch 2/2, Loss: 3.474103505141102e-05\n",
            "Validation: Batch 1/1, Loss: 3.3844342397060245e-05\n",
            "Epoch: 1588/10000\n",
            "Training: Batch 1/2, Loss: 3.5207787732360885e-05\n",
            "Training: Batch 2/2, Loss: 3.530667527229525e-05\n",
            "Validation: Batch 1/1, Loss: 3.3558924769749865e-05\n",
            "Epoch: 1589/10000\n",
            "Training: Batch 1/2, Loss: 3.4522119676694274e-05\n",
            "Training: Batch 2/2, Loss: 3.539957106113434e-05\n",
            "Validation: Batch 1/1, Loss: 3.327577724121511e-05\n",
            "Epoch: 1590/10000\n",
            "Training: Batch 1/2, Loss: 3.574309448595159e-05\n",
            "Training: Batch 2/2, Loss: 3.359699257998727e-05\n",
            "Validation: Batch 1/1, Loss: 3.299421223346144e-05\n",
            "Epoch: 1591/10000\n",
            "Training: Batch 1/2, Loss: 3.304741403553635e-05\n",
            "Training: Batch 2/2, Loss: 3.570123590179719e-05\n",
            "Validation: Batch 1/1, Loss: 3.2716150599299e-05\n",
            "Epoch: 1592/10000\n",
            "Training: Batch 1/2, Loss: 3.553314309101552e-05\n",
            "Training: Batch 2/2, Loss: 3.264854967710562e-05\n",
            "Validation: Batch 1/1, Loss: 3.243880928494036e-05\n",
            "Epoch: 1593/10000\n",
            "Training: Batch 1/2, Loss: 3.338999522384256e-05\n",
            "Training: Batch 2/2, Loss: 3.4210843296023086e-05\n",
            "Validation: Batch 1/1, Loss: 3.216420009266585e-05\n",
            "Epoch: 1594/10000\n",
            "Training: Batch 1/2, Loss: 3.331247717142105e-05\n",
            "Training: Batch 2/2, Loss: 3.372033461346291e-05\n",
            "Validation: Batch 1/1, Loss: 3.1891810067463666e-05\n",
            "Epoch: 1595/10000\n",
            "Training: Batch 1/2, Loss: 3.326071964693256e-05\n",
            "Training: Batch 2/2, Loss: 3.3208183594979346e-05\n",
            "Validation: Batch 1/1, Loss: 3.1621668313164264e-05\n",
            "Epoch: 1596/10000\n",
            "Training: Batch 1/2, Loss: 3.2627609471092e-05\n",
            "Training: Batch 2/2, Loss: 3.328025559312664e-05\n",
            "Validation: Batch 1/1, Loss: 3.135378210572526e-05\n",
            "Epoch: 1597/10000\n",
            "Training: Batch 1/2, Loss: 3.254902912885882e-05\n",
            "Training: Batch 2/2, Loss: 3.28038731822744e-05\n",
            "Validation: Batch 1/1, Loss: 3.108783494099043e-05\n",
            "Epoch: 1598/10000\n",
            "Training: Batch 1/2, Loss: 3.1948053219821304e-05\n",
            "Training: Batch 2/2, Loss: 3.2852527510840446e-05\n",
            "Validation: Batch 1/1, Loss: 3.082412513322197e-05\n",
            "Epoch: 1599/10000\n",
            "Training: Batch 1/2, Loss: 3.429716525715776e-05\n",
            "Training: Batch 2/2, Loss: 2.9966577130835503e-05\n",
            "Validation: Batch 1/1, Loss: 3.056144851143472e-05\n",
            "Epoch: 1600/10000\n",
            "Training: Batch 1/2, Loss: 3.062332689296454e-05\n",
            "Training: Batch 2/2, Loss: 3.308361192466691e-05\n",
            "Validation: Batch 1/1, Loss: 3.0302293453132734e-05\n",
            "Epoch: 1601/10000\n",
            "Training: Batch 1/2, Loss: 3.2303469197358936e-05\n",
            "Training: Batch 2/2, Loss: 3.0873041396262124e-05\n",
            "Validation: Batch 1/1, Loss: 3.004445170518011e-05\n",
            "Epoch: 1602/10000\n",
            "Training: Batch 1/2, Loss: 3.2748015655670315e-05\n",
            "Training: Batch 2/2, Loss: 2.9896709747845307e-05\n",
            "Validation: Batch 1/1, Loss: 2.978867632918991e-05\n",
            "Epoch: 1603/10000\n",
            "Training: Batch 1/2, Loss: 2.969791785289999e-05\n",
            "Training: Batch 2/2, Loss: 3.2406256650574505e-05\n",
            "Validation: Batch 1/1, Loss: 2.953570947283879e-05\n",
            "Epoch: 1604/10000\n",
            "Training: Batch 1/2, Loss: 3.156903767376207e-05\n",
            "Training: Batch 2/2, Loss: 3.001748882525135e-05\n",
            "Validation: Batch 1/1, Loss: 2.9283974072313868e-05\n",
            "Epoch: 1605/10000\n",
            "Training: Batch 1/2, Loss: 3.1116116588236764e-05\n",
            "Training: Batch 2/2, Loss: 2.9947304938104935e-05\n",
            "Validation: Batch 1/1, Loss: 2.903423046518583e-05\n",
            "Epoch: 1606/10000\n",
            "Training: Batch 1/2, Loss: 3.1101157219382e-05\n",
            "Training: Batch 2/2, Loss: 2.944516745628789e-05\n",
            "Validation: Batch 1/1, Loss: 2.878640589187853e-05\n",
            "Epoch: 1607/10000\n",
            "Training: Batch 1/2, Loss: 2.8785021640942432e-05\n",
            "Training: Batch 2/2, Loss: 3.124019713141024e-05\n",
            "Validation: Batch 1/1, Loss: 2.85414371319348e-05\n",
            "Epoch: 1608/10000\n",
            "Training: Batch 1/2, Loss: 2.9866869226680137e-05\n",
            "Training: Batch 2/2, Loss: 2.9655260732397437e-05\n",
            "Validation: Batch 1/1, Loss: 2.8297663448029198e-05\n",
            "Epoch: 1609/10000\n",
            "Training: Batch 1/2, Loss: 2.9413697120617144e-05\n",
            "Training: Batch 2/2, Loss: 2.9601893402286805e-05\n",
            "Validation: Batch 1/1, Loss: 2.8056012524757534e-05\n",
            "Epoch: 1610/10000\n",
            "Training: Batch 1/2, Loss: 2.8388605642248876e-05\n",
            "Training: Batch 2/2, Loss: 3.012316119566094e-05\n",
            "Validation: Batch 1/1, Loss: 2.7816504371003248e-05\n",
            "Epoch: 1611/10000\n",
            "Training: Batch 1/2, Loss: 2.9337770683923736e-05\n",
            "Training: Batch 2/2, Loss: 2.8681557523668744e-05\n",
            "Validation: Batch 1/1, Loss: 2.7578362278291024e-05\n",
            "Epoch: 1612/10000\n",
            "Training: Batch 1/2, Loss: 2.874153506127186e-05\n",
            "Training: Batch 2/2, Loss: 2.8782344088540412e-05\n",
            "Validation: Batch 1/1, Loss: 2.7342601242708042e-05\n",
            "Epoch: 1613/10000\n",
            "Training: Batch 1/2, Loss: 2.8205784474266693e-05\n",
            "Training: Batch 2/2, Loss: 2.8827507776441053e-05\n",
            "Validation: Batch 1/1, Loss: 2.710849184950348e-05\n",
            "Epoch: 1614/10000\n",
            "Training: Batch 1/2, Loss: 2.8810545700252987e-05\n",
            "Training: Batch 2/2, Loss: 2.7740543373511173e-05\n",
            "Validation: Batch 1/1, Loss: 2.6875921321334317e-05\n",
            "Epoch: 1615/10000\n",
            "Training: Batch 1/2, Loss: 2.8019703677273355e-05\n",
            "Training: Batch 2/2, Loss: 2.8047033993061632e-05\n",
            "Validation: Batch 1/1, Loss: 2.6645717298379168e-05\n",
            "Epoch: 1616/10000\n",
            "Training: Batch 1/2, Loss: 2.8198628569953144e-05\n",
            "Training: Batch 2/2, Loss: 2.7391461117076688e-05\n",
            "Validation: Batch 1/1, Loss: 2.6416993932798505e-05\n",
            "Epoch: 1617/10000\n",
            "Training: Batch 1/2, Loss: 2.6408335543237627e-05\n",
            "Training: Batch 2/2, Loss: 2.8701519113383256e-05\n",
            "Validation: Batch 1/1, Loss: 2.619088081701193e-05\n",
            "Epoch: 1618/10000\n",
            "Training: Batch 1/2, Loss: 2.7456908355816267e-05\n",
            "Training: Batch 2/2, Loss: 2.718757968978025e-05\n",
            "Validation: Batch 1/1, Loss: 2.5965928216464818e-05\n",
            "Epoch: 1619/10000\n",
            "Training: Batch 1/2, Loss: 2.7766440325649455e-05\n",
            "Training: Batch 2/2, Loss: 2.6413132218294777e-05\n",
            "Validation: Batch 1/1, Loss: 2.5742310754139908e-05\n",
            "Epoch: 1620/10000\n",
            "Training: Batch 1/2, Loss: 2.7086502086604014e-05\n",
            "Training: Batch 2/2, Loss: 2.662745100678876e-05\n",
            "Validation: Batch 1/1, Loss: 2.5520894268993288e-05\n",
            "Epoch: 1621/10000\n",
            "Training: Batch 1/2, Loss: 2.7435788069851696e-05\n",
            "Training: Batch 2/2, Loss: 2.582091292424593e-05\n",
            "Validation: Batch 1/1, Loss: 2.530111487430986e-05\n",
            "Epoch: 1622/10000\n",
            "Training: Batch 1/2, Loss: 2.6749630706035532e-05\n",
            "Training: Batch 2/2, Loss: 2.6048837753478438e-05\n",
            "Validation: Batch 1/1, Loss: 2.5083203581743874e-05\n",
            "Epoch: 1623/10000\n",
            "Training: Batch 1/2, Loss: 2.7111900635645725e-05\n",
            "Training: Batch 2/2, Loss: 2.5236759029212408e-05\n",
            "Validation: Batch 1/1, Loss: 2.4866685635061003e-05\n",
            "Epoch: 1624/10000\n",
            "Training: Batch 1/2, Loss: 2.554581078584306e-05\n",
            "Training: Batch 2/2, Loss: 2.6348467144998722e-05\n",
            "Validation: Batch 1/1, Loss: 2.465291618136689e-05\n",
            "Epoch: 1625/10000\n",
            "Training: Batch 1/2, Loss: 2.64983009401476e-05\n",
            "Training: Batch 2/2, Loss: 2.4955825210781768e-05\n",
            "Validation: Batch 1/1, Loss: 2.4440210836473852e-05\n",
            "Epoch: 1626/10000\n",
            "Training: Batch 1/2, Loss: 2.587664312159177e-05\n",
            "Training: Batch 2/2, Loss: 2.5134360839729197e-05\n",
            "Validation: Batch 1/1, Loss: 2.4229111659224145e-05\n",
            "Epoch: 1627/10000\n",
            "Training: Batch 1/2, Loss: 2.4460870918119326e-05\n",
            "Training: Batch 2/2, Loss: 2.610748379083816e-05\n",
            "Validation: Batch 1/1, Loss: 2.402024438197259e-05\n",
            "Epoch: 1628/10000\n",
            "Training: Batch 1/2, Loss: 2.4453976948279887e-05\n",
            "Training: Batch 2/2, Loss: 2.5681179977254942e-05\n",
            "Validation: Batch 1/1, Loss: 2.3812901417841204e-05\n",
            "Epoch: 1629/10000\n",
            "Training: Batch 1/2, Loss: 2.5013072445290163e-05\n",
            "Training: Batch 2/2, Loss: 2.469437640684191e-05\n",
            "Validation: Batch 1/1, Loss: 2.360695907555055e-05\n",
            "Epoch: 1630/10000\n",
            "Training: Batch 1/2, Loss: 2.3514756321674213e-05\n",
            "Training: Batch 2/2, Loss: 2.576012775534764e-05\n",
            "Validation: Batch 1/1, Loss: 2.3403177692671306e-05\n",
            "Epoch: 1631/10000\n",
            "Training: Batch 1/2, Loss: 2.3929331291583367e-05\n",
            "Training: Batch 2/2, Loss: 2.4923894670791924e-05\n",
            "Validation: Batch 1/1, Loss: 2.3200598661787808e-05\n",
            "Epoch: 1632/10000\n",
            "Training: Batch 1/2, Loss: 2.475650398992002e-05\n",
            "Training: Batch 2/2, Loss: 2.3679656806052662e-05\n",
            "Validation: Batch 1/1, Loss: 2.299920015502721e-05\n",
            "Epoch: 1633/10000\n",
            "Training: Batch 1/2, Loss: 2.505603333702311e-05\n",
            "Training: Batch 2/2, Loss: 2.296403363288846e-05\n",
            "Validation: Batch 1/1, Loss: 2.2799393263994716e-05\n",
            "Epoch: 1634/10000\n",
            "Training: Batch 1/2, Loss: 2.369212052144576e-05\n",
            "Training: Batch 2/2, Loss: 2.3908765797386877e-05\n",
            "Validation: Batch 1/1, Loss: 2.2601318050874397e-05\n",
            "Epoch: 1635/10000\n",
            "Training: Batch 1/2, Loss: 2.3413282178808004e-05\n",
            "Training: Batch 2/2, Loss: 2.3776197849656455e-05\n",
            "Validation: Batch 1/1, Loss: 2.2405163690564223e-05\n",
            "Epoch: 1636/10000\n",
            "Training: Batch 1/2, Loss: 2.3092437913874164e-05\n",
            "Training: Batch 2/2, Loss: 2.3688953660894185e-05\n",
            "Validation: Batch 1/1, Loss: 2.221079557784833e-05\n",
            "Epoch: 1637/10000\n",
            "Training: Batch 1/2, Loss: 2.3423644961440004e-05\n",
            "Training: Batch 2/2, Loss: 2.295591366419103e-05\n",
            "Validation: Batch 1/1, Loss: 2.2017522496753372e-05\n",
            "Epoch: 1638/10000\n",
            "Training: Batch 1/2, Loss: 2.408935506537091e-05\n",
            "Training: Batch 2/2, Loss: 2.1892263248446397e-05\n",
            "Validation: Batch 1/1, Loss: 2.182546813855879e-05\n",
            "Epoch: 1639/10000\n",
            "Training: Batch 1/2, Loss: 2.2391812308342196e-05\n",
            "Training: Batch 2/2, Loss: 2.3185093596111983e-05\n",
            "Validation: Batch 1/1, Loss: 2.163590215786826e-05\n",
            "Epoch: 1640/10000\n",
            "Training: Batch 1/2, Loss: 2.176799353037495e-05\n",
            "Training: Batch 2/2, Loss: 2.341283470741473e-05\n",
            "Validation: Batch 1/1, Loss: 2.1447971448651515e-05\n",
            "Epoch: 1641/10000\n",
            "Training: Batch 1/2, Loss: 2.3289125238079578e-05\n",
            "Training: Batch 2/2, Loss: 2.15072141145356e-05\n",
            "Validation: Batch 1/1, Loss: 2.1260884750518017e-05\n",
            "Epoch: 1642/10000\n",
            "Training: Batch 1/2, Loss: 2.2724392692907713e-05\n",
            "Training: Batch 2/2, Loss: 2.1681464204448275e-05\n",
            "Validation: Batch 1/1, Loss: 2.1075264157843776e-05\n",
            "Epoch: 1643/10000\n",
            "Training: Batch 1/2, Loss: 2.2130756406113505e-05\n",
            "Training: Batch 2/2, Loss: 2.1887954062549397e-05\n",
            "Validation: Batch 1/1, Loss: 2.089158988383133e-05\n",
            "Epoch: 1644/10000\n",
            "Training: Batch 1/2, Loss: 2.107052023347933e-05\n",
            "Training: Batch 2/2, Loss: 2.2562959202332422e-05\n",
            "Validation: Batch 1/1, Loss: 2.070943810394965e-05\n",
            "Epoch: 1645/10000\n",
            "Training: Batch 1/2, Loss: 2.14931587834144e-05\n",
            "Training: Batch 2/2, Loss: 2.176385351049248e-05\n",
            "Validation: Batch 1/1, Loss: 2.0528812456177548e-05\n",
            "Epoch: 1646/10000\n",
            "Training: Batch 1/2, Loss: 2.1197673049755394e-05\n",
            "Training: Batch 2/2, Loss: 2.168300125049427e-05\n",
            "Validation: Batch 1/1, Loss: 2.0349327314761467e-05\n",
            "Epoch: 1647/10000\n",
            "Training: Batch 1/2, Loss: 2.2504886146634817e-05\n",
            "Training: Batch 2/2, Loss: 2.0008463252452202e-05\n",
            "Validation: Batch 1/1, Loss: 2.0171009964542463e-05\n",
            "Epoch: 1648/10000\n",
            "Training: Batch 1/2, Loss: 2.1193673092056997e-05\n",
            "Training: Batch 2/2, Loss: 2.0944455172866583e-05\n",
            "Validation: Batch 1/1, Loss: 1.9994660760858096e-05\n",
            "Epoch: 1649/10000\n",
            "Training: Batch 1/2, Loss: 2.15479976759525e-05\n",
            "Training: Batch 2/2, Loss: 2.022512308030855e-05\n",
            "Validation: Batch 1/1, Loss: 1.9819282897515222e-05\n",
            "Epoch: 1650/10000\n",
            "Training: Batch 1/2, Loss: 2.0120236513321288e-05\n",
            "Training: Batch 2/2, Loss: 2.1283723981468938e-05\n",
            "Validation: Batch 1/1, Loss: 1.9646067812573165e-05\n",
            "Epoch: 1651/10000\n",
            "Training: Batch 1/2, Loss: 2.087277789541986e-05\n",
            "Training: Batch 2/2, Loss: 2.017390579567291e-05\n",
            "Validation: Batch 1/1, Loss: 1.9473616703180596e-05\n",
            "Epoch: 1652/10000\n",
            "Training: Batch 1/2, Loss: 2.1206969904596917e-05\n",
            "Training: Batch 2/2, Loss: 1.9483162759570405e-05\n",
            "Validation: Batch 1/1, Loss: 1.9302497094031423e-05\n",
            "Epoch: 1653/10000\n",
            "Training: Batch 1/2, Loss: 2.031390613410622e-05\n",
            "Training: Batch 2/2, Loss: 2.001712709898129e-05\n",
            "Validation: Batch 1/1, Loss: 1.913310006784741e-05\n",
            "Epoch: 1654/10000\n",
            "Training: Batch 1/2, Loss: 1.991204590012785e-05\n",
            "Training: Batch 2/2, Loss: 2.0065805074409582e-05\n",
            "Validation: Batch 1/1, Loss: 1.896501998999156e-05\n",
            "Epoch: 1655/10000\n",
            "Training: Batch 1/2, Loss: 2.0211029550409876e-05\n",
            "Training: Batch 2/2, Loss: 1.9419203454162925e-05\n",
            "Validation: Batch 1/1, Loss: 1.879813862615265e-05\n",
            "Epoch: 1656/10000\n",
            "Training: Batch 1/2, Loss: 1.8840310076484457e-05\n",
            "Training: Batch 2/2, Loss: 2.043849599431269e-05\n",
            "Validation: Batch 1/1, Loss: 1.8633241779753007e-05\n",
            "Epoch: 1657/10000\n",
            "Training: Batch 1/2, Loss: 1.8613556676427834e-05\n",
            "Training: Batch 2/2, Loss: 2.032167321885936e-05\n",
            "Validation: Batch 1/1, Loss: 1.8469638234819286e-05\n",
            "Epoch: 1658/10000\n",
            "Training: Batch 1/2, Loss: 1.9985873223049566e-05\n",
            "Training: Batch 2/2, Loss: 1.8614029613672756e-05\n",
            "Validation: Batch 1/1, Loss: 1.8306593119632453e-05\n",
            "Epoch: 1659/10000\n",
            "Training: Batch 1/2, Loss: 1.8906357581727207e-05\n",
            "Training: Batch 2/2, Loss: 1.9350985894561745e-05\n",
            "Validation: Batch 1/1, Loss: 1.8145366993849166e-05\n",
            "Epoch: 1660/10000\n",
            "Training: Batch 1/2, Loss: 1.9174733097315766e-05\n",
            "Training: Batch 2/2, Loss: 1.8748673028312624e-05\n",
            "Validation: Batch 1/1, Loss: 1.7985288650379516e-05\n",
            "Epoch: 1661/10000\n",
            "Training: Batch 1/2, Loss: 1.9453269487712532e-05\n",
            "Training: Batch 2/2, Loss: 1.813871131162159e-05\n",
            "Validation: Batch 1/1, Loss: 1.7826185285230167e-05\n",
            "Epoch: 1662/10000\n",
            "Training: Batch 1/2, Loss: 1.872816210379824e-05\n",
            "Training: Batch 2/2, Loss: 1.8530801753513515e-05\n",
            "Validation: Batch 1/1, Loss: 1.7668720829533413e-05\n",
            "Epoch: 1663/10000\n",
            "Training: Batch 1/2, Loss: 1.8140257452614605e-05\n",
            "Training: Batch 2/2, Loss: 1.878922739706468e-05\n",
            "Validation: Batch 1/1, Loss: 1.7512664271635003e-05\n",
            "Epoch: 1664/10000\n",
            "Training: Batch 1/2, Loss: 1.8049031496047974e-05\n",
            "Training: Batch 2/2, Loss: 1.855617665569298e-05\n",
            "Validation: Batch 1/1, Loss: 1.7357973774778657e-05\n",
            "Epoch: 1665/10000\n",
            "Training: Batch 1/2, Loss: 1.8413060388411395e-05\n",
            "Training: Batch 2/2, Loss: 1.7871885575004853e-05\n",
            "Validation: Batch 1/1, Loss: 1.7204143659910187e-05\n",
            "Epoch: 1666/10000\n",
            "Training: Batch 1/2, Loss: 1.7888030924950726e-05\n",
            "Training: Batch 2/2, Loss: 1.8075583284371532e-05\n",
            "Validation: Batch 1/1, Loss: 1.7051939721568488e-05\n",
            "Epoch: 1667/10000\n",
            "Training: Batch 1/2, Loss: 1.793171941244509e-05\n",
            "Training: Batch 2/2, Loss: 1.7715608919388615e-05\n",
            "Validation: Batch 1/1, Loss: 1.690062345005572e-05\n",
            "Epoch: 1668/10000\n",
            "Training: Batch 1/2, Loss: 1.807005173759535e-05\n",
            "Training: Batch 2/2, Loss: 1.726356458675582e-05\n",
            "Validation: Batch 1/1, Loss: 1.6750622307881713e-05\n",
            "Epoch: 1669/10000\n",
            "Training: Batch 1/2, Loss: 1.658904511714354e-05\n",
            "Training: Batch 2/2, Loss: 1.842743222368881e-05\n",
            "Validation: Batch 1/1, Loss: 1.6602316463831812e-05\n",
            "Epoch: 1670/10000\n",
            "Training: Batch 1/2, Loss: 1.7371321519021876e-05\n",
            "Training: Batch 2/2, Loss: 1.734002580633387e-05\n",
            "Validation: Batch 1/1, Loss: 1.6454849173896946e-05\n",
            "Epoch: 1671/10000\n",
            "Training: Batch 1/2, Loss: 1.7145117453765124e-05\n",
            "Training: Batch 2/2, Loss: 1.7258971638511866e-05\n",
            "Validation: Batch 1/1, Loss: 1.6308682461385615e-05\n",
            "Epoch: 1672/10000\n",
            "Training: Batch 1/2, Loss: 1.702394729363732e-05\n",
            "Training: Batch 2/2, Loss: 1.7075710275094025e-05\n",
            "Validation: Batch 1/1, Loss: 1.6163681721081957e-05\n",
            "Epoch: 1673/10000\n",
            "Training: Batch 1/2, Loss: 1.703858833934646e-05\n",
            "Training: Batch 2/2, Loss: 1.6759817299316637e-05\n",
            "Validation: Batch 1/1, Loss: 1.6019710528780706e-05\n",
            "Epoch: 1674/10000\n",
            "Training: Batch 1/2, Loss: 1.7031747120199725e-05\n",
            "Training: Batch 2/2, Loss: 1.6467432942590676e-05\n",
            "Validation: Batch 1/1, Loss: 1.587698898219969e-05\n",
            "Epoch: 1675/10000\n",
            "Training: Batch 1/2, Loss: 1.6428972230642103e-05\n",
            "Training: Batch 2/2, Loss: 1.6771400623838417e-05\n",
            "Validation: Batch 1/1, Loss: 1.573542795085814e-05\n",
            "Epoch: 1676/10000\n",
            "Training: Batch 1/2, Loss: 1.6818552467157133e-05\n",
            "Training: Batch 2/2, Loss: 1.6089163182186894e-05\n",
            "Validation: Batch 1/1, Loss: 1.5594974684063345e-05\n",
            "Epoch: 1677/10000\n",
            "Training: Batch 1/2, Loss: 1.7385915271006525e-05\n",
            "Training: Batch 2/2, Loss: 1.5232079022098333e-05\n",
            "Validation: Batch 1/1, Loss: 1.545551640447229e-05\n",
            "Epoch: 1678/10000\n",
            "Training: Batch 1/2, Loss: 1.6023408534238115e-05\n",
            "Training: Batch 2/2, Loss: 1.6299269191222265e-05\n",
            "Validation: Batch 1/1, Loss: 1.5317571524064988e-05\n",
            "Epoch: 1679/10000\n",
            "Training: Batch 1/2, Loss: 1.647882891120389e-05\n",
            "Training: Batch 2/2, Loss: 1.5559084204141982e-05\n",
            "Validation: Batch 1/1, Loss: 1.518052704341244e-05\n",
            "Epoch: 1680/10000\n",
            "Training: Batch 1/2, Loss: 1.6209720342885703e-05\n",
            "Training: Batch 2/2, Loss: 1.5542314940830693e-05\n",
            "Validation: Batch 1/1, Loss: 1.504490774095757e-05\n",
            "Epoch: 1681/10000\n",
            "Training: Batch 1/2, Loss: 1.581472679390572e-05\n",
            "Training: Batch 2/2, Loss: 1.565347338328138e-05\n",
            "Validation: Batch 1/1, Loss: 1.4910377103660721e-05\n",
            "Epoch: 1682/10000\n",
            "Training: Batch 1/2, Loss: 1.4996311620052438e-05\n",
            "Training: Batch 2/2, Loss: 1.6189083908102475e-05\n",
            "Validation: Batch 1/1, Loss: 1.4777145224798005e-05\n",
            "Epoch: 1683/10000\n",
            "Training: Batch 1/2, Loss: 1.5624778825440444e-05\n",
            "Training: Batch 2/2, Loss: 1.528600841993466e-05\n",
            "Validation: Batch 1/1, Loss: 1.4644625480286777e-05\n",
            "Epoch: 1684/10000\n",
            "Training: Batch 1/2, Loss: 1.5914454706944525e-05\n",
            "Training: Batch 2/2, Loss: 1.472218627895927e-05\n",
            "Validation: Batch 1/1, Loss: 1.4513190762954764e-05\n",
            "Epoch: 1685/10000\n",
            "Training: Batch 1/2, Loss: 1.591608815942891e-05\n",
            "Training: Batch 2/2, Loss: 1.4446983186644502e-05\n",
            "Validation: Batch 1/1, Loss: 1.4382782865141053e-05\n",
            "Epoch: 1686/10000\n",
            "Training: Batch 1/2, Loss: 1.577145667397417e-05\n",
            "Training: Batch 2/2, Loss: 1.4319938600237947e-05\n",
            "Validation: Batch 1/1, Loss: 1.4253575500333682e-05\n",
            "Epoch: 1687/10000\n",
            "Training: Batch 1/2, Loss: 1.4641937013948336e-05\n",
            "Training: Batch 2/2, Loss: 1.51763515532366e-05\n",
            "Validation: Batch 1/1, Loss: 1.412599885952659e-05\n",
            "Epoch: 1688/10000\n",
            "Training: Batch 1/2, Loss: 1.5119116142159328e-05\n",
            "Training: Batch 2/2, Loss: 1.4435135199164506e-05\n",
            "Validation: Batch 1/1, Loss: 1.3999091606820002e-05\n",
            "Epoch: 1689/10000\n",
            "Training: Batch 1/2, Loss: 1.4527507119055372e-05\n",
            "Training: Batch 2/2, Loss: 1.4760245903744362e-05\n",
            "Validation: Batch 1/1, Loss: 1.3873282114218455e-05\n",
            "Epoch: 1690/10000\n",
            "Training: Batch 1/2, Loss: 1.4666840797872283e-05\n",
            "Training: Batch 2/2, Loss: 1.4359772649186198e-05\n",
            "Validation: Batch 1/1, Loss: 1.3748272976954468e-05\n",
            "Epoch: 1691/10000\n",
            "Training: Batch 1/2, Loss: 1.4288813872553874e-05\n",
            "Training: Batch 2/2, Loss: 1.447630438633496e-05\n",
            "Validation: Batch 1/1, Loss: 1.3624508937937208e-05\n",
            "Epoch: 1692/10000\n",
            "Training: Batch 1/2, Loss: 1.433893612556858e-05\n",
            "Training: Batch 2/2, Loss: 1.4169077076076064e-05\n",
            "Validation: Batch 1/1, Loss: 1.350175170955481e-05\n",
            "Epoch: 1693/10000\n",
            "Training: Batch 1/2, Loss: 1.4400460713659413e-05\n",
            "Training: Batch 2/2, Loss: 1.3852039046469145e-05\n",
            "Validation: Batch 1/1, Loss: 1.3379953088588081e-05\n",
            "Epoch: 1694/10000\n",
            "Training: Batch 1/2, Loss: 1.3349885193747468e-05\n",
            "Training: Batch 2/2, Loss: 1.4645035662397277e-05\n",
            "Validation: Batch 1/1, Loss: 1.3259482329885941e-05\n",
            "Epoch: 1695/10000\n",
            "Training: Batch 1/2, Loss: 1.3649823813466355e-05\n",
            "Training: Batch 2/2, Loss: 1.4095439837547019e-05\n",
            "Validation: Batch 1/1, Loss: 1.3139639122528024e-05\n",
            "Epoch: 1696/10000\n",
            "Training: Batch 1/2, Loss: 1.3703111108043231e-05\n",
            "Training: Batch 2/2, Loss: 1.3793062862532679e-05\n",
            "Validation: Batch 1/1, Loss: 1.302084911003476e-05\n",
            "Epoch: 1697/10000\n",
            "Training: Batch 1/2, Loss: 1.4036822904017754e-05\n",
            "Training: Batch 2/2, Loss: 1.3213521015131846e-05\n",
            "Validation: Batch 1/1, Loss: 1.2902811249659862e-05\n",
            "Epoch: 1698/10000\n",
            "Training: Batch 1/2, Loss: 1.3286082321428694e-05\n",
            "Training: Batch 2/2, Loss: 1.3715634850086644e-05\n",
            "Validation: Batch 1/1, Loss: 1.2786072147719096e-05\n",
            "Epoch: 1699/10000\n",
            "Training: Batch 1/2, Loss: 1.3910575944464654e-05\n",
            "Training: Batch 2/2, Loss: 1.2850641724071465e-05\n",
            "Validation: Batch 1/1, Loss: 1.2670103387790732e-05\n",
            "Epoch: 1700/10000\n",
            "Training: Batch 1/2, Loss: 1.3783127542410512e-05\n",
            "Training: Batch 2/2, Loss: 1.2736122698697727e-05\n",
            "Validation: Batch 1/1, Loss: 1.2555051398521755e-05\n",
            "Epoch: 1701/10000\n",
            "Training: Batch 1/2, Loss: 1.3247610695543699e-05\n",
            "Training: Batch 2/2, Loss: 1.3030094123678282e-05\n",
            "Validation: Batch 1/1, Loss: 1.2441045328159817e-05\n",
            "Epoch: 1702/10000\n",
            "Training: Batch 1/2, Loss: 1.2861481991421897e-05\n",
            "Training: Batch 2/2, Loss: 1.3177605978853535e-05\n",
            "Validation: Batch 1/1, Loss: 1.2328324373811483e-05\n",
            "Epoch: 1703/10000\n",
            "Training: Batch 1/2, Loss: 1.3260704690765124e-05\n",
            "Training: Batch 2/2, Loss: 1.254516337212408e-05\n",
            "Validation: Batch 1/1, Loss: 1.2216140021337196e-05\n",
            "Epoch: 1704/10000\n",
            "Training: Batch 1/2, Loss: 1.3054802366241347e-05\n",
            "Training: Batch 2/2, Loss: 1.2516865353973117e-05\n",
            "Validation: Batch 1/1, Loss: 1.2104952475056052e-05\n",
            "Epoch: 1705/10000\n",
            "Training: Batch 1/2, Loss: 1.2566930308821611e-05\n",
            "Training: Batch 2/2, Loss: 1.277152568945894e-05\n",
            "Validation: Batch 1/1, Loss: 1.1995083696092479e-05\n",
            "Epoch: 1706/10000\n",
            "Training: Batch 1/2, Loss: 1.2542625881906133e-05\n",
            "Training: Batch 2/2, Loss: 1.256677569472231e-05\n",
            "Validation: Batch 1/1, Loss: 1.1885879757755902e-05\n",
            "Epoch: 1707/10000\n",
            "Training: Batch 1/2, Loss: 1.2910857549286447e-05\n",
            "Training: Batch 2/2, Loss: 1.197283563669771e-05\n",
            "Validation: Batch 1/1, Loss: 1.1777538020396605e-05\n",
            "Epoch: 1708/10000\n",
            "Training: Batch 1/2, Loss: 1.2517450159066357e-05\n",
            "Training: Batch 2/2, Loss: 1.2138731108279899e-05\n",
            "Validation: Batch 1/1, Loss: 1.167013033409603e-05\n",
            "Epoch: 1709/10000\n",
            "Training: Batch 1/2, Loss: 1.2019286259601358e-05\n",
            "Training: Batch 2/2, Loss: 1.2411348507157527e-05\n",
            "Validation: Batch 1/1, Loss: 1.1563844964257441e-05\n",
            "Epoch: 1710/10000\n",
            "Training: Batch 1/2, Loss: 1.1777839063142892e-05\n",
            "Training: Batch 2/2, Loss: 1.243029964825837e-05\n",
            "Validation: Batch 1/1, Loss: 1.1458449080237187e-05\n",
            "Epoch: 1711/10000\n",
            "Training: Batch 1/2, Loss: 1.1669295417959802e-05\n",
            "Training: Batch 2/2, Loss: 1.2318735571170691e-05\n",
            "Validation: Batch 1/1, Loss: 1.1353821719239932e-05\n",
            "Epoch: 1712/10000\n",
            "Training: Batch 1/2, Loss: 1.1793242265412118e-05\n",
            "Training: Batch 2/2, Loss: 1.1977548638242297e-05\n",
            "Validation: Batch 1/1, Loss: 1.124998107115971e-05\n",
            "Epoch: 1713/10000\n",
            "Training: Batch 1/2, Loss: 1.2011132639599964e-05\n",
            "Training: Batch 2/2, Loss: 1.1544245353434235e-05\n",
            "Validation: Batch 1/1, Loss: 1.114689075620845e-05\n",
            "Epoch: 1714/10000\n",
            "Training: Batch 1/2, Loss: 1.1642092431429774e-05\n",
            "Training: Batch 2/2, Loss: 1.1697264199028723e-05\n",
            "Validation: Batch 1/1, Loss: 1.1044812708860263e-05\n",
            "Epoch: 1715/10000\n",
            "Training: Batch 1/2, Loss: 1.1463618648122065e-05\n",
            "Training: Batch 2/2, Loss: 1.1662556971714366e-05\n",
            "Validation: Batch 1/1, Loss: 1.0943617780867498e-05\n",
            "Epoch: 1716/10000\n",
            "Training: Batch 1/2, Loss: 1.129095926444279e-05\n",
            "Training: Batch 2/2, Loss: 1.1623820682871155e-05\n",
            "Validation: Batch 1/1, Loss: 1.0843447853403632e-05\n",
            "Epoch: 1717/10000\n",
            "Training: Batch 1/2, Loss: 1.1300280675641261e-05\n",
            "Training: Batch 2/2, Loss: 1.1405589248170145e-05\n",
            "Validation: Batch 1/1, Loss: 1.0744064638856798e-05\n",
            "Epoch: 1718/10000\n",
            "Training: Batch 1/2, Loss: 1.1155438187415712e-05\n",
            "Training: Batch 2/2, Loss: 1.1342777725076303e-05\n",
            "Validation: Batch 1/1, Loss: 1.0645338079484645e-05\n",
            "Epoch: 1719/10000\n",
            "Training: Batch 1/2, Loss: 1.1210418961127289e-05\n",
            "Training: Batch 2/2, Loss: 1.1082439414167311e-05\n",
            "Validation: Batch 1/1, Loss: 1.0547541933192406e-05\n",
            "Epoch: 1720/10000\n",
            "Training: Batch 1/2, Loss: 1.0941086657112464e-05\n",
            "Training: Batch 2/2, Loss: 1.1146871656819712e-05\n",
            "Validation: Batch 1/1, Loss: 1.0450615263835061e-05\n",
            "Epoch: 1721/10000\n",
            "Training: Batch 1/2, Loss: 1.0677654245228041e-05\n",
            "Training: Batch 2/2, Loss: 1.120731121773133e-05\n",
            "Validation: Batch 1/1, Loss: 1.0354580808780156e-05\n",
            "Epoch: 1722/10000\n",
            "Training: Batch 1/2, Loss: 1.0634615136950742e-05\n",
            "Training: Batch 2/2, Loss: 1.1050030479964335e-05\n",
            "Validation: Batch 1/1, Loss: 1.0259303053317126e-05\n",
            "Epoch: 1723/10000\n",
            "Training: Batch 1/2, Loss: 1.0816343092301395e-05\n",
            "Training: Batch 2/2, Loss: 1.0670529263734352e-05\n",
            "Validation: Batch 1/1, Loss: 1.0164626473851968e-05\n",
            "Epoch: 1724/10000\n",
            "Training: Batch 1/2, Loss: 1.0359590305597521e-05\n",
            "Training: Batch 2/2, Loss: 1.0928301890089642e-05\n",
            "Validation: Batch 1/1, Loss: 1.007089394988725e-05\n",
            "Epoch: 1725/10000\n",
            "Training: Batch 1/2, Loss: 1.065406650013756e-05\n",
            "Training: Batch 2/2, Loss: 1.0439793186378665e-05\n",
            "Validation: Batch 1/1, Loss: 9.977787158277351e-06\n",
            "Epoch: 1726/10000\n",
            "Training: Batch 1/2, Loss: 1.0297960216121282e-05\n",
            "Training: Batch 2/2, Loss: 1.0600509085634258e-05\n",
            "Validation: Batch 1/1, Loss: 9.885610779747367e-06\n",
            "Epoch: 1727/10000\n",
            "Training: Batch 1/2, Loss: 1.0179252058151178e-05\n",
            "Training: Batch 2/2, Loss: 1.0526649020903278e-05\n",
            "Validation: Batch 1/1, Loss: 9.79422839009203e-06\n",
            "Epoch: 1728/10000\n",
            "Training: Batch 1/2, Loss: 1.0253053005726542e-05\n",
            "Training: Batch 2/2, Loss: 1.0262678188155405e-05\n",
            "Validation: Batch 1/1, Loss: 9.703590876597445e-06\n",
            "Epoch: 1729/10000\n",
            "Training: Batch 1/2, Loss: 9.994030733651016e-06\n",
            "Training: Batch 2/2, Loss: 1.0331588782719336e-05\n",
            "Validation: Batch 1/1, Loss: 9.613692782295402e-06\n",
            "Epoch: 1730/10000\n",
            "Training: Batch 1/2, Loss: 1.0156223652302288e-05\n",
            "Training: Batch 2/2, Loss: 9.982733899960294e-06\n",
            "Validation: Batch 1/1, Loss: 9.524606866762042e-06\n",
            "Epoch: 1731/10000\n",
            "Training: Batch 1/2, Loss: 9.773212696018163e-06\n",
            "Training: Batch 2/2, Loss: 1.01784935395699e-05\n",
            "Validation: Batch 1/1, Loss: 9.436374966753647e-06\n",
            "Epoch: 1732/10000\n",
            "Training: Batch 1/2, Loss: 9.826911991694942e-06\n",
            "Training: Batch 2/2, Loss: 9.941167263605166e-06\n",
            "Validation: Batch 1/1, Loss: 9.348789717478212e-06\n",
            "Epoch: 1733/10000\n",
            "Training: Batch 1/2, Loss: 9.756956387718674e-06\n",
            "Training: Batch 2/2, Loss: 9.828338988882024e-06\n",
            "Validation: Batch 1/1, Loss: 9.261959348805249e-06\n",
            "Epoch: 1734/10000\n",
            "Training: Batch 1/2, Loss: 9.751339348440524e-06\n",
            "Training: Batch 2/2, Loss: 9.652969310991466e-06\n",
            "Validation: Batch 1/1, Loss: 9.175932063953951e-06\n",
            "Epoch: 1735/10000\n",
            "Training: Batch 1/2, Loss: 9.775860235095024e-06\n",
            "Training: Batch 2/2, Loss: 9.449107892578468e-06\n",
            "Validation: Batch 1/1, Loss: 9.090560524782632e-06\n",
            "Epoch: 1736/10000\n",
            "Training: Batch 1/2, Loss: 9.770114047569223e-06\n",
            "Training: Batch 2/2, Loss: 9.276805030822288e-06\n",
            "Validation: Batch 1/1, Loss: 9.005893844005186e-06\n",
            "Epoch: 1737/10000\n",
            "Training: Batch 1/2, Loss: 9.66567768045934e-06\n",
            "Training: Batch 2/2, Loss: 9.204163688991684e-06\n",
            "Validation: Batch 1/1, Loss: 8.92189382284414e-06\n",
            "Epoch: 1738/10000\n",
            "Training: Batch 1/2, Loss: 9.642257282393984e-06\n",
            "Training: Batch 2/2, Loss: 9.052436325873714e-06\n",
            "Validation: Batch 1/1, Loss: 8.838607755023986e-06\n",
            "Epoch: 1739/10000\n",
            "Training: Batch 1/2, Loss: 9.061827768164221e-06\n",
            "Training: Batch 2/2, Loss: 9.456835869059432e-06\n",
            "Validation: Batch 1/1, Loss: 8.75632122188108e-06\n",
            "Epoch: 1740/10000\n",
            "Training: Batch 1/2, Loss: 8.985923159343656e-06\n",
            "Training: Batch 2/2, Loss: 9.360816875414457e-06\n",
            "Validation: Batch 1/1, Loss: 8.674718628753908e-06\n",
            "Epoch: 1741/10000\n",
            "Training: Batch 1/2, Loss: 9.008829692902509e-06\n",
            "Training: Batch 2/2, Loss: 9.167822099698242e-06\n",
            "Validation: Batch 1/1, Loss: 8.593669008405413e-06\n",
            "Epoch: 1742/10000\n",
            "Training: Batch 1/2, Loss: 8.94764616532484e-06\n",
            "Training: Batch 2/2, Loss: 9.059815056389198e-06\n",
            "Validation: Batch 1/1, Loss: 8.513385182595812e-06\n",
            "Epoch: 1743/10000\n",
            "Training: Batch 1/2, Loss: 9.090525963983964e-06\n",
            "Training: Batch 2/2, Loss: 8.750010238145478e-06\n",
            "Validation: Batch 1/1, Loss: 8.433652510575484e-06\n",
            "Epoch: 1744/10000\n",
            "Training: Batch 1/2, Loss: 8.790945685177576e-06\n",
            "Training: Batch 2/2, Loss: 8.882141628419049e-06\n",
            "Validation: Batch 1/1, Loss: 8.354710189450998e-06\n",
            "Epoch: 1745/10000\n",
            "Training: Batch 1/2, Loss: 8.72300188348163e-06\n",
            "Training: Batch 2/2, Loss: 8.785159479884896e-06\n",
            "Validation: Batch 1/1, Loss: 8.276483640656807e-06\n",
            "Epoch: 1746/10000\n",
            "Training: Batch 1/2, Loss: 8.799912393442355e-06\n",
            "Training: Batch 2/2, Loss: 8.545477612642571e-06\n",
            "Validation: Batch 1/1, Loss: 8.198838258977048e-06\n",
            "Epoch: 1747/10000\n",
            "Training: Batch 1/2, Loss: 8.294035978906322e-06\n",
            "Training: Batch 2/2, Loss: 8.887393960321788e-06\n",
            "Validation: Batch 1/1, Loss: 8.122078725136817e-06\n",
            "Epoch: 1748/10000\n",
            "Training: Batch 1/2, Loss: 8.61180615174817e-06\n",
            "Training: Batch 2/2, Loss: 8.410723239649087e-06\n",
            "Validation: Batch 1/1, Loss: 8.045715730986558e-06\n",
            "Epoch: 1749/10000\n",
            "Training: Batch 1/2, Loss: 8.619434083811939e-06\n",
            "Training: Batch 2/2, Loss: 8.243912816396914e-06\n",
            "Validation: Batch 1/1, Loss: 7.970021215442102e-06\n",
            "Epoch: 1750/10000\n",
            "Training: Batch 1/2, Loss: 8.676232027937658e-06\n",
            "Training: Batch 2/2, Loss: 8.02941394795198e-06\n",
            "Validation: Batch 1/1, Loss: 7.894905138527974e-06\n",
            "Epoch: 1751/10000\n",
            "Training: Batch 1/2, Loss: 7.952420673973393e-06\n",
            "Training: Batch 2/2, Loss: 8.593583515903447e-06\n",
            "Validation: Batch 1/1, Loss: 7.820735845598392e-06\n",
            "Epoch: 1752/10000\n",
            "Training: Batch 1/2, Loss: 8.204780897358432e-06\n",
            "Training: Batch 2/2, Loss: 8.18755324871745e-06\n",
            "Validation: Batch 1/1, Loss: 7.747086783638224e-06\n",
            "Epoch: 1753/10000\n",
            "Training: Batch 1/2, Loss: 7.950717190396972e-06\n",
            "Training: Batch 2/2, Loss: 8.28693009680137e-06\n",
            "Validation: Batch 1/1, Loss: 7.67416804592358e-06\n",
            "Epoch: 1754/10000\n",
            "Training: Batch 1/2, Loss: 8.13914266473148e-06\n",
            "Training: Batch 2/2, Loss: 7.94694551586872e-06\n",
            "Validation: Batch 1/1, Loss: 7.6016194725525565e-06\n",
            "Epoch: 1755/10000\n",
            "Training: Batch 1/2, Loss: 8.01446003606543e-06\n",
            "Training: Batch 2/2, Loss: 7.919804374978412e-06\n",
            "Validation: Batch 1/1, Loss: 7.529770300607197e-06\n",
            "Epoch: 1756/10000\n",
            "Training: Batch 1/2, Loss: 7.812089279468637e-06\n",
            "Training: Batch 2/2, Loss: 7.97153825260466e-06\n",
            "Validation: Batch 1/1, Loss: 7.4586259870557114e-06\n",
            "Epoch: 1757/10000\n",
            "Training: Batch 1/2, Loss: 8.000710295164026e-06\n",
            "Training: Batch 2/2, Loss: 7.635230758751277e-06\n",
            "Validation: Batch 1/1, Loss: 7.388036465272307e-06\n",
            "Epoch: 1758/10000\n",
            "Training: Batch 1/2, Loss: 7.6124097176943906e-06\n",
            "Training: Batch 2/2, Loss: 7.874571565480437e-06\n",
            "Validation: Batch 1/1, Loss: 7.318082225538092e-06\n",
            "Epoch: 1759/10000\n",
            "Training: Batch 1/2, Loss: 7.762500899843872e-06\n",
            "Training: Batch 2/2, Loss: 7.579217253805837e-06\n",
            "Validation: Batch 1/1, Loss: 7.248682322824607e-06\n",
            "Epoch: 1760/10000\n",
            "Training: Batch 1/2, Loss: 7.503953384002671e-06\n",
            "Training: Batch 2/2, Loss: 7.6919795901631e-06\n",
            "Validation: Batch 1/1, Loss: 7.179964995884802e-06\n",
            "Epoch: 1761/10000\n",
            "Training: Batch 1/2, Loss: 7.705950338277034e-06\n",
            "Training: Batch 2/2, Loss: 7.347518931055674e-06\n",
            "Validation: Batch 1/1, Loss: 7.111840204743203e-06\n",
            "Epoch: 1762/10000\n",
            "Training: Batch 1/2, Loss: 7.413450475723948e-06\n",
            "Training: Batch 2/2, Loss: 7.496524631278589e-06\n",
            "Validation: Batch 1/1, Loss: 7.044336143735563e-06\n",
            "Epoch: 1763/10000\n",
            "Training: Batch 1/2, Loss: 7.350128271355061e-06\n",
            "Training: Batch 2/2, Loss: 7.418593668262474e-06\n",
            "Validation: Batch 1/1, Loss: 6.977434168220498e-06\n",
            "Epoch: 1764/10000\n",
            "Training: Batch 1/2, Loss: 7.381207979051396e-06\n",
            "Training: Batch 2/2, Loss: 7.247998837556224e-06\n",
            "Validation: Batch 1/1, Loss: 6.910966931172879e-06\n",
            "Epoch: 1765/10000\n",
            "Training: Batch 1/2, Loss: 7.179887234087801e-06\n",
            "Training: Batch 2/2, Loss: 7.309905413421802e-06\n",
            "Validation: Batch 1/1, Loss: 6.845261395937996e-06\n",
            "Epoch: 1766/10000\n",
            "Training: Batch 1/2, Loss: 6.961959115869831e-06\n",
            "Training: Batch 2/2, Loss: 7.389673100988148e-06\n",
            "Validation: Batch 1/1, Loss: 6.780131116101984e-06\n",
            "Epoch: 1767/10000\n",
            "Training: Batch 1/2, Loss: 7.021568762866082e-06\n",
            "Training: Batch 2/2, Loss: 7.1943227339943405e-06\n",
            "Validation: Batch 1/1, Loss: 6.715486506436719e-06\n",
            "Epoch: 1768/10000\n",
            "Training: Batch 1/2, Loss: 6.7754185693047475e-06\n",
            "Training: Batch 2/2, Loss: 7.304587597900536e-06\n",
            "Validation: Batch 1/1, Loss: 6.651454896200448e-06\n",
            "Epoch: 1769/10000\n",
            "Training: Batch 1/2, Loss: 7.293480848602485e-06\n",
            "Training: Batch 2/2, Loss: 6.655204742855858e-06\n",
            "Validation: Batch 1/1, Loss: 6.587773896171711e-06\n",
            "Epoch: 1770/10000\n",
            "Training: Batch 1/2, Loss: 6.975245923968032e-06\n",
            "Training: Batch 2/2, Loss: 6.839104571554344e-06\n",
            "Validation: Batch 1/1, Loss: 6.524735908897128e-06\n",
            "Epoch: 1771/10000\n",
            "Training: Batch 1/2, Loss: 7.017080406512832e-06\n",
            "Training: Batch 2/2, Loss: 6.665961791441077e-06\n",
            "Validation: Batch 1/1, Loss: 6.462182227551239e-06\n",
            "Epoch: 1772/10000\n",
            "Training: Batch 1/2, Loss: 6.932388259883737e-06\n",
            "Training: Batch 2/2, Loss: 6.619844953092979e-06\n",
            "Validation: Batch 1/1, Loss: 6.4002565522969235e-06\n",
            "Epoch: 1773/10000\n",
            "Training: Batch 1/2, Loss: 6.566296178789344e-06\n",
            "Training: Batch 2/2, Loss: 6.855054380139336e-06\n",
            "Validation: Batch 1/1, Loss: 6.339068931993097e-06\n",
            "Epoch: 1774/10000\n",
            "Training: Batch 1/2, Loss: 7.024950264167273e-06\n",
            "Training: Batch 2/2, Loss: 6.270571702771122e-06\n",
            "Validation: Batch 1/1, Loss: 6.278140062931925e-06\n",
            "Epoch: 1775/10000\n",
            "Training: Batch 1/2, Loss: 6.140572168078506e-06\n",
            "Training: Batch 2/2, Loss: 7.024058959359536e-06\n",
            "Validation: Batch 1/1, Loss: 6.218177077244036e-06\n",
            "Epoch: 1776/10000\n",
            "Training: Batch 1/2, Loss: 6.531096914841328e-06\n",
            "Training: Batch 2/2, Loss: 6.509958893730072e-06\n",
            "Validation: Batch 1/1, Loss: 6.1585747062054e-06\n",
            "Epoch: 1777/10000\n",
            "Training: Batch 1/2, Loss: 6.528267022076761e-06\n",
            "Training: Batch 2/2, Loss: 6.388141173374606e-06\n",
            "Validation: Batch 1/1, Loss: 6.099370239098789e-06\n",
            "Epoch: 1778/10000\n",
            "Training: Batch 1/2, Loss: 6.375336397468345e-06\n",
            "Training: Batch 2/2, Loss: 6.4168307289946824e-06\n",
            "Validation: Batch 1/1, Loss: 6.040736934664892e-06\n",
            "Epoch: 1779/10000\n",
            "Training: Batch 1/2, Loss: 6.634666078753071e-06\n",
            "Training: Batch 2/2, Loss: 6.03619582761894e-06\n",
            "Validation: Batch 1/1, Loss: 5.982470156595809e-06\n",
            "Epoch: 1780/10000\n",
            "Training: Batch 1/2, Loss: 5.889980457141064e-06\n",
            "Training: Batch 2/2, Loss: 6.656138339167228e-06\n",
            "Validation: Batch 1/1, Loss: 5.925047389609972e-06\n",
            "Epoch: 1781/10000\n",
            "Training: Batch 1/2, Loss: 6.180699529068079e-06\n",
            "Training: Batch 2/2, Loss: 6.246670182008529e-06\n",
            "Validation: Batch 1/1, Loss: 5.868008429388283e-06\n",
            "Epoch: 1782/10000\n",
            "Training: Batch 1/2, Loss: 6.0434226725192275e-06\n",
            "Training: Batch 2/2, Loss: 6.26423025096301e-06\n",
            "Validation: Batch 1/1, Loss: 5.811409664602252e-06\n",
            "Epoch: 1783/10000\n",
            "Training: Batch 1/2, Loss: 6.2849953792465385e-06\n",
            "Training: Batch 2/2, Loss: 5.905482339585433e-06\n",
            "Validation: Batch 1/1, Loss: 5.755262463935651e-06\n",
            "Epoch: 1784/10000\n",
            "Training: Batch 1/2, Loss: 6.230832241271855e-06\n",
            "Training: Batch 2/2, Loss: 5.84209601584007e-06\n",
            "Validation: Batch 1/1, Loss: 5.699582743545761e-06\n",
            "Epoch: 1785/10000\n",
            "Training: Batch 1/2, Loss: 6.054306140867993e-06\n",
            "Training: Batch 2/2, Loss: 5.9015460465161595e-06\n",
            "Validation: Batch 1/1, Loss: 5.644329576171003e-06\n",
            "Epoch: 1786/10000\n",
            "Training: Batch 1/2, Loss: 6.094881427998189e-06\n",
            "Training: Batch 2/2, Loss: 5.745893759012688e-06\n",
            "Validation: Batch 1/1, Loss: 5.589622105617309e-06\n",
            "Epoch: 1787/10000\n",
            "Training: Batch 1/2, Loss: 5.893768957321299e-06\n",
            "Training: Batch 2/2, Loss: 5.831907401443459e-06\n",
            "Validation: Batch 1/1, Loss: 5.5355039876303636e-06\n",
            "Epoch: 1788/10000\n",
            "Training: Batch 1/2, Loss: 5.699556822946761e-06\n",
            "Training: Batch 2/2, Loss: 5.912254891882185e-06\n",
            "Validation: Batch 1/1, Loss: 5.4819652177684475e-06\n",
            "Epoch: 1789/10000\n",
            "Training: Batch 1/2, Loss: 5.7015809034055565e-06\n",
            "Training: Batch 2/2, Loss: 5.79829656999209e-06\n",
            "Validation: Batch 1/1, Loss: 5.428837084764382e-06\n",
            "Epoch: 1790/10000\n",
            "Training: Batch 1/2, Loss: 5.759228770330083e-06\n",
            "Training: Batch 2/2, Loss: 5.629807674267795e-06\n",
            "Validation: Batch 1/1, Loss: 5.376162789616501e-06\n",
            "Epoch: 1791/10000\n",
            "Training: Batch 1/2, Loss: 5.759567557106493e-06\n",
            "Training: Batch 2/2, Loss: 5.519453679880826e-06\n",
            "Validation: Batch 1/1, Loss: 5.323880031937733e-06\n",
            "Epoch: 1792/10000\n",
            "Training: Batch 1/2, Loss: 5.579219759965781e-06\n",
            "Training: Batch 2/2, Loss: 5.5897362472023815e-06\n",
            "Validation: Batch 1/1, Loss: 5.2720984058396425e-06\n",
            "Epoch: 1793/10000\n",
            "Training: Batch 1/2, Loss: 5.5120772231020965e-06\n",
            "Training: Batch 2/2, Loss: 5.548453827941557e-06\n",
            "Validation: Batch 1/1, Loss: 5.220829734753352e-06\n",
            "Epoch: 1794/10000\n",
            "Training: Batch 1/2, Loss: 5.56643544769031e-06\n",
            "Training: Batch 2/2, Loss: 5.38716085429769e-06\n",
            "Validation: Batch 1/1, Loss: 5.169926225789823e-06\n",
            "Epoch: 1795/10000\n",
            "Training: Batch 1/2, Loss: 5.669711754308082e-06\n",
            "Training: Batch 2/2, Loss: 5.178056653676322e-06\n",
            "Validation: Batch 1/1, Loss: 5.119384240970248e-06\n",
            "Epoch: 1796/10000\n",
            "Training: Batch 1/2, Loss: 5.479202172864461e-06\n",
            "Training: Batch 2/2, Loss: 5.262243575998582e-06\n",
            "Validation: Batch 1/1, Loss: 5.069423878012458e-06\n",
            "Epoch: 1797/10000\n",
            "Training: Batch 1/2, Loss: 5.354559107217938e-06\n",
            "Training: Batch 2/2, Loss: 5.281934591039317e-06\n",
            "Validation: Batch 1/1, Loss: 5.019966010877397e-06\n",
            "Epoch: 1798/10000\n",
            "Training: Batch 1/2, Loss: 5.419547505880473e-06\n",
            "Training: Batch 2/2, Loss: 5.113849510962609e-06\n",
            "Validation: Batch 1/1, Loss: 4.970890131517081e-06\n",
            "Epoch: 1799/10000\n",
            "Training: Batch 1/2, Loss: 5.300372777128359e-06\n",
            "Training: Batch 2/2, Loss: 5.130002136866096e-06\n",
            "Validation: Batch 1/1, Loss: 4.922242624161299e-06\n",
            "Epoch: 1800/10000\n",
            "Training: Batch 1/2, Loss: 5.238657649897505e-06\n",
            "Training: Batch 2/2, Loss: 5.089781097922241e-06\n",
            "Validation: Batch 1/1, Loss: 4.87406305182958e-06\n",
            "Epoch: 1801/10000\n",
            "Training: Batch 1/2, Loss: 5.1013394113397226e-06\n",
            "Training: Batch 2/2, Loss: 5.12587939738296e-06\n",
            "Validation: Batch 1/1, Loss: 4.826301392313326e-06\n",
            "Epoch: 1802/10000\n",
            "Training: Batch 1/2, Loss: 4.937910034641391e-06\n",
            "Training: Batch 2/2, Loss: 5.188820523471804e-06\n",
            "Validation: Batch 1/1, Loss: 4.779029495693976e-06\n",
            "Epoch: 1803/10000\n",
            "Training: Batch 1/2, Loss: 4.780002200277522e-06\n",
            "Training: Batch 2/2, Loss: 5.2473255891527515e-06\n",
            "Validation: Batch 1/1, Loss: 4.7322773752966896e-06\n",
            "Epoch: 1804/10000\n",
            "Training: Batch 1/2, Loss: 5.005991170037305e-06\n",
            "Training: Batch 2/2, Loss: 4.924523182125995e-06\n",
            "Validation: Batch 1/1, Loss: 4.685808562499005e-06\n",
            "Epoch: 1805/10000\n",
            "Training: Batch 1/2, Loss: 5.114080067869509e-06\n",
            "Training: Batch 2/2, Loss: 4.719738171843346e-06\n",
            "Validation: Batch 1/1, Loss: 4.6396789912250824e-06\n",
            "Epoch: 1806/10000\n",
            "Training: Batch 1/2, Loss: 4.816880846192362e-06\n",
            "Training: Batch 2/2, Loss: 4.919239472656045e-06\n",
            "Validation: Batch 1/1, Loss: 4.594046913553029e-06\n",
            "Epoch: 1807/10000\n",
            "Training: Batch 1/2, Loss: 4.607801656675292e-06\n",
            "Training: Batch 2/2, Loss: 5.032124590798048e-06\n",
            "Validation: Batch 1/1, Loss: 4.5489505282603204e-06\n",
            "Epoch: 1808/10000\n",
            "Training: Batch 1/2, Loss: 4.814626663574018e-06\n",
            "Training: Batch 2/2, Loss: 4.7318008000729606e-06\n",
            "Validation: Batch 1/1, Loss: 4.5040624172543176e-06\n",
            "Epoch: 1809/10000\n",
            "Training: Batch 1/2, Loss: 4.916919351671822e-06\n",
            "Training: Batch 2/2, Loss: 4.536298547463957e-06\n",
            "Validation: Batch 1/1, Loss: 4.459450337890303e-06\n",
            "Epoch: 1810/10000\n",
            "Training: Batch 1/2, Loss: 4.909031758870697e-06\n",
            "Training: Batch 2/2, Loss: 4.45100522483699e-06\n",
            "Validation: Batch 1/1, Loss: 4.415402145241387e-06\n",
            "Epoch: 1811/10000\n",
            "Training: Batch 1/2, Loss: 4.814331532543292e-06\n",
            "Training: Batch 2/2, Loss: 4.453192559594754e-06\n",
            "Validation: Batch 1/1, Loss: 4.3716991058317944e-06\n",
            "Epoch: 1812/10000\n",
            "Training: Batch 1/2, Loss: 4.459100637177471e-06\n",
            "Training: Batch 2/2, Loss: 4.715566319646314e-06\n",
            "Validation: Batch 1/1, Loss: 4.328586328483652e-06\n",
            "Epoch: 1813/10000\n",
            "Training: Batch 1/2, Loss: 4.530073510977672e-06\n",
            "Training: Batch 2/2, Loss: 4.554742645268561e-06\n",
            "Validation: Batch 1/1, Loss: 4.285847353457939e-06\n",
            "Epoch: 1814/10000\n",
            "Training: Batch 1/2, Loss: 4.452803295862395e-06\n",
            "Training: Batch 2/2, Loss: 4.5421811591950245e-06\n",
            "Validation: Batch 1/1, Loss: 4.243437615514267e-06\n",
            "Epoch: 1815/10000\n",
            "Training: Batch 1/2, Loss: 4.5330652937991545e-06\n",
            "Training: Batch 2/2, Loss: 4.373630417831009e-06\n",
            "Validation: Batch 1/1, Loss: 4.2013120946649e-06\n",
            "Epoch: 1816/10000\n",
            "Training: Batch 1/2, Loss: 4.374408945295727e-06\n",
            "Training: Batch 2/2, Loss: 4.4435705603973474e-06\n",
            "Validation: Batch 1/1, Loss: 4.159656327828998e-06\n",
            "Epoch: 1817/10000\n",
            "Training: Batch 1/2, Loss: 4.354748853074852e-06\n",
            "Training: Batch 2/2, Loss: 4.37596509073046e-06\n",
            "Validation: Batch 1/1, Loss: 4.118368906347314e-06\n",
            "Epoch: 1818/10000\n",
            "Training: Batch 1/2, Loss: 4.3135814848938026e-06\n",
            "Training: Batch 2/2, Loss: 4.330612682679202e-06\n",
            "Validation: Batch 1/1, Loss: 4.077441190020181e-06\n",
            "Epoch: 1819/10000\n",
            "Training: Batch 1/2, Loss: 4.289227490517078e-06\n",
            "Training: Batch 2/2, Loss: 4.269341843610164e-06\n",
            "Validation: Batch 1/1, Loss: 4.036889095004881e-06\n",
            "Epoch: 1820/10000\n",
            "Training: Batch 1/2, Loss: 4.184256795269903e-06\n",
            "Training: Batch 2/2, Loss: 4.289129719836637e-06\n",
            "Validation: Batch 1/1, Loss: 3.99678810936166e-06\n",
            "Epoch: 1821/10000\n",
            "Training: Batch 1/2, Loss: 4.194072971586138e-06\n",
            "Training: Batch 2/2, Loss: 4.195441306364955e-06\n",
            "Validation: Batch 1/1, Loss: 3.956927230319707e-06\n",
            "Epoch: 1822/10000\n",
            "Training: Batch 1/2, Loss: 4.2555884647299536e-06\n",
            "Training: Batch 2/2, Loss: 4.050887582707219e-06\n",
            "Validation: Batch 1/1, Loss: 3.917448793799849e-06\n",
            "Epoch: 1823/10000\n",
            "Training: Batch 1/2, Loss: 4.179534244030947e-06\n",
            "Training: Batch 2/2, Loss: 4.044049092044588e-06\n",
            "Validation: Batch 1/1, Loss: 3.878325514961034e-06\n",
            "Epoch: 1824/10000\n",
            "Training: Batch 1/2, Loss: 4.026688202429796e-06\n",
            "Training: Batch 2/2, Loss: 4.114422608836321e-06\n",
            "Validation: Batch 1/1, Loss: 3.83963879357907e-06\n",
            "Epoch: 1825/10000\n",
            "Training: Batch 1/2, Loss: 3.953136911150068e-06\n",
            "Training: Batch 2/2, Loss: 4.10678376283613e-06\n",
            "Validation: Batch 1/1, Loss: 3.8012653931218665e-06\n",
            "Epoch: 1826/10000\n",
            "Training: Batch 1/2, Loss: 4.060131686856039e-06\n",
            "Training: Batch 2/2, Loss: 3.920024937542621e-06\n",
            "Validation: Batch 1/1, Loss: 3.7632410112564685e-06\n",
            "Epoch: 1827/10000\n",
            "Training: Batch 1/2, Loss: 3.851361725537572e-06\n",
            "Training: Batch 2/2, Loss: 4.048301889270078e-06\n",
            "Validation: Batch 1/1, Loss: 3.725547230715165e-06\n",
            "Epoch: 1828/10000\n",
            "Training: Batch 1/2, Loss: 4.0058530430542305e-06\n",
            "Training: Batch 2/2, Loss: 3.8156968003022484e-06\n",
            "Validation: Batch 1/1, Loss: 3.6881617688777624e-06\n",
            "Epoch: 1829/10000\n",
            "Training: Batch 1/2, Loss: 3.886024387611542e-06\n",
            "Training: Batch 2/2, Loss: 3.8568114177905954e-06\n",
            "Validation: Batch 1/1, Loss: 3.651178076324868e-06\n",
            "Epoch: 1830/10000\n",
            "Training: Batch 1/2, Loss: 3.912887677870458e-06\n",
            "Training: Batch 2/2, Loss: 3.7527317999774823e-06\n",
            "Validation: Batch 1/1, Loss: 3.6144913337921025e-06\n",
            "Epoch: 1831/10000\n",
            "Training: Batch 1/2, Loss: 3.632696007116465e-06\n",
            "Training: Batch 2/2, Loss: 3.954934527428122e-06\n",
            "Validation: Batch 1/1, Loss: 3.5782454688160215e-06\n",
            "Epoch: 1832/10000\n",
            "Training: Batch 1/2, Loss: 3.7691634133807383e-06\n",
            "Training: Batch 2/2, Loss: 3.7432944282045355e-06\n",
            "Validation: Batch 1/1, Loss: 3.5422385735728312e-06\n",
            "Epoch: 1833/10000\n",
            "Training: Batch 1/2, Loss: 3.6472602005233057e-06\n",
            "Training: Batch 2/2, Loss: 3.7893676108069485e-06\n",
            "Validation: Batch 1/1, Loss: 3.506594794089324e-06\n",
            "Epoch: 1834/10000\n",
            "Training: Batch 1/2, Loss: 3.7641591461579083e-06\n",
            "Training: Batch 2/2, Loss: 3.598453531594714e-06\n",
            "Validation: Batch 1/1, Loss: 3.4712193155428395e-06\n",
            "Epoch: 1835/10000\n",
            "Training: Batch 1/2, Loss: 3.54916596734256e-06\n",
            "Training: Batch 2/2, Loss: 3.7385066207207274e-06\n",
            "Validation: Batch 1/1, Loss: 3.4363340546406107e-06\n",
            "Epoch: 1836/10000\n",
            "Training: Batch 1/2, Loss: 3.6168130463920534e-06\n",
            "Training: Batch 2/2, Loss: 3.5981124710815493e-06\n",
            "Validation: Batch 1/1, Loss: 3.40163251166814e-06\n",
            "Epoch: 1837/10000\n",
            "Training: Batch 1/2, Loss: 3.575140681277844e-06\n",
            "Training: Batch 2/2, Loss: 3.567009798643994e-06\n",
            "Validation: Batch 1/1, Loss: 3.367273393450887e-06\n",
            "Epoch: 1838/10000\n",
            "Training: Batch 1/2, Loss: 3.47131958733371e-06\n",
            "Training: Batch 2/2, Loss: 3.5984830901725218e-06\n",
            "Validation: Batch 1/1, Loss: 3.333296035634703e-06\n",
            "Epoch: 1839/10000\n",
            "Training: Batch 1/2, Loss: 3.4603306175995385e-06\n",
            "Training: Batch 2/2, Loss: 3.5383072827244177e-06\n",
            "Validation: Batch 1/1, Loss: 3.299587433502893e-06\n",
            "Epoch: 1840/10000\n",
            "Training: Batch 1/2, Loss: 3.5384462080401136e-06\n",
            "Training: Batch 2/2, Loss: 3.390072834008606e-06\n",
            "Validation: Batch 1/1, Loss: 3.2661864679539576e-06\n",
            "Epoch: 1841/10000\n",
            "Training: Batch 1/2, Loss: 3.2058951546787284e-06\n",
            "Training: Batch 2/2, Loss: 3.651208317023702e-06\n",
            "Validation: Batch 1/1, Loss: 3.2331699912901968e-06\n",
            "Epoch: 1842/10000\n",
            "Training: Batch 1/2, Loss: 3.4450972634658683e-06\n",
            "Training: Batch 2/2, Loss: 3.343980552017456e-06\n",
            "Validation: Batch 1/1, Loss: 3.200361334165791e-06\n",
            "Epoch: 1843/10000\n",
            "Training: Batch 1/2, Loss: 3.3784758670662995e-06\n",
            "Training: Batch 2/2, Loss: 3.3416840778954793e-06\n",
            "Validation: Batch 1/1, Loss: 3.1678237064625137e-06\n",
            "Epoch: 1844/10000\n",
            "Training: Batch 1/2, Loss: 3.29836507262371e-06\n",
            "Training: Batch 2/2, Loss: 3.353460215294035e-06\n",
            "Validation: Batch 1/1, Loss: 3.135715815005824e-06\n",
            "Epoch: 1845/10000\n",
            "Training: Batch 1/2, Loss: 3.340100420246017e-06\n",
            "Training: Batch 2/2, Loss: 3.244634854127071e-06\n",
            "Validation: Batch 1/1, Loss: 3.1037989174365066e-06\n",
            "Epoch: 1846/10000\n",
            "Training: Batch 1/2, Loss: 3.1964734716893872e-06\n",
            "Training: Batch 2/2, Loss: 3.320838004583493e-06\n",
            "Validation: Batch 1/1, Loss: 3.0722744668310042e-06\n",
            "Epoch: 1847/10000\n",
            "Training: Batch 1/2, Loss: 3.2470688893226907e-06\n",
            "Training: Batch 2/2, Loss: 3.2044249564933125e-06\n",
            "Validation: Batch 1/1, Loss: 3.0409883038373664e-06\n",
            "Epoch: 1848/10000\n",
            "Training: Batch 1/2, Loss: 3.2668647236278048e-06\n",
            "Training: Batch 2/2, Loss: 3.119295115538989e-06\n",
            "Validation: Batch 1/1, Loss: 3.0099517971393652e-06\n",
            "Epoch: 1849/10000\n",
            "Training: Batch 1/2, Loss: 3.117363121418748e-06\n",
            "Training: Batch 2/2, Loss: 3.203216692782007e-06\n",
            "Validation: Batch 1/1, Loss: 2.979261125801713e-06\n",
            "Epoch: 1850/10000\n",
            "Training: Batch 1/2, Loss: 3.2193256629398093e-06\n",
            "Training: Batch 2/2, Loss: 3.0375169899343746e-06\n",
            "Validation: Batch 1/1, Loss: 2.948761675725109e-06\n",
            "Epoch: 1851/10000\n",
            "Training: Batch 1/2, Loss: 3.10004293169186e-06\n",
            "Training: Batch 2/2, Loss: 3.092538463533856e-06\n",
            "Validation: Batch 1/1, Loss: 2.9186232950451085e-06\n",
            "Epoch: 1852/10000\n",
            "Training: Batch 1/2, Loss: 2.923848796854145e-06\n",
            "Training: Batch 2/2, Loss: 3.2048799312178744e-06\n",
            "Validation: Batch 1/1, Loss: 2.888843482651282e-06\n",
            "Epoch: 1853/10000\n",
            "Training: Batch 1/2, Loss: 3.100580215686932e-06\n",
            "Training: Batch 2/2, Loss: 2.9666509817616316e-06\n",
            "Validation: Batch 1/1, Loss: 2.859257847376284e-06\n",
            "Epoch: 1854/10000\n",
            "Training: Batch 1/2, Loss: 3.1186150408757385e-06\n",
            "Training: Batch 2/2, Loss: 2.8867907531093806e-06\n",
            "Validation: Batch 1/1, Loss: 2.8299159566813614e-06\n",
            "Epoch: 1855/10000\n",
            "Training: Batch 1/2, Loss: 3.084696345467819e-06\n",
            "Training: Batch 2/2, Loss: 2.859121195797343e-06\n",
            "Validation: Batch 1/1, Loss: 2.8008141725877067e-06\n",
            "Epoch: 1856/10000\n",
            "Training: Batch 1/2, Loss: 2.944616198874428e-06\n",
            "Training: Batch 2/2, Loss: 2.9376669772318564e-06\n",
            "Validation: Batch 1/1, Loss: 2.772052084765164e-06\n",
            "Epoch: 1857/10000\n",
            "Training: Batch 1/2, Loss: 2.842695948856999e-06\n",
            "Training: Batch 2/2, Loss: 2.9789407562930137e-06\n",
            "Validation: Batch 1/1, Loss: 2.743641516644857e-06\n",
            "Epoch: 1858/10000\n",
            "Training: Batch 1/2, Loss: 2.999211801579804e-06\n",
            "Training: Batch 2/2, Loss: 2.763690190477064e-06\n",
            "Validation: Batch 1/1, Loss: 2.715408754738746e-06\n",
            "Epoch: 1859/10000\n",
            "Training: Batch 1/2, Loss: 2.870332082238747e-06\n",
            "Training: Batch 2/2, Loss: 2.832860218404676e-06\n",
            "Validation: Batch 1/1, Loss: 2.68745020548522e-06\n",
            "Epoch: 1860/10000\n",
            "Training: Batch 1/2, Loss: 2.8502797704277327e-06\n",
            "Training: Batch 2/2, Loss: 2.794331066979794e-06\n",
            "Validation: Batch 1/1, Loss: 2.659782012415235e-06\n",
            "Epoch: 1861/10000\n",
            "Training: Batch 1/2, Loss: 2.903376525864587e-06\n",
            "Training: Batch 2/2, Loss: 2.6835446078621317e-06\n",
            "Validation: Batch 1/1, Loss: 2.632335508678807e-06\n",
            "Epoch: 1862/10000\n",
            "Training: Batch 1/2, Loss: 2.8861929877166403e-06\n",
            "Training: Batch 2/2, Loss: 2.643209427333204e-06\n",
            "Validation: Batch 1/1, Loss: 2.605170038805227e-06\n",
            "Epoch: 1863/10000\n",
            "Training: Batch 1/2, Loss: 2.710538410610752e-06\n",
            "Training: Batch 2/2, Loss: 2.761211590041057e-06\n",
            "Validation: Batch 1/1, Loss: 2.5783010642044246e-06\n",
            "Epoch: 1864/10000\n",
            "Training: Batch 1/2, Loss: 2.6635193535184953e-06\n",
            "Training: Batch 2/2, Loss: 2.7517608032212593e-06\n",
            "Validation: Batch 1/1, Loss: 2.5516958430671366e-06\n",
            "Epoch: 1865/10000\n",
            "Training: Batch 1/2, Loss: 2.754086608547368e-06\n",
            "Training: Batch 2/2, Loss: 2.6059183255711105e-06\n",
            "Validation: Batch 1/1, Loss: 2.5253054900531424e-06\n",
            "Epoch: 1866/10000\n",
            "Training: Batch 1/2, Loss: 2.6576610707707005e-06\n",
            "Training: Batch 2/2, Loss: 2.6466236704436596e-06\n",
            "Validation: Batch 1/1, Loss: 2.499152515156311e-06\n",
            "Epoch: 1867/10000\n",
            "Training: Batch 1/2, Loss: 2.585410811661859e-06\n",
            "Training: Batch 2/2, Loss: 2.6638338113116333e-06\n",
            "Validation: Batch 1/1, Loss: 2.473269660185906e-06\n",
            "Epoch: 1868/10000\n",
            "Training: Batch 1/2, Loss: 2.6657633043214446e-06\n",
            "Training: Batch 2/2, Loss: 2.5296756120951613e-06\n",
            "Validation: Batch 1/1, Loss: 2.4476064481859794e-06\n",
            "Epoch: 1869/10000\n",
            "Training: Batch 1/2, Loss: 2.6053160127048614e-06\n",
            "Training: Batch 2/2, Loss: 2.5361737243656535e-06\n",
            "Validation: Batch 1/1, Loss: 2.4221858438977506e-06\n",
            "Epoch: 1870/10000\n",
            "Training: Batch 1/2, Loss: 2.5431959329580422e-06\n",
            "Training: Batch 2/2, Loss: 2.544785047575715e-06\n",
            "Validation: Batch 1/1, Loss: 2.3970242182258517e-06\n",
            "Epoch: 1871/10000\n",
            "Training: Batch 1/2, Loss: 2.56007206189679e-06\n",
            "Training: Batch 2/2, Loss: 2.4753178422542987e-06\n",
            "Validation: Batch 1/1, Loss: 2.372083827140159e-06\n",
            "Epoch: 1872/10000\n",
            "Training: Batch 1/2, Loss: 2.636429144331487e-06\n",
            "Training: Batch 2/2, Loss: 2.347152076254133e-06\n",
            "Validation: Batch 1/1, Loss: 2.347370582356234e-06\n",
            "Epoch: 1873/10000\n",
            "Training: Batch 1/2, Loss: 2.4636647140141577e-06\n",
            "Training: Batch 2/2, Loss: 2.4673499865457416e-06\n",
            "Validation: Batch 1/1, Loss: 2.3229038106364897e-06\n",
            "Epoch: 1874/10000\n",
            "Training: Batch 1/2, Loss: 2.5476597329543438e-06\n",
            "Training: Batch 2/2, Loss: 2.3325285383180017e-06\n",
            "Validation: Batch 1/1, Loss: 2.2986125713941874e-06\n",
            "Epoch: 1875/10000\n",
            "Training: Batch 1/2, Loss: 2.3461925593437627e-06\n",
            "Training: Batch 2/2, Loss: 2.4822948034852743e-06\n",
            "Validation: Batch 1/1, Loss: 2.274674670843524e-06\n",
            "Epoch: 1876/10000\n",
            "Training: Batch 1/2, Loss: 2.5395149805262918e-06\n",
            "Training: Batch 2/2, Loss: 2.239716422991478e-06\n",
            "Validation: Batch 1/1, Loss: 2.250850002383231e-06\n",
            "Epoch: 1877/10000\n",
            "Training: Batch 1/2, Loss: 2.4103101168293506e-06\n",
            "Training: Batch 2/2, Loss: 2.3184734345704783e-06\n",
            "Validation: Batch 1/1, Loss: 2.2273209197010146e-06\n",
            "Epoch: 1878/10000\n",
            "Training: Batch 1/2, Loss: 2.311779780939105e-06\n",
            "Training: Batch 2/2, Loss: 2.367261686231359e-06\n",
            "Validation: Batch 1/1, Loss: 2.20402876038861e-06\n",
            "Epoch: 1879/10000\n",
            "Training: Batch 1/2, Loss: 2.340628270758316e-06\n",
            "Training: Batch 2/2, Loss: 2.289808890054701e-06\n",
            "Validation: Batch 1/1, Loss: 2.180940100515727e-06\n",
            "Epoch: 1880/10000\n",
            "Training: Batch 1/2, Loss: 2.2706024083163356e-06\n",
            "Training: Batch 2/2, Loss: 2.3111613245418994e-06\n",
            "Validation: Batch 1/1, Loss: 2.1581383862212533e-06\n",
            "Epoch: 1881/10000\n",
            "Training: Batch 1/2, Loss: 2.2159181298775366e-06\n",
            "Training: Batch 2/2, Loss: 2.317827465958544e-06\n",
            "Validation: Batch 1/1, Loss: 2.1355247099563712e-06\n",
            "Epoch: 1882/10000\n",
            "Training: Batch 1/2, Loss: 2.312841616003425e-06\n",
            "Training: Batch 2/2, Loss: 2.173984512410243e-06\n",
            "Validation: Batch 1/1, Loss: 2.1130697405169485e-06\n",
            "Epoch: 1883/10000\n",
            "Training: Batch 1/2, Loss: 2.2291030745691387e-06\n",
            "Training: Batch 2/2, Loss: 2.210317461504019e-06\n",
            "Validation: Batch 1/1, Loss: 2.0908958049403736e-06\n",
            "Epoch: 1884/10000\n",
            "Training: Batch 1/2, Loss: 2.2139918200991815e-06\n",
            "Training: Batch 2/2, Loss: 2.178911699957098e-06\n",
            "Validation: Batch 1/1, Loss: 2.068890353257302e-06\n",
            "Epoch: 1885/10000\n",
            "Training: Batch 1/2, Loss: 2.1868086150789168e-06\n",
            "Training: Batch 2/2, Loss: 2.1598730199912097e-06\n",
            "Validation: Batch 1/1, Loss: 2.0470874915190507e-06\n",
            "Epoch: 1886/10000\n",
            "Training: Batch 1/2, Loss: 2.1420059965748806e-06\n",
            "Training: Batch 2/2, Loss: 2.1588305116893025e-06\n",
            "Validation: Batch 1/1, Loss: 2.0255299659766024e-06\n",
            "Epoch: 1887/10000\n",
            "Training: Batch 1/2, Loss: 2.110853301928728e-06\n",
            "Training: Batch 2/2, Loss: 2.1446981008921284e-06\n",
            "Validation: Batch 1/1, Loss: 2.0041500192746753e-06\n",
            "Epoch: 1888/10000\n",
            "Training: Batch 1/2, Loss: 2.051041292361333e-06\n",
            "Training: Batch 2/2, Loss: 2.1594707959593507e-06\n",
            "Validation: Batch 1/1, Loss: 1.983014271900174e-06\n",
            "Epoch: 1889/10000\n",
            "Training: Batch 1/2, Loss: 2.109540673700394e-06\n",
            "Training: Batch 2/2, Loss: 2.0570137166942004e-06\n",
            "Validation: Batch 1/1, Loss: 1.9620413240772905e-06\n",
            "Epoch: 1890/10000\n",
            "Training: Batch 1/2, Loss: 2.112407628374058e-06\n",
            "Training: Batch 2/2, Loss: 2.01023135559808e-06\n",
            "Validation: Batch 1/1, Loss: 1.9412379970162874e-06\n",
            "Epoch: 1891/10000\n",
            "Training: Batch 1/2, Loss: 2.072015831799945e-06\n",
            "Training: Batch 2/2, Loss: 2.006878048632643e-06\n",
            "Validation: Batch 1/1, Loss: 1.9206654542358592e-06\n",
            "Epoch: 1892/10000\n",
            "Training: Batch 1/2, Loss: 2.076781129289884e-06\n",
            "Training: Batch 2/2, Loss: 1.9590434021665715e-06\n",
            "Validation: Batch 1/1, Loss: 1.9002779936272418e-06\n",
            "Epoch: 1893/10000\n",
            "Training: Batch 1/2, Loss: 2.014350684476085e-06\n",
            "Training: Batch 2/2, Loss: 1.9785084077739157e-06\n",
            "Validation: Batch 1/1, Loss: 1.8801016494762735e-06\n",
            "Epoch: 1894/10000\n",
            "Training: Batch 1/2, Loss: 1.9971287201769883e-06\n",
            "Training: Batch 2/2, Loss: 1.953409082489088e-06\n",
            "Validation: Batch 1/1, Loss: 1.860151542132371e-06\n",
            "Epoch: 1895/10000\n",
            "Training: Batch 1/2, Loss: 1.9291269381938037e-06\n",
            "Training: Batch 2/2, Loss: 1.9793033061432652e-06\n",
            "Validation: Batch 1/1, Loss: 1.8404261936666444e-06\n",
            "Epoch: 1896/10000\n",
            "Training: Batch 1/2, Loss: 1.9556171082513174e-06\n",
            "Training: Batch 2/2, Loss: 1.9116118892270606e-06\n",
            "Validation: Batch 1/1, Loss: 1.8208080518888892e-06\n",
            "Epoch: 1897/10000\n",
            "Training: Batch 1/2, Loss: 1.9554017853806727e-06\n",
            "Training: Batch 2/2, Loss: 1.8707439721765695e-06\n",
            "Validation: Batch 1/1, Loss: 1.8013955696005723e-06\n",
            "Epoch: 1898/10000\n",
            "Training: Batch 1/2, Loss: 1.9193548723706044e-06\n",
            "Training: Batch 2/2, Loss: 1.8659920897334814e-06\n",
            "Validation: Batch 1/1, Loss: 1.782217736945313e-06\n",
            "Epoch: 1899/10000\n",
            "Training: Batch 1/2, Loss: 1.8984663938681479e-06\n",
            "Training: Batch 2/2, Loss: 1.8466089386492968e-06\n",
            "Validation: Batch 1/1, Loss: 1.7632105482334737e-06\n",
            "Epoch: 1900/10000\n",
            "Training: Batch 1/2, Loss: 1.8423941128276056e-06\n",
            "Training: Batch 2/2, Loss: 1.862592853285605e-06\n",
            "Validation: Batch 1/1, Loss: 1.7443580873077735e-06\n",
            "Epoch: 1901/10000\n",
            "Training: Batch 1/2, Loss: 1.817101633605489e-06\n",
            "Training: Batch 2/2, Loss: 1.8483361827748013e-06\n",
            "Validation: Batch 1/1, Loss: 1.7257613080801093e-06\n",
            "Epoch: 1902/10000\n",
            "Training: Batch 1/2, Loss: 1.811153538255894e-06\n",
            "Training: Batch 2/2, Loss: 1.8152722986997105e-06\n",
            "Validation: Batch 1/1, Loss: 1.7073215303753386e-06\n",
            "Epoch: 1903/10000\n",
            "Training: Batch 1/2, Loss: 1.7515628769615432e-06\n",
            "Training: Batch 2/2, Loss: 1.8359563682679436e-06\n",
            "Validation: Batch 1/1, Loss: 1.689103896751476e-06\n",
            "Epoch: 1904/10000\n",
            "Training: Batch 1/2, Loss: 1.7933807612280361e-06\n",
            "Training: Batch 2/2, Loss: 1.7561003460286884e-06\n",
            "Validation: Batch 1/1, Loss: 1.670968913458637e-06\n",
            "Epoch: 1905/10000\n",
            "Training: Batch 1/2, Loss: 1.7218261518792133e-06\n",
            "Training: Batch 2/2, Loss: 1.789319071576756e-06\n",
            "Validation: Batch 1/1, Loss: 1.65304788879439e-06\n",
            "Epoch: 1906/10000\n",
            "Training: Batch 1/2, Loss: 1.7107813619077206e-06\n",
            "Training: Batch 2/2, Loss: 1.7627736497161095e-06\n",
            "Validation: Batch 1/1, Loss: 1.6353186538253794e-06\n",
            "Epoch: 1907/10000\n",
            "Training: Batch 1/2, Loss: 1.736253125272924e-06\n",
            "Training: Batch 2/2, Loss: 1.700309098850994e-06\n",
            "Validation: Batch 1/1, Loss: 1.6177014003915247e-06\n",
            "Epoch: 1908/10000\n",
            "Training: Batch 1/2, Loss: 1.7126587863458553e-06\n",
            "Training: Batch 2/2, Loss: 1.686933728706208e-06\n",
            "Validation: Batch 1/1, Loss: 1.6003282325982582e-06\n",
            "Epoch: 1909/10000\n",
            "Training: Batch 1/2, Loss: 1.6197220702451887e-06\n",
            "Training: Batch 2/2, Loss: 1.7430255638828385e-06\n",
            "Validation: Batch 1/1, Loss: 1.583155039952544e-06\n",
            "Epoch: 1910/10000\n",
            "Training: Batch 1/2, Loss: 1.5850037016207352e-06\n",
            "Training: Batch 2/2, Loss: 1.741574806146673e-06\n",
            "Validation: Batch 1/1, Loss: 1.5661062207072973e-06\n",
            "Epoch: 1911/10000\n",
            "Training: Batch 1/2, Loss: 1.652381797612179e-06\n",
            "Training: Batch 2/2, Loss: 1.6388303265557624e-06\n",
            "Validation: Batch 1/1, Loss: 1.5492289549001725e-06\n",
            "Epoch: 1912/10000\n",
            "Training: Batch 1/2, Loss: 1.6945432435022667e-06\n",
            "Training: Batch 2/2, Loss: 1.5615377151334542e-06\n",
            "Validation: Batch 1/1, Loss: 1.5324548030548613e-06\n",
            "Epoch: 1913/10000\n",
            "Training: Batch 1/2, Loss: 1.5640397350580315e-06\n",
            "Training: Batch 2/2, Loss: 1.6563083136134082e-06\n",
            "Validation: Batch 1/1, Loss: 1.5159163240241469e-06\n",
            "Epoch: 1914/10000\n",
            "Training: Batch 1/2, Loss: 1.5473680150535074e-06\n",
            "Training: Batch 2/2, Loss: 1.6382705325668212e-06\n",
            "Validation: Batch 1/1, Loss: 1.499565541962511e-06\n",
            "Epoch: 1915/10000\n",
            "Training: Batch 1/2, Loss: 1.5913147990431753e-06\n",
            "Training: Batch 2/2, Loss: 1.5602622625010554e-06\n",
            "Validation: Batch 1/1, Loss: 1.4833141221970436e-06\n",
            "Epoch: 1916/10000\n",
            "Training: Batch 1/2, Loss: 1.5688640360167483e-06\n",
            "Training: Batch 2/2, Loss: 1.5485845779039664e-06\n",
            "Validation: Batch 1/1, Loss: 1.4672434645035537e-06\n",
            "Epoch: 1917/10000\n",
            "Training: Batch 1/2, Loss: 1.5507116586377379e-06\n",
            "Training: Batch 2/2, Loss: 1.5329750340242754e-06\n",
            "Validation: Batch 1/1, Loss: 1.4513398127746768e-06\n",
            "Epoch: 1918/10000\n",
            "Training: Batch 1/2, Loss: 1.5897162484179717e-06\n",
            "Training: Batch 2/2, Loss: 1.4608432366003399e-06\n",
            "Validation: Batch 1/1, Loss: 1.4355581470226753e-06\n",
            "Epoch: 1919/10000\n",
            "Training: Batch 1/2, Loss: 1.4839503137409338e-06\n",
            "Training: Batch 2/2, Loss: 1.5330202813856886e-06\n",
            "Validation: Batch 1/1, Loss: 1.4199844144968665e-06\n",
            "Epoch: 1920/10000\n",
            "Training: Batch 1/2, Loss: 1.5099448091859813e-06\n",
            "Training: Batch 2/2, Loss: 1.4745012322237017e-06\n",
            "Validation: Batch 1/1, Loss: 1.4045280067875865e-06\n",
            "Epoch: 1921/10000\n",
            "Training: Batch 1/2, Loss: 1.490545969318191e-06\n",
            "Training: Batch 2/2, Loss: 1.4614641941079753e-06\n",
            "Validation: Batch 1/1, Loss: 1.3892315564589808e-06\n",
            "Epoch: 1922/10000\n",
            "Training: Batch 1/2, Loss: 1.4409074537979905e-06\n",
            "Training: Batch 2/2, Loss: 1.4788042790314648e-06\n",
            "Validation: Batch 1/1, Loss: 1.374117687191756e-06\n",
            "Epoch: 1923/10000\n",
            "Training: Batch 1/2, Loss: 1.4978523950048839e-06\n",
            "Training: Batch 2/2, Loss: 1.3904771094530588e-06\n",
            "Validation: Batch 1/1, Loss: 1.3591356946562883e-06\n",
            "Epoch: 1924/10000\n",
            "Training: Batch 1/2, Loss: 1.4218187516235048e-06\n",
            "Training: Batch 2/2, Loss: 1.4347208434628556e-06\n",
            "Validation: Batch 1/1, Loss: 1.344326278740482e-06\n",
            "Epoch: 1925/10000\n",
            "Training: Batch 1/2, Loss: 1.3668097835761728e-06\n",
            "Training: Batch 2/2, Loss: 1.4584067002942902e-06\n",
            "Validation: Batch 1/1, Loss: 1.329687393081258e-06\n",
            "Epoch: 1926/10000\n",
            "Training: Batch 1/2, Loss: 1.389007707075507e-06\n",
            "Training: Batch 2/2, Loss: 1.4056500958758988e-06\n",
            "Validation: Batch 1/1, Loss: 1.3151621942597558e-06\n",
            "Epoch: 1927/10000\n",
            "Training: Batch 1/2, Loss: 1.416950794919103e-06\n",
            "Training: Batch 2/2, Loss: 1.347352736047469e-06\n",
            "Validation: Batch 1/1, Loss: 1.3007439747525495e-06\n",
            "Epoch: 1928/10000\n",
            "Training: Batch 1/2, Loss: 1.374477847093658e-06\n",
            "Training: Batch 2/2, Loss: 1.3594309393738513e-06\n",
            "Validation: Batch 1/1, Loss: 1.2864729797001928e-06\n",
            "Epoch: 1929/10000\n",
            "Training: Batch 1/2, Loss: 1.2940098486069473e-06\n",
            "Training: Batch 2/2, Loss: 1.4096469840296777e-06\n",
            "Validation: Batch 1/1, Loss: 1.2724057114610332e-06\n",
            "Epoch: 1930/10000\n",
            "Training: Batch 1/2, Loss: 1.4326279824672383e-06\n",
            "Training: Batch 2/2, Loss: 1.2422239024090231e-06\n",
            "Validation: Batch 1/1, Loss: 1.2584006299221073e-06\n",
            "Epoch: 1931/10000\n",
            "Training: Batch 1/2, Loss: 1.347409124718979e-06\n",
            "Training: Batch 2/2, Loss: 1.2976852303836495e-06\n",
            "Validation: Batch 1/1, Loss: 1.2445649417713867e-06\n",
            "Epoch: 1932/10000\n",
            "Training: Batch 1/2, Loss: 1.2922095038447878e-06\n",
            "Training: Batch 2/2, Loss: 1.3236383438197663e-06\n",
            "Validation: Batch 1/1, Loss: 1.2309041039770818e-06\n",
            "Epoch: 1933/10000\n",
            "Training: Batch 1/2, Loss: 1.2857833553425735e-06\n",
            "Training: Batch 2/2, Loss: 1.3014100659347605e-06\n",
            "Validation: Batch 1/1, Loss: 1.2173628647360601e-06\n",
            "Epoch: 1934/10000\n",
            "Training: Batch 1/2, Loss: 1.291061039410124e-06\n",
            "Training: Batch 2/2, Loss: 1.2677865015575662e-06\n",
            "Validation: Batch 1/1, Loss: 1.2039395187457558e-06\n",
            "Epoch: 1935/10000\n",
            "Training: Batch 1/2, Loss: 1.225778419211565e-06\n",
            "Training: Batch 2/2, Loss: 1.3046329740973306e-06\n",
            "Validation: Batch 1/1, Loss: 1.1906896588698146e-06\n",
            "Epoch: 1936/10000\n",
            "Training: Batch 1/2, Loss: 1.3149217465979746e-06\n",
            "Training: Batch 2/2, Loss: 1.1881794534929213e-06\n",
            "Validation: Batch 1/1, Loss: 1.1774953918575193e-06\n",
            "Epoch: 1937/10000\n",
            "Training: Batch 1/2, Loss: 1.1769490129154292e-06\n",
            "Training: Batch 2/2, Loss: 1.297842231906543e-06\n",
            "Validation: Batch 1/1, Loss: 1.1645292943285313e-06\n",
            "Epoch: 1938/10000\n",
            "Training: Batch 1/2, Loss: 1.2139875025241054e-06\n",
            "Training: Batch 2/2, Loss: 1.2337816315266537e-06\n",
            "Validation: Batch 1/1, Loss: 1.151649826169887e-06\n",
            "Epoch: 1939/10000\n",
            "Training: Batch 1/2, Loss: 1.2446211030692211e-06\n",
            "Training: Batch 2/2, Loss: 1.176312025563675e-06\n",
            "Validation: Batch 1/1, Loss: 1.138903485298215e-06\n",
            "Epoch: 1940/10000\n",
            "Training: Batch 1/2, Loss: 1.2246817959749023e-06\n",
            "Training: Batch 2/2, Loss: 1.1694534123307676e-06\n",
            "Validation: Batch 1/1, Loss: 1.1262897032793262e-06\n",
            "Epoch: 1941/10000\n",
            "Training: Batch 1/2, Loss: 1.130202917920542e-06\n",
            "Training: Batch 2/2, Loss: 1.2370124977678643e-06\n",
            "Validation: Batch 1/1, Loss: 1.1138478157590725e-06\n",
            "Epoch: 1942/10000\n",
            "Training: Batch 1/2, Loss: 1.2109485396649688e-06\n",
            "Training: Batch 2/2, Loss: 1.1305544376227772e-06\n",
            "Validation: Batch 1/1, Loss: 1.1014645906470832e-06\n",
            "Epoch: 1943/10000\n",
            "Training: Batch 1/2, Loss: 1.1366396392986644e-06\n",
            "Training: Batch 2/2, Loss: 1.1785639344452647e-06\n",
            "Validation: Batch 1/1, Loss: 1.089208353732829e-06\n",
            "Epoch: 1944/10000\n",
            "Training: Batch 1/2, Loss: 1.1607564829319017e-06\n",
            "Training: Batch 2/2, Loss: 1.1289331496300292e-06\n",
            "Validation: Batch 1/1, Loss: 1.0770976359708584e-06\n",
            "Epoch: 1945/10000\n",
            "Training: Batch 1/2, Loss: 1.1535613566593383e-06\n",
            "Training: Batch 2/2, Loss: 1.1106991451015347e-06\n",
            "Validation: Batch 1/1, Loss: 1.0650848025761661e-06\n",
            "Epoch: 1946/10000\n",
            "Training: Batch 1/2, Loss: 1.0997664503520355e-06\n",
            "Training: Batch 2/2, Loss: 1.1390879990358371e-06\n",
            "Validation: Batch 1/1, Loss: 1.0532347687330912e-06\n",
            "Epoch: 1947/10000\n",
            "Training: Batch 1/2, Loss: 1.0937582146652858e-06\n",
            "Training: Batch 2/2, Loss: 1.1202361065443256e-06\n",
            "Validation: Batch 1/1, Loss: 1.0415266160634928e-06\n",
            "Epoch: 1948/10000\n",
            "Training: Batch 1/2, Loss: 1.0910767969107837e-06\n",
            "Training: Batch 2/2, Loss: 1.098346729122568e-06\n",
            "Validation: Batch 1/1, Loss: 1.0299062296326156e-06\n",
            "Epoch: 1949/10000\n",
            "Training: Batch 1/2, Loss: 1.0942205790342996e-06\n",
            "Training: Batch 2/2, Loss: 1.070886696652451e-06\n",
            "Validation: Batch 1/1, Loss: 1.0184104439758812e-06\n",
            "Epoch: 1950/10000\n",
            "Training: Batch 1/2, Loss: 1.0848024203369278e-06\n",
            "Training: Batch 2/2, Loss: 1.056124119713786e-06\n",
            "Validation: Batch 1/1, Loss: 1.007003220365732e-06\n",
            "Epoch: 1951/10000\n",
            "Training: Batch 1/2, Loss: 1.0638981393640279e-06\n",
            "Training: Batch 2/2, Loss: 1.0530328609092976e-06\n",
            "Validation: Batch 1/1, Loss: 9.9573185252666e-07\n",
            "Epoch: 1952/10000\n",
            "Training: Batch 1/2, Loss: 9.697822633825126e-07\n",
            "Training: Batch 2/2, Loss: 1.1230841892029275e-06\n",
            "Validation: Batch 1/1, Loss: 9.84612483989622e-07\n",
            "Epoch: 1953/10000\n",
            "Training: Batch 1/2, Loss: 1.0543858479650225e-06\n",
            "Training: Batch 2/2, Loss: 1.0156011285289424e-06\n",
            "Validation: Batch 1/1, Loss: 9.73549845184607e-07\n",
            "Epoch: 1954/10000\n",
            "Training: Batch 1/2, Loss: 1.0049574257209315e-06\n",
            "Training: Batch 2/2, Loss: 1.0416081295261392e-06\n",
            "Validation: Batch 1/1, Loss: 9.626473911339417e-07\n",
            "Epoch: 1955/10000\n",
            "Training: Batch 1/2, Loss: 1.0446389069329598e-06\n",
            "Training: Batch 2/2, Loss: 9.792689752430306e-07\n",
            "Validation: Batch 1/1, Loss: 9.518266210761794e-07\n",
            "Epoch: 1956/10000\n",
            "Training: Batch 1/2, Loss: 1.0031279771283153e-06\n",
            "Training: Batch 2/2, Loss: 9.978907655749936e-07\n",
            "Validation: Batch 1/1, Loss: 9.41152507039078e-07\n",
            "Epoch: 1957/10000\n",
            "Training: Batch 1/2, Loss: 9.926440043273033e-07\n",
            "Training: Batch 2/2, Loss: 9.859171541393152e-07\n",
            "Validation: Batch 1/1, Loss: 9.305362027589581e-07\n",
            "Epoch: 1958/10000\n",
            "Training: Batch 1/2, Loss: 9.995836762755061e-07\n",
            "Training: Batch 2/2, Loss: 9.568054792907787e-07\n",
            "Validation: Batch 1/1, Loss: 9.200637691719749e-07\n",
            "Epoch: 1959/10000\n",
            "Training: Batch 1/2, Loss: 9.72813495536684e-07\n",
            "Training: Batch 2/2, Loss: 9.61451519287948e-07\n",
            "Validation: Batch 1/1, Loss: 9.096650614992541e-07\n",
            "Epoch: 1960/10000\n",
            "Training: Batch 1/2, Loss: 9.811157042349805e-07\n",
            "Training: Batch 2/2, Loss: 9.314238695878885e-07\n",
            "Validation: Batch 1/1, Loss: 8.993994811135053e-07\n",
            "Epoch: 1961/10000\n",
            "Training: Batch 1/2, Loss: 9.169935992758838e-07\n",
            "Training: Batch 2/2, Loss: 9.73711166807334e-07\n",
            "Validation: Batch 1/1, Loss: 8.892538971849717e-07\n",
            "Epoch: 1962/10000\n",
            "Training: Batch 1/2, Loss: 9.41856171721156e-07\n",
            "Training: Batch 2/2, Loss: 9.277160302190168e-07\n",
            "Validation: Batch 1/1, Loss: 8.791957952780649e-07\n",
            "Epoch: 1963/10000\n",
            "Training: Batch 1/2, Loss: 9.699676866148366e-07\n",
            "Training: Batch 2/2, Loss: 8.786870466792607e-07\n",
            "Validation: Batch 1/1, Loss: 8.692259712006489e-07\n",
            "Epoch: 1964/10000\n",
            "Training: Batch 1/2, Loss: 9.242172041012964e-07\n",
            "Training: Batch 2/2, Loss: 9.032717684931413e-07\n",
            "Validation: Batch 1/1, Loss: 8.593803499934438e-07\n",
            "Epoch: 1965/10000\n",
            "Training: Batch 1/2, Loss: 9.059644980879966e-07\n",
            "Training: Batch 2/2, Loss: 9.007764560919895e-07\n",
            "Validation: Batch 1/1, Loss: 8.496094210386218e-07\n",
            "Epoch: 1966/10000\n",
            "Training: Batch 1/2, Loss: 9.408069558958232e-07\n",
            "Training: Batch 2/2, Loss: 8.456438536086353e-07\n",
            "Validation: Batch 1/1, Loss: 8.399505304623744e-07\n",
            "Epoch: 1967/10000\n",
            "Training: Batch 1/2, Loss: 8.672552667121636e-07\n",
            "Training: Batch 2/2, Loss: 8.985557542473543e-07\n",
            "Validation: Batch 1/1, Loss: 8.304222092192504e-07\n",
            "Epoch: 1968/10000\n",
            "Training: Batch 1/2, Loss: 8.444382615380164e-07\n",
            "Training: Batch 2/2, Loss: 9.01308396805689e-07\n",
            "Validation: Batch 1/1, Loss: 8.2100547160735e-07\n",
            "Epoch: 1969/10000\n",
            "Training: Batch 1/2, Loss: 8.193117650989734e-07\n",
            "Training: Batch 2/2, Loss: 9.065944368558121e-07\n",
            "Validation: Batch 1/1, Loss: 8.117028187371034e-07\n",
            "Epoch: 1970/10000\n",
            "Training: Batch 1/2, Loss: 8.714120554031979e-07\n",
            "Training: Batch 2/2, Loss: 8.352078566531418e-07\n",
            "Validation: Batch 1/1, Loss: 8.0245638400811e-07\n",
            "Epoch: 1971/10000\n",
            "Training: Batch 1/2, Loss: 8.279228040919406e-07\n",
            "Training: Batch 2/2, Loss: 8.590875495428918e-07\n",
            "Validation: Batch 1/1, Loss: 7.933079473332327e-07\n",
            "Epoch: 1972/10000\n",
            "Training: Batch 1/2, Loss: 8.220573022299504e-07\n",
            "Training: Batch 2/2, Loss: 8.457368494418915e-07\n",
            "Validation: Batch 1/1, Loss: 7.842504601285327e-07\n",
            "Epoch: 1973/10000\n",
            "Training: Batch 1/2, Loss: 7.924223268673813e-07\n",
            "Training: Batch 2/2, Loss: 8.562361699659959e-07\n",
            "Validation: Batch 1/1, Loss: 7.752886403977755e-07\n",
            "Epoch: 1974/10000\n",
            "Training: Batch 1/2, Loss: 7.921415772216278e-07\n",
            "Training: Batch 2/2, Loss: 8.37725451674487e-07\n",
            "Validation: Batch 1/1, Loss: 7.664221470804478e-07\n",
            "Epoch: 1975/10000\n",
            "Training: Batch 1/2, Loss: 7.965257395881054e-07\n",
            "Training: Batch 2/2, Loss: 8.147947596626182e-07\n",
            "Validation: Batch 1/1, Loss: 7.57650411742361e-07\n",
            "Epoch: 1976/10000\n",
            "Training: Batch 1/2, Loss: 7.495381737498974e-07\n",
            "Training: Batch 2/2, Loss: 8.431257469965203e-07\n",
            "Validation: Batch 1/1, Loss: 7.489838935725857e-07\n",
            "Epoch: 1977/10000\n",
            "Training: Batch 1/2, Loss: 7.874468224144948e-07\n",
            "Training: Batch 2/2, Loss: 7.872538390074624e-07\n",
            "Validation: Batch 1/1, Loss: 7.403777999570593e-07\n",
            "Epoch: 1978/10000\n",
            "Training: Batch 1/2, Loss: 7.947193125801277e-07\n",
            "Training: Batch 2/2, Loss: 7.619999564667523e-07\n",
            "Validation: Batch 1/1, Loss: 7.318620305341028e-07\n",
            "Epoch: 1979/10000\n",
            "Training: Batch 1/2, Loss: 7.859623565309448e-07\n",
            "Training: Batch 2/2, Loss: 7.528628316322283e-07\n",
            "Validation: Batch 1/1, Loss: 7.234600047922868e-07\n",
            "Epoch: 1980/10000\n",
            "Training: Batch 1/2, Loss: 7.531413643846463e-07\n",
            "Training: Batch 2/2, Loss: 7.679019518036512e-07\n",
            "Validation: Batch 1/1, Loss: 7.151456316023541e-07\n",
            "Epoch: 1981/10000\n",
            "Training: Batch 1/2, Loss: 7.551162752861273e-07\n",
            "Training: Batch 2/2, Loss: 7.485011792596197e-07\n",
            "Validation: Batch 1/1, Loss: 7.069128287184867e-07\n",
            "Epoch: 1982/10000\n",
            "Training: Batch 1/2, Loss: 7.259374683599162e-07\n",
            "Training: Batch 2/2, Loss: 7.602765776937304e-07\n",
            "Validation: Batch 1/1, Loss: 6.987569918237568e-07\n",
            "Epoch: 1983/10000\n",
            "Training: Batch 1/2, Loss: 7.203135510280845e-07\n",
            "Training: Batch 2/2, Loss: 7.487855100407614e-07\n",
            "Validation: Batch 1/1, Loss: 6.907094984853757e-07\n",
            "Epoch: 1984/10000\n",
            "Training: Batch 1/2, Loss: 6.910469210197334e-07\n",
            "Training: Batch 2/2, Loss: 7.610186116835393e-07\n",
            "Validation: Batch 1/1, Loss: 6.827419269939128e-07\n",
            "Epoch: 1985/10000\n",
            "Training: Batch 1/2, Loss: 7.154787908802973e-07\n",
            "Training: Batch 2/2, Loss: 7.200247864602716e-07\n",
            "Validation: Batch 1/1, Loss: 6.748537657585985e-07\n",
            "Epoch: 1986/10000\n",
            "Training: Batch 1/2, Loss: 7.337236525017943e-07\n",
            "Training: Batch 2/2, Loss: 6.853418881291873e-07\n",
            "Validation: Batch 1/1, Loss: 6.670266543551406e-07\n",
            "Epoch: 1987/10000\n",
            "Training: Batch 1/2, Loss: 6.921975455043139e-07\n",
            "Training: Batch 2/2, Loss: 7.102415793269756e-07\n",
            "Validation: Batch 1/1, Loss: 6.593078296646127e-07\n",
            "Epoch: 1988/10000\n",
            "Training: Batch 1/2, Loss: 6.889930546094547e-07\n",
            "Training: Batch 2/2, Loss: 6.972516644054849e-07\n",
            "Validation: Batch 1/1, Loss: 6.516659141198033e-07\n",
            "Epoch: 1989/10000\n",
            "Training: Batch 1/2, Loss: 6.685116318294604e-07\n",
            "Training: Batch 2/2, Loss: 7.016055860731285e-07\n",
            "Validation: Batch 1/1, Loss: 6.441212576646649e-07\n",
            "Epoch: 1990/10000\n",
            "Training: Batch 1/2, Loss: 6.706087560814922e-07\n",
            "Training: Batch 2/2, Loss: 6.836809802734933e-07\n",
            "Validation: Batch 1/1, Loss: 6.366165621329856e-07\n",
            "Epoch: 1991/10000\n",
            "Training: Batch 1/2, Loss: 6.836095849394042e-07\n",
            "Training: Batch 2/2, Loss: 6.550443458763766e-07\n",
            "Validation: Batch 1/1, Loss: 6.292118541750824e-07\n",
            "Epoch: 1992/10000\n",
            "Training: Batch 1/2, Loss: 6.432011900869838e-07\n",
            "Training: Batch 2/2, Loss: 6.797133096370089e-07\n",
            "Validation: Batch 1/1, Loss: 6.218842827365734e-07\n",
            "Epoch: 1993/10000\n",
            "Training: Batch 1/2, Loss: 6.784157449146733e-07\n",
            "Training: Batch 2/2, Loss: 6.293371939136705e-07\n",
            "Validation: Batch 1/1, Loss: 6.146385658212239e-07\n",
            "Epoch: 1994/10000\n",
            "Training: Batch 1/2, Loss: 6.302559540927177e-07\n",
            "Training: Batch 2/2, Loss: 6.620542762902915e-07\n",
            "Validation: Batch 1/1, Loss: 6.074678822187707e-07\n",
            "Epoch: 1995/10000\n",
            "Training: Batch 1/2, Loss: 6.54641098662978e-07\n",
            "Training: Batch 2/2, Loss: 6.227815561032912e-07\n",
            "Validation: Batch 1/1, Loss: 6.00392127125815e-07\n",
            "Epoch: 1996/10000\n",
            "Training: Batch 1/2, Loss: 6.242929657673812e-07\n",
            "Training: Batch 2/2, Loss: 6.381113166753494e-07\n",
            "Validation: Batch 1/1, Loss: 5.933861757512204e-07\n",
            "Epoch: 1997/10000\n",
            "Training: Batch 1/2, Loss: 6.308221713879902e-07\n",
            "Training: Batch 2/2, Loss: 6.169183848214743e-07\n",
            "Validation: Batch 1/1, Loss: 5.864503691555001e-07\n",
            "Epoch: 1998/10000\n",
            "Training: Batch 1/2, Loss: 6.239931735763093e-07\n",
            "Training: Batch 2/2, Loss: 6.091597697377438e-07\n",
            "Validation: Batch 1/1, Loss: 5.795795345875376e-07\n",
            "Epoch: 1999/10000\n",
            "Training: Batch 1/2, Loss: 6.12606811500882e-07\n",
            "Training: Batch 2/2, Loss: 6.060748773961677e-07\n",
            "Validation: Batch 1/1, Loss: 5.727742404815217e-07\n",
            "Epoch: 2000/10000\n",
            "Training: Batch 1/2, Loss: 6.12812243616645e-07\n",
            "Training: Batch 2/2, Loss: 5.916032250752323e-07\n",
            "Validation: Batch 1/1, Loss: 5.660422743858362e-07\n",
            "Epoch: 2001/10000\n",
            "Training: Batch 1/2, Loss: 6.107990770942706e-07\n",
            "Training: Batch 2/2, Loss: 5.794920525659109e-07\n",
            "Validation: Batch 1/1, Loss: 5.593732907982485e-07\n",
            "Epoch: 2002/10000\n",
            "Training: Batch 1/2, Loss: 6.397593210749619e-07\n",
            "Training: Batch 2/2, Loss: 5.367211315387976e-07\n",
            "Validation: Batch 1/1, Loss: 5.527825237550132e-07\n",
            "Epoch: 2003/10000\n",
            "Training: Batch 1/2, Loss: 5.621374725706119e-07\n",
            "Training: Batch 2/2, Loss: 6.001235988151166e-07\n",
            "Validation: Batch 1/1, Loss: 5.463012371365039e-07\n",
            "Epoch: 2004/10000\n",
            "Training: Batch 1/2, Loss: 5.855903282281361e-07\n",
            "Training: Batch 2/2, Loss: 5.631982276099734e-07\n",
            "Validation: Batch 1/1, Loss: 5.398637199505174e-07\n",
            "Epoch: 2005/10000\n",
            "Training: Batch 1/2, Loss: 5.637735398522636e-07\n",
            "Training: Batch 2/2, Loss: 5.714025519409915e-07\n",
            "Validation: Batch 1/1, Loss: 5.335023729458044e-07\n",
            "Epoch: 2006/10000\n",
            "Training: Batch 1/2, Loss: 5.876414093108906e-07\n",
            "Training: Batch 2/2, Loss: 5.34325010903558e-07\n",
            "Validation: Batch 1/1, Loss: 5.271977556731144e-07\n",
            "Epoch: 2007/10000\n",
            "Training: Batch 1/2, Loss: 5.478061098074249e-07\n",
            "Training: Batch 2/2, Loss: 5.607381012850965e-07\n",
            "Validation: Batch 1/1, Loss: 5.209802793615381e-07\n",
            "Epoch: 2008/10000\n",
            "Training: Batch 1/2, Loss: 5.661822228830715e-07\n",
            "Training: Batch 2/2, Loss: 5.294184006743308e-07\n",
            "Validation: Batch 1/1, Loss: 5.148139621269365e-07\n",
            "Epoch: 2009/10000\n",
            "Training: Batch 1/2, Loss: 5.385741701502411e-07\n",
            "Training: Batch 2/2, Loss: 5.439492269943003e-07\n",
            "Validation: Batch 1/1, Loss: 5.087224508315558e-07\n",
            "Epoch: 2010/10000\n",
            "Training: Batch 1/2, Loss: 5.241431040303723e-07\n",
            "Training: Batch 2/2, Loss: 5.455294740386307e-07\n",
            "Validation: Batch 1/1, Loss: 5.027094971410406e-07\n",
            "Epoch: 2011/10000\n",
            "Training: Batch 1/2, Loss: 5.189440912545251e-07\n",
            "Training: Batch 2/2, Loss: 5.381083951760957e-07\n",
            "Validation: Batch 1/1, Loss: 4.967566837876802e-07\n",
            "Epoch: 2012/10000\n",
            "Training: Batch 1/2, Loss: 5.34661523943214e-07\n",
            "Training: Batch 2/2, Loss: 5.099905706629215e-07\n",
            "Validation: Batch 1/1, Loss: 4.908521304969327e-07\n",
            "Epoch: 2013/10000\n",
            "Training: Batch 1/2, Loss: 4.6837595846227487e-07\n",
            "Training: Batch 2/2, Loss: 5.63559012789483e-07\n",
            "Validation: Batch 1/1, Loss: 4.850453478866257e-07\n",
            "Epoch: 2014/10000\n",
            "Training: Batch 1/2, Loss: 5.084788199383183e-07\n",
            "Training: Batch 2/2, Loss: 5.114879400025529e-07\n",
            "Validation: Batch 1/1, Loss: 4.792676691067754e-07\n",
            "Epoch: 2015/10000\n",
            "Training: Batch 1/2, Loss: 5.123687287778012e-07\n",
            "Training: Batch 2/2, Loss: 4.955118129146285e-07\n",
            "Validation: Batch 1/1, Loss: 4.7356425625366683e-07\n",
            "Epoch: 2016/10000\n",
            "Training: Batch 1/2, Loss: 5.192439971324347e-07\n",
            "Training: Batch 2/2, Loss: 4.767121595250501e-07\n",
            "Validation: Batch 1/1, Loss: 4.6790276542196807e-07\n",
            "Epoch: 2017/10000\n",
            "Training: Batch 1/2, Loss: 4.970445388607914e-07\n",
            "Training: Batch 2/2, Loss: 4.869288545705786e-07\n",
            "Validation: Batch 1/1, Loss: 4.623256018021493e-07\n",
            "Epoch: 2018/10000\n",
            "Training: Batch 1/2, Loss: 4.7137234560068464e-07\n",
            "Training: Batch 2/2, Loss: 5.007566414860776e-07\n",
            "Validation: Batch 1/1, Loss: 4.568079248201684e-07\n",
            "Epoch: 2019/10000\n",
            "Training: Batch 1/2, Loss: 4.88041109747428e-07\n",
            "Training: Batch 2/2, Loss: 4.7263142732845154e-07\n",
            "Validation: Batch 1/1, Loss: 4.5137704773878795e-07\n",
            "Epoch: 2020/10000\n",
            "Training: Batch 1/2, Loss: 4.6959070232333033e-07\n",
            "Training: Batch 2/2, Loss: 4.795536483470642e-07\n",
            "Validation: Batch 1/1, Loss: 4.4596856696443865e-07\n",
            "Epoch: 2021/10000\n",
            "Training: Batch 1/2, Loss: 4.482523081605905e-07\n",
            "Training: Batch 2/2, Loss: 4.894518497167155e-07\n",
            "Validation: Batch 1/1, Loss: 4.406360289976874e-07\n",
            "Epoch: 2022/10000\n",
            "Training: Batch 1/2, Loss: 4.660455203975289e-07\n",
            "Training: Batch 2/2, Loss: 4.60561722093189e-07\n",
            "Validation: Batch 1/1, Loss: 4.353463509687572e-07\n",
            "Epoch: 2023/10000\n",
            "Training: Batch 1/2, Loss: 4.6607166837020486e-07\n",
            "Training: Batch 2/2, Loss: 4.494468157645315e-07\n",
            "Validation: Batch 1/1, Loss: 4.301092246805638e-07\n",
            "Epoch: 2024/10000\n",
            "Training: Batch 1/2, Loss: 4.666105155592959e-07\n",
            "Training: Batch 2/2, Loss: 4.379372455787234e-07\n",
            "Validation: Batch 1/1, Loss: 4.2494386320868216e-07\n",
            "Epoch: 2025/10000\n",
            "Training: Batch 1/2, Loss: 4.3942560523646534e-07\n",
            "Training: Batch 2/2, Loss: 4.5415563931783254e-07\n",
            "Validation: Batch 1/1, Loss: 4.198459464532789e-07\n",
            "Epoch: 2026/10000\n",
            "Training: Batch 1/2, Loss: 4.2947772271872964e-07\n",
            "Training: Batch 2/2, Loss: 4.533640947101958e-07\n",
            "Validation: Batch 1/1, Loss: 4.1482201140752295e-07\n",
            "Epoch: 2027/10000\n",
            "Training: Batch 1/2, Loss: 4.260266734945617e-07\n",
            "Training: Batch 2/2, Loss: 4.462401648197556e-07\n",
            "Validation: Batch 1/1, Loss: 4.0983084659274027e-07\n",
            "Epoch: 2028/10000\n",
            "Training: Batch 1/2, Loss: 4.317867592362745e-07\n",
            "Training: Batch 2/2, Loss: 4.3005206862289924e-07\n",
            "Validation: Batch 1/1, Loss: 4.0488171748620516e-07\n",
            "Epoch: 2029/10000\n",
            "Training: Batch 1/2, Loss: 4.439432927938469e-07\n",
            "Training: Batch 2/2, Loss: 4.076007087405742e-07\n",
            "Validation: Batch 1/1, Loss: 3.9999184764383244e-07\n",
            "Epoch: 2030/10000\n",
            "Training: Batch 1/2, Loss: 4.1407921003155934e-07\n",
            "Training: Batch 2/2, Loss: 4.2705667624431953e-07\n",
            "Validation: Batch 1/1, Loss: 3.951673193114402e-07\n",
            "Epoch: 2031/10000\n",
            "Training: Batch 1/2, Loss: 4.175585957000294e-07\n",
            "Training: Batch 2/2, Loss: 4.134846278702753e-07\n",
            "Validation: Batch 1/1, Loss: 3.904059724391118e-07\n",
            "Epoch: 2032/10000\n",
            "Training: Batch 1/2, Loss: 4.056374223182502e-07\n",
            "Training: Batch 2/2, Loss: 4.153377801685565e-07\n",
            "Validation: Batch 1/1, Loss: 3.856913792787964e-07\n",
            "Epoch: 2033/10000\n",
            "Training: Batch 1/2, Loss: 4.175674632733717e-07\n",
            "Training: Batch 2/2, Loss: 3.9357090031444386e-07\n",
            "Validation: Batch 1/1, Loss: 3.8102706412246334e-07\n",
            "Epoch: 2034/10000\n",
            "Training: Batch 1/2, Loss: 3.941011357255775e-07\n",
            "Training: Batch 2/2, Loss: 4.071279704476183e-07\n",
            "Validation: Batch 1/1, Loss: 3.7641251537934295e-07\n",
            "Epoch: 2035/10000\n",
            "Training: Batch 1/2, Loss: 3.9985025068745017e-07\n",
            "Training: Batch 2/2, Loss: 3.917336641734437e-07\n",
            "Validation: Batch 1/1, Loss: 3.7183778545113455e-07\n",
            "Epoch: 2036/10000\n",
            "Training: Batch 1/2, Loss: 3.874258425184962e-07\n",
            "Training: Batch 2/2, Loss: 3.944914226394758e-07\n",
            "Validation: Batch 1/1, Loss: 3.673104913559655e-07\n",
            "Epoch: 2037/10000\n",
            "Training: Batch 1/2, Loss: 3.9943591900737374e-07\n",
            "Training: Batch 2/2, Loss: 3.7307762568161706e-07\n",
            "Validation: Batch 1/1, Loss: 3.6283395843383914e-07\n",
            "Epoch: 2038/10000\n",
            "Training: Batch 1/2, Loss: 4.034609730751981e-07\n",
            "Training: Batch 2/2, Loss: 3.5968929523733095e-07\n",
            "Validation: Batch 1/1, Loss: 3.5841227941091347e-07\n",
            "Epoch: 2039/10000\n",
            "Training: Batch 1/2, Loss: 3.8401907431762083e-07\n",
            "Training: Batch 2/2, Loss: 3.6976297224100563e-07\n",
            "Validation: Batch 1/1, Loss: 3.5404093523538904e-07\n",
            "Epoch: 2040/10000\n",
            "Training: Batch 1/2, Loss: 3.5952720622844936e-07\n",
            "Training: Batch 2/2, Loss: 3.849538643407868e-07\n",
            "Validation: Batch 1/1, Loss: 3.4973552942574315e-07\n",
            "Epoch: 2041/10000\n",
            "Training: Batch 1/2, Loss: 3.7373993677647377e-07\n",
            "Training: Batch 2/2, Loss: 3.618002892835648e-07\n",
            "Validation: Batch 1/1, Loss: 3.454705392869073e-07\n",
            "Epoch: 2042/10000\n",
            "Training: Batch 1/2, Loss: 3.5556044508666673e-07\n",
            "Training: Batch 2/2, Loss: 3.7093758464834536e-07\n",
            "Validation: Batch 1/1, Loss: 3.412596356611175e-07\n",
            "Epoch: 2043/10000\n",
            "Training: Batch 1/2, Loss: 3.5843467571794463e-07\n",
            "Training: Batch 2/2, Loss: 3.592391522033722e-07\n",
            "Validation: Batch 1/1, Loss: 3.370982710748649e-07\n",
            "Epoch: 2044/10000\n",
            "Training: Batch 1/2, Loss: 3.4485574929021823e-07\n",
            "Training: Batch 2/2, Loss: 3.6399706004885957e-07\n",
            "Validation: Batch 1/1, Loss: 3.329709272748005e-07\n",
            "Epoch: 2045/10000\n",
            "Training: Batch 1/2, Loss: 3.5372198681216105e-07\n",
            "Training: Batch 2/2, Loss: 3.465278837211372e-07\n",
            "Validation: Batch 1/1, Loss: 3.2888027590161073e-07\n",
            "Epoch: 2046/10000\n",
            "Training: Batch 1/2, Loss: 3.5583846624831494e-07\n",
            "Training: Batch 2/2, Loss: 3.358564981681411e-07\n",
            "Validation: Batch 1/1, Loss: 3.248529480970319e-07\n",
            "Epoch: 2047/10000\n",
            "Training: Batch 1/2, Loss: 3.349776704908436e-07\n",
            "Training: Batch 2/2, Loss: 3.481416683825955e-07\n",
            "Validation: Batch 1/1, Loss: 3.2086043688650534e-07\n",
            "Epoch: 2048/10000\n",
            "Training: Batch 1/2, Loss: 3.474836489658628e-07\n",
            "Training: Batch 2/2, Loss: 3.273376307788567e-07\n",
            "Validation: Batch 1/1, Loss: 3.169142246406409e-07\n",
            "Epoch: 2049/10000\n",
            "Training: Batch 1/2, Loss: 3.420645100504771e-07\n",
            "Training: Batch 2/2, Loss: 3.244516051381652e-07\n",
            "Validation: Batch 1/1, Loss: 3.1301016178986174e-07\n",
            "Epoch: 2050/10000\n",
            "Training: Batch 1/2, Loss: 3.3969368473663053e-07\n",
            "Training: Batch 2/2, Loss: 3.186239609931363e-07\n",
            "Validation: Batch 1/1, Loss: 3.091408871114254e-07\n",
            "Epoch: 2051/10000\n",
            "Training: Batch 1/2, Loss: 3.284974354755832e-07\n",
            "Training: Batch 2/2, Loss: 3.2164277286028664e-07\n",
            "Validation: Batch 1/1, Loss: 3.053234536309901e-07\n",
            "Epoch: 2052/10000\n",
            "Training: Batch 1/2, Loss: 3.1884465556686337e-07\n",
            "Training: Batch 2/2, Loss: 3.2324746257472725e-07\n",
            "Validation: Batch 1/1, Loss: 3.0155715080582013e-07\n",
            "Epoch: 2053/10000\n",
            "Training: Batch 1/2, Loss: 3.319895824915875e-07\n",
            "Training: Batch 2/2, Loss: 3.022864518698043e-07\n",
            "Validation: Batch 1/1, Loss: 2.978225950300839e-07\n",
            "Epoch: 2054/10000\n",
            "Training: Batch 1/2, Loss: 3.0590109645345365e-07\n",
            "Training: Batch 2/2, Loss: 3.203997209766385e-07\n",
            "Validation: Batch 1/1, Loss: 2.9415028279800026e-07\n",
            "Epoch: 2055/10000\n",
            "Training: Batch 1/2, Loss: 3.1883061524240475e-07\n",
            "Training: Batch 2/2, Loss: 2.9984366278767993e-07\n",
            "Validation: Batch 1/1, Loss: 2.905007363551704e-07\n",
            "Epoch: 2056/10000\n",
            "Training: Batch 1/2, Loss: 3.0820248753116175e-07\n",
            "Training: Batch 2/2, Loss: 3.027703883162758e-07\n",
            "Validation: Batch 1/1, Loss: 2.8691516718026833e-07\n",
            "Epoch: 2057/10000\n",
            "Training: Batch 1/2, Loss: 2.920407666806568e-07\n",
            "Training: Batch 2/2, Loss: 3.1131219202507054e-07\n",
            "Validation: Batch 1/1, Loss: 2.8336415880403365e-07\n",
            "Epoch: 2058/10000\n",
            "Training: Batch 1/2, Loss: 3.1070680961420294e-07\n",
            "Training: Batch 2/2, Loss: 2.8529694873213884e-07\n",
            "Validation: Batch 1/1, Loss: 2.798320792862796e-07\n",
            "Epoch: 2059/10000\n",
            "Training: Batch 1/2, Loss: 3.0181527677086706e-07\n",
            "Training: Batch 2/2, Loss: 2.8674347163359926e-07\n",
            "Validation: Batch 1/1, Loss: 2.7636193067337445e-07\n",
            "Epoch: 2060/10000\n",
            "Training: Batch 1/2, Loss: 2.970306240968057e-07\n",
            "Training: Batch 2/2, Loss: 2.8421604270079115e-07\n",
            "Validation: Batch 1/1, Loss: 2.7291849846733385e-07\n",
            "Epoch: 2061/10000\n",
            "Training: Batch 1/2, Loss: 2.7668838242789207e-07\n",
            "Training: Batch 2/2, Loss: 2.972240338294796e-07\n",
            "Validation: Batch 1/1, Loss: 2.695295222565619e-07\n",
            "Epoch: 2062/10000\n",
            "Training: Batch 1/2, Loss: 2.819676012677519e-07\n",
            "Training: Batch 2/2, Loss: 2.848689746315358e-07\n",
            "Validation: Batch 1/1, Loss: 2.661696498762467e-07\n",
            "Epoch: 2063/10000\n",
            "Training: Batch 1/2, Loss: 2.788450217394711e-07\n",
            "Training: Batch 2/2, Loss: 2.809345858167944e-07\n",
            "Validation: Batch 1/1, Loss: 2.628493120937492e-07\n",
            "Epoch: 2064/10000\n",
            "Training: Batch 1/2, Loss: 2.7148470849169826e-07\n",
            "Training: Batch 2/2, Loss: 2.81300259530326e-07\n",
            "Validation: Batch 1/1, Loss: 2.595814407868602e-07\n",
            "Epoch: 2065/10000\n",
            "Training: Batch 1/2, Loss: 2.7828482984659786e-07\n",
            "Training: Batch 2/2, Loss: 2.6767924055093317e-07\n",
            "Validation: Batch 1/1, Loss: 2.563378984632436e-07\n",
            "Epoch: 2066/10000\n",
            "Training: Batch 1/2, Loss: 2.773744540718326e-07\n",
            "Training: Batch 2/2, Loss: 2.6177238510172174e-07\n",
            "Validation: Batch 1/1, Loss: 2.531140523842623e-07\n",
            "Epoch: 2067/10000\n",
            "Training: Batch 1/2, Loss: 2.7100148258796253e-07\n",
            "Training: Batch 2/2, Loss: 2.61353278574461e-07\n",
            "Validation: Batch 1/1, Loss: 2.4993903480208246e-07\n",
            "Epoch: 2068/10000\n",
            "Training: Batch 1/2, Loss: 2.636736269323592e-07\n",
            "Training: Batch 2/2, Loss: 2.6198787850262306e-07\n",
            "Validation: Batch 1/1, Loss: 2.468027275881468e-07\n",
            "Epoch: 2069/10000\n",
            "Training: Batch 1/2, Loss: 2.6330397417950735e-07\n",
            "Training: Batch 2/2, Loss: 2.5577304541002377e-07\n",
            "Validation: Batch 1/1, Loss: 2.4371391305066936e-07\n",
            "Epoch: 2070/10000\n",
            "Training: Batch 1/2, Loss: 2.452905789596116e-07\n",
            "Training: Batch 2/2, Loss: 2.672044843166077e-07\n",
            "Validation: Batch 1/1, Loss: 2.406631267604098e-07\n",
            "Epoch: 2071/10000\n",
            "Training: Batch 1/2, Loss: 2.502127642856067e-07\n",
            "Training: Batch 2/2, Loss: 2.5590082941562287e-07\n",
            "Validation: Batch 1/1, Loss: 2.3763229251017037e-07\n",
            "Epoch: 2072/10000\n",
            "Training: Batch 1/2, Loss: 2.534918621677207e-07\n",
            "Training: Batch 2/2, Loss: 2.4629534323139524e-07\n",
            "Validation: Batch 1/1, Loss: 2.34646464036814e-07\n",
            "Epoch: 2073/10000\n",
            "Training: Batch 1/2, Loss: 2.426657488285855e-07\n",
            "Training: Batch 2/2, Loss: 2.507970293663675e-07\n",
            "Validation: Batch 1/1, Loss: 2.3169671692357952e-07\n",
            "Epoch: 2074/10000\n",
            "Training: Batch 1/2, Loss: 2.430871859360195e-07\n",
            "Training: Batch 2/2, Loss: 2.441846902456746e-07\n",
            "Validation: Batch 1/1, Loss: 2.2876808714045183e-07\n",
            "Epoch: 2075/10000\n",
            "Training: Batch 1/2, Loss: 2.428925256481307e-07\n",
            "Training: Batch 2/2, Loss: 2.3824328820865048e-07\n",
            "Validation: Batch 1/1, Loss: 2.2588302783788095e-07\n",
            "Epoch: 2076/10000\n",
            "Training: Batch 1/2, Loss: 2.5115903667938255e-07\n",
            "Training: Batch 2/2, Loss: 2.239731884401408e-07\n",
            "Validation: Batch 1/1, Loss: 2.2301513524780603e-07\n",
            "Epoch: 2077/10000\n",
            "Training: Batch 1/2, Loss: 2.2960351486744912e-07\n",
            "Training: Batch 2/2, Loss: 2.393879867668147e-07\n",
            "Validation: Batch 1/1, Loss: 2.201891078357221e-07\n",
            "Epoch: 2078/10000\n",
            "Training: Batch 1/2, Loss: 2.2784983855217433e-07\n",
            "Training: Batch 2/2, Loss: 2.3522206049619854e-07\n",
            "Validation: Batch 1/1, Loss: 2.1741061573266052e-07\n",
            "Epoch: 2079/10000\n",
            "Training: Batch 1/2, Loss: 2.2234725349790097e-07\n",
            "Training: Batch 2/2, Loss: 2.348582910371988e-07\n",
            "Validation: Batch 1/1, Loss: 2.1466112798407266e-07\n",
            "Epoch: 2080/10000\n",
            "Training: Batch 1/2, Loss: 2.2546393552147492e-07\n",
            "Training: Batch 2/2, Loss: 2.2598523230499268e-07\n",
            "Validation: Batch 1/1, Loss: 2.119282811463563e-07\n",
            "Epoch: 2081/10000\n",
            "Training: Batch 1/2, Loss: 2.2609468430800916e-07\n",
            "Training: Batch 2/2, Loss: 2.1963444396533305e-07\n",
            "Validation: Batch 1/1, Loss: 2.0923627630509145e-07\n",
            "Epoch: 2082/10000\n",
            "Training: Batch 1/2, Loss: 2.1316863296760857e-07\n",
            "Training: Batch 2/2, Loss: 2.2684677958295651e-07\n",
            "Validation: Batch 1/1, Loss: 2.0658396238104615e-07\n",
            "Epoch: 2083/10000\n",
            "Training: Batch 1/2, Loss: 2.1572904529421066e-07\n",
            "Training: Batch 2/2, Loss: 2.187280045973239e-07\n",
            "Validation: Batch 1/1, Loss: 2.0394594457684434e-07\n",
            "Epoch: 2084/10000\n",
            "Training: Batch 1/2, Loss: 2.1764853386230243e-07\n",
            "Training: Batch 2/2, Loss: 2.112946617671696e-07\n",
            "Validation: Batch 1/1, Loss: 2.013521083199521e-07\n",
            "Epoch: 2085/10000\n",
            "Training: Batch 1/2, Loss: 2.1044104414613685e-07\n",
            "Training: Batch 2/2, Loss: 2.1301727315403696e-07\n",
            "Validation: Batch 1/1, Loss: 1.9878277157658886e-07\n",
            "Epoch: 2086/10000\n",
            "Training: Batch 1/2, Loss: 2.1213753598203766e-07\n",
            "Training: Batch 2/2, Loss: 2.0593427052517654e-07\n",
            "Validation: Batch 1/1, Loss: 1.9623516323008516e-07\n",
            "Epoch: 2087/10000\n",
            "Training: Batch 1/2, Loss: 2.1630627600188745e-07\n",
            "Training: Batch 2/2, Loss: 1.9645838733595156e-07\n",
            "Validation: Batch 1/1, Loss: 1.9373277382328524e-07\n",
            "Epoch: 2088/10000\n",
            "Training: Batch 1/2, Loss: 2.1782291526051267e-07\n",
            "Training: Batch 2/2, Loss: 1.896979426874168e-07\n",
            "Validation: Batch 1/1, Loss: 1.9124468053632881e-07\n",
            "Epoch: 2089/10000\n",
            "Training: Batch 1/2, Loss: 1.9588958366512088e-07\n",
            "Training: Batch 2/2, Loss: 2.0629003927297163e-07\n",
            "Validation: Batch 1/1, Loss: 1.8879815399941435e-07\n",
            "Epoch: 2090/10000\n",
            "Training: Batch 1/2, Loss: 1.9879307444625738e-07\n",
            "Training: Batch 2/2, Loss: 1.9826416064461228e-07\n",
            "Validation: Batch 1/1, Loss: 1.863782870259456e-07\n",
            "Epoch: 2091/10000\n",
            "Training: Batch 1/2, Loss: 1.9454520838735334e-07\n",
            "Training: Batch 2/2, Loss: 1.9742559231872292e-07\n",
            "Validation: Batch 1/1, Loss: 1.8399002499336348e-07\n",
            "Epoch: 2092/10000\n",
            "Training: Batch 1/2, Loss: 1.89408424944304e-07\n",
            "Training: Batch 2/2, Loss: 1.9752198454625614e-07\n",
            "Validation: Batch 1/1, Loss: 1.8163119364089653e-07\n",
            "Epoch: 2093/10000\n",
            "Training: Batch 1/2, Loss: 1.9366933656783658e-07\n",
            "Training: Batch 2/2, Loss: 1.8833945603091706e-07\n",
            "Validation: Batch 1/1, Loss: 1.7930273088495596e-07\n",
            "Epoch: 2094/10000\n",
            "Training: Batch 1/2, Loss: 1.8884135499774857e-07\n",
            "Training: Batch 2/2, Loss: 1.8826196424015507e-07\n",
            "Validation: Batch 1/1, Loss: 1.7700283194699296e-07\n",
            "Epoch: 2095/10000\n",
            "Training: Batch 1/2, Loss: 1.8512524491143267e-07\n",
            "Training: Batch 2/2, Loss: 1.8711671145865694e-07\n",
            "Validation: Batch 1/1, Loss: 1.7472230240400677e-07\n",
            "Epoch: 2096/10000\n",
            "Training: Batch 1/2, Loss: 1.8505514276512258e-07\n",
            "Training: Batch 2/2, Loss: 1.82403042003898e-07\n",
            "Validation: Batch 1/1, Loss: 1.7246779293600412e-07\n",
            "Epoch: 2097/10000\n",
            "Training: Batch 1/2, Loss: 1.737707151505674e-07\n",
            "Training: Batch 2/2, Loss: 1.8889967634549976e-07\n",
            "Validation: Batch 1/1, Loss: 1.7024234466589405e-07\n",
            "Epoch: 2098/10000\n",
            "Training: Batch 1/2, Loss: 1.8741565099844593e-07\n",
            "Training: Batch 2/2, Loss: 1.706702477122235e-07\n",
            "Validation: Batch 1/1, Loss: 1.680366210621287e-07\n",
            "Epoch: 2099/10000\n",
            "Training: Batch 1/2, Loss: 1.8027441228696262e-07\n",
            "Training: Batch 2/2, Loss: 1.7314515332600422e-07\n",
            "Validation: Batch 1/1, Loss: 1.6586456297318364e-07\n",
            "Epoch: 2100/10000\n",
            "Training: Batch 1/2, Loss: 1.774474611693222e-07\n",
            "Training: Batch 2/2, Loss: 1.713961239602213e-07\n",
            "Validation: Batch 1/1, Loss: 1.6371062372400047e-07\n",
            "Epoch: 2101/10000\n",
            "Training: Batch 1/2, Loss: 1.6984523654173245e-07\n",
            "Training: Batch 2/2, Loss: 1.7444224909013428e-07\n",
            "Validation: Batch 1/1, Loss: 1.6158371352048562e-07\n",
            "Epoch: 2102/10000\n",
            "Training: Batch 1/2, Loss: 1.6413881098742422e-07\n",
            "Training: Batch 2/2, Loss: 1.756544207864863e-07\n",
            "Validation: Batch 1/1, Loss: 1.5949164833273244e-07\n",
            "Epoch: 2103/10000\n",
            "Training: Batch 1/2, Loss: 1.6621257259430422e-07\n",
            "Training: Batch 2/2, Loss: 1.6921435985750577e-07\n",
            "Validation: Batch 1/1, Loss: 1.5742192260859156e-07\n",
            "Epoch: 2104/10000\n",
            "Training: Batch 1/2, Loss: 1.6862534835127008e-07\n",
            "Training: Batch 2/2, Loss: 1.6247798839685856e-07\n",
            "Validation: Batch 1/1, Loss: 1.5537500530626858e-07\n",
            "Epoch: 2105/10000\n",
            "Training: Batch 1/2, Loss: 1.699854976777715e-07\n",
            "Training: Batch 2/2, Loss: 1.568316463362862e-07\n",
            "Validation: Batch 1/1, Loss: 1.533532270059368e-07\n",
            "Epoch: 2106/10000\n",
            "Training: Batch 1/2, Loss: 1.6164257488071598e-07\n",
            "Training: Batch 2/2, Loss: 1.6089009591269132e-07\n",
            "Validation: Batch 1/1, Loss: 1.513615615067465e-07\n",
            "Epoch: 2107/10000\n",
            "Training: Batch 1/2, Loss: 1.596174428186714e-07\n",
            "Training: Batch 2/2, Loss: 1.5872380743076064e-07\n",
            "Validation: Batch 1/1, Loss: 1.4938645165329945e-07\n",
            "Epoch: 2108/10000\n",
            "Training: Batch 1/2, Loss: 1.525540653801727e-07\n",
            "Training: Batch 2/2, Loss: 1.6160814197974105e-07\n",
            "Validation: Batch 1/1, Loss: 1.4744188092663535e-07\n",
            "Epoch: 2109/10000\n",
            "Training: Batch 1/2, Loss: 1.566416898413081e-07\n",
            "Training: Batch 2/2, Loss: 1.5346385850989464e-07\n",
            "Validation: Batch 1/1, Loss: 1.4551855542777048e-07\n",
            "Epoch: 2110/10000\n",
            "Training: Batch 1/2, Loss: 1.550879602518762e-07\n",
            "Training: Batch 2/2, Loss: 1.5097394623353466e-07\n",
            "Validation: Batch 1/1, Loss: 1.436195304904686e-07\n",
            "Epoch: 2111/10000\n",
            "Training: Batch 1/2, Loss: 1.5155798394062003e-07\n",
            "Training: Batch 2/2, Loss: 1.505044195937444e-07\n",
            "Validation: Batch 1/1, Loss: 1.4174031548463972e-07\n",
            "Epoch: 2112/10000\n",
            "Training: Batch 1/2, Loss: 1.5502320138693904e-07\n",
            "Training: Batch 2/2, Loss: 1.4311540041944681e-07\n",
            "Validation: Batch 1/1, Loss: 1.3989307490192004e-07\n",
            "Epoch: 2113/10000\n",
            "Training: Batch 1/2, Loss: 1.4666214553926693e-07\n",
            "Training: Batch 2/2, Loss: 1.4754246535630955e-07\n",
            "Validation: Batch 1/1, Loss: 1.3805562559809914e-07\n",
            "Epoch: 2114/10000\n",
            "Training: Batch 1/2, Loss: 1.485860252614657e-07\n",
            "Training: Batch 2/2, Loss: 1.4178785079366207e-07\n",
            "Validation: Batch 1/1, Loss: 1.3624220684960164e-07\n",
            "Epoch: 2115/10000\n",
            "Training: Batch 1/2, Loss: 1.4840155415640766e-07\n",
            "Training: Batch 2/2, Loss: 1.3816531918564579e-07\n",
            "Validation: Batch 1/1, Loss: 1.3445666979805537e-07\n",
            "Epoch: 2116/10000\n",
            "Training: Batch 1/2, Loss: 1.4472944087629003e-07\n",
            "Training: Batch 2/2, Loss: 1.380676479811882e-07\n",
            "Validation: Batch 1/1, Loss: 1.3269090004541795e-07\n",
            "Epoch: 2117/10000\n",
            "Training: Batch 1/2, Loss: 1.489979979396594e-07\n",
            "Training: Batch 2/2, Loss: 1.3011411681418394e-07\n",
            "Validation: Batch 1/1, Loss: 1.3093213624415512e-07\n",
            "Epoch: 2118/10000\n",
            "Training: Batch 1/2, Loss: 1.361135275601555e-07\n",
            "Training: Batch 2/2, Loss: 1.392414787915186e-07\n",
            "Validation: Batch 1/1, Loss: 1.2920044412112475e-07\n",
            "Epoch: 2119/10000\n",
            "Training: Batch 1/2, Loss: 1.3678663890459575e-07\n",
            "Training: Batch 2/2, Loss: 1.349481095758165e-07\n",
            "Validation: Batch 1/1, Loss: 1.275033696401806e-07\n",
            "Epoch: 2120/10000\n",
            "Training: Batch 1/2, Loss: 1.37587989001986e-07\n",
            "Training: Batch 2/2, Loss: 1.3059441528184834e-07\n",
            "Validation: Batch 1/1, Loss: 1.2582125918925158e-07\n",
            "Epoch: 2121/10000\n",
            "Training: Batch 1/2, Loss: 1.277403356425566e-07\n",
            "Training: Batch 2/2, Loss: 1.3685256305961957e-07\n",
            "Validation: Batch 1/1, Loss: 1.2415461014825269e-07\n",
            "Epoch: 2122/10000\n",
            "Training: Batch 1/2, Loss: 1.297154739177131e-07\n",
            "Training: Batch 2/2, Loss: 1.314018476250567e-07\n",
            "Validation: Batch 1/1, Loss: 1.2251848602318205e-07\n",
            "Epoch: 2123/10000\n",
            "Training: Batch 1/2, Loss: 1.31561805005731e-07\n",
            "Training: Batch 2/2, Loss: 1.26133500089054e-07\n",
            "Validation: Batch 1/1, Loss: 1.2089854806163203e-07\n",
            "Epoch: 2124/10000\n",
            "Training: Batch 1/2, Loss: 1.2582003705574607e-07\n",
            "Training: Batch 2/2, Loss: 1.28435644342062e-07\n",
            "Validation: Batch 1/1, Loss: 1.1929321885872923e-07\n",
            "Epoch: 2125/10000\n",
            "Training: Batch 1/2, Loss: 1.2291724260649062e-07\n",
            "Training: Batch 2/2, Loss: 1.279607602100441e-07\n",
            "Validation: Batch 1/1, Loss: 1.1771534502713621e-07\n",
            "Epoch: 2126/10000\n",
            "Training: Batch 1/2, Loss: 1.247399126214077e-07\n",
            "Training: Batch 2/2, Loss: 1.2283659600598185e-07\n",
            "Validation: Batch 1/1, Loss: 1.1614837092110974e-07\n",
            "Epoch: 2127/10000\n",
            "Training: Batch 1/2, Loss: 1.232339457146736e-07\n",
            "Training: Batch 2/2, Loss: 1.210562174946972e-07\n",
            "Validation: Batch 1/1, Loss: 1.1460505078275673e-07\n",
            "Epoch: 2128/10000\n",
            "Training: Batch 1/2, Loss: 1.2481464750635496e-07\n",
            "Training: Batch 2/2, Loss: 1.162466674031748e-07\n",
            "Validation: Batch 1/1, Loss: 1.1307172798069587e-07\n",
            "Epoch: 2129/10000\n",
            "Training: Batch 1/2, Loss: 1.164533998121442e-07\n",
            "Training: Batch 2/2, Loss: 1.2134979954225855e-07\n",
            "Validation: Batch 1/1, Loss: 1.1157511181636437e-07\n",
            "Epoch: 2130/10000\n",
            "Training: Batch 1/2, Loss: 1.1661475696200796e-07\n",
            "Training: Batch 2/2, Loss: 1.180431254965697e-07\n",
            "Validation: Batch 1/1, Loss: 1.1008737743622987e-07\n",
            "Epoch: 2131/10000\n",
            "Training: Batch 1/2, Loss: 1.179888613478397e-07\n",
            "Training: Batch 2/2, Loss: 1.1355574258686829e-07\n",
            "Validation: Batch 1/1, Loss: 1.0861456445354634e-07\n",
            "Epoch: 2132/10000\n",
            "Training: Batch 1/2, Loss: 1.1647163944417116e-07\n",
            "Training: Batch 2/2, Loss: 1.119808956673296e-07\n",
            "Validation: Batch 1/1, Loss: 1.0715987031062468e-07\n",
            "Epoch: 2133/10000\n",
            "Training: Batch 1/2, Loss: 1.1248187092860462e-07\n",
            "Training: Batch 2/2, Loss: 1.128919038251297e-07\n",
            "Validation: Batch 1/1, Loss: 1.0573000963631785e-07\n",
            "Epoch: 2134/10000\n",
            "Training: Batch 1/2, Loss: 1.1200184957260717e-07\n",
            "Training: Batch 2/2, Loss: 1.1036745206638443e-07\n",
            "Validation: Batch 1/1, Loss: 1.0431162422719353e-07\n",
            "Epoch: 2135/10000\n",
            "Training: Batch 1/2, Loss: 1.1205736427655211e-07\n",
            "Training: Batch 2/2, Loss: 1.0734123634392745e-07\n",
            "Validation: Batch 1/1, Loss: 1.0291270058360169e-07\n",
            "Epoch: 2136/10000\n",
            "Training: Batch 1/2, Loss: 1.0218857227073386e-07\n",
            "Training: Batch 2/2, Loss: 1.1421776946463069e-07\n",
            "Validation: Batch 1/1, Loss: 1.0154261786965435e-07\n",
            "Epoch: 2137/10000\n",
            "Training: Batch 1/2, Loss: 1.0591288912564778e-07\n",
            "Training: Batch 2/2, Loss: 1.0764085800474277e-07\n",
            "Validation: Batch 1/1, Loss: 1.0018104745768142e-07\n",
            "Epoch: 2138/10000\n",
            "Training: Batch 1/2, Loss: 1.0999146837775697e-07\n",
            "Training: Batch 2/2, Loss: 1.0073205913840866e-07\n",
            "Validation: Batch 1/1, Loss: 9.883362395157747e-08\n",
            "Epoch: 2139/10000\n",
            "Training: Batch 1/2, Loss: 1.0678644457584596e-07\n",
            "Training: Batch 2/2, Loss: 1.0109165060612213e-07\n",
            "Validation: Batch 1/1, Loss: 9.749782492463055e-08\n",
            "Epoch: 2140/10000\n",
            "Training: Batch 1/2, Loss: 1.0451166332359207e-07\n",
            "Training: Batch 2/2, Loss: 1.0054707644258087e-07\n",
            "Validation: Batch 1/1, Loss: 9.618288743240555e-08\n",
            "Epoch: 2141/10000\n",
            "Training: Batch 1/2, Loss: 1.0061429378538378e-07\n",
            "Training: Batch 2/2, Loss: 1.0166978370307334e-07\n",
            "Validation: Batch 1/1, Loss: 9.488963570447595e-08\n",
            "Epoch: 2142/10000\n",
            "Training: Batch 1/2, Loss: 1.0662246552328725e-07\n",
            "Training: Batch 2/2, Loss: 9.298678804725569e-08\n",
            "Validation: Batch 1/1, Loss: 9.360659447565922e-08\n",
            "Epoch: 2143/10000\n",
            "Training: Batch 1/2, Loss: 1.0265679151189033e-07\n",
            "Training: Batch 2/2, Loss: 9.423473557035322e-08\n",
            "Validation: Batch 1/1, Loss: 9.23352487802731e-08\n",
            "Epoch: 2144/10000\n",
            "Training: Batch 1/2, Loss: 9.82224364065587e-08\n",
            "Training: Batch 2/2, Loss: 9.597818717566042e-08\n",
            "Validation: Batch 1/1, Loss: 9.109101739568359e-08\n",
            "Epoch: 2145/10000\n",
            "Training: Batch 1/2, Loss: 9.794622002345932e-08\n",
            "Training: Batch 2/2, Loss: 9.363762387692987e-08\n",
            "Validation: Batch 1/1, Loss: 8.985913524384159e-08\n",
            "Epoch: 2146/10000\n",
            "Training: Batch 1/2, Loss: 9.36317690047872e-08\n",
            "Training: Batch 2/2, Loss: 9.534330303040406e-08\n",
            "Validation: Batch 1/1, Loss: 8.864054734658566e-08\n",
            "Epoch: 2147/10000\n",
            "Training: Batch 1/2, Loss: 9.45770324278783e-08\n",
            "Training: Batch 2/2, Loss: 9.184942939555185e-08\n",
            "Validation: Batch 1/1, Loss: 8.743207757788696e-08\n",
            "Epoch: 2148/10000\n",
            "Training: Batch 1/2, Loss: 8.85654287685611e-08\n",
            "Training: Batch 2/2, Loss: 9.529388478313194e-08\n",
            "Validation: Batch 1/1, Loss: 8.625138292472911e-08\n",
            "Epoch: 2149/10000\n",
            "Training: Batch 1/2, Loss: 9.2507477233994e-08\n",
            "Training: Batch 2/2, Loss: 8.889986702342867e-08\n",
            "Validation: Batch 1/1, Loss: 8.507917215183625e-08\n",
            "Epoch: 2150/10000\n",
            "Training: Batch 1/2, Loss: 8.844722287904005e-08\n",
            "Training: Batch 2/2, Loss: 9.04789629885272e-08\n",
            "Validation: Batch 1/1, Loss: 8.392279937652347e-08\n",
            "Epoch: 2151/10000\n",
            "Training: Batch 1/2, Loss: 8.689927710747725e-08\n",
            "Training: Batch 2/2, Loss: 8.958286912275071e-08\n",
            "Validation: Batch 1/1, Loss: 8.278085061874663e-08\n",
            "Epoch: 2152/10000\n",
            "Training: Batch 1/2, Loss: 8.62526263745167e-08\n",
            "Training: Batch 2/2, Loss: 8.783744931406545e-08\n",
            "Validation: Batch 1/1, Loss: 8.165604015175632e-08\n",
            "Epoch: 2153/10000\n",
            "Training: Batch 1/2, Loss: 8.659265660071469e-08\n",
            "Training: Batch 2/2, Loss: 8.513502081086699e-08\n",
            "Validation: Batch 1/1, Loss: 8.054139755131473e-08\n",
            "Epoch: 2154/10000\n",
            "Training: Batch 1/2, Loss: 8.518983918293088e-08\n",
            "Training: Batch 2/2, Loss: 8.41984046928701e-08\n",
            "Validation: Batch 1/1, Loss: 7.944002078374979e-08\n",
            "Epoch: 2155/10000\n",
            "Training: Batch 1/2, Loss: 8.53240038622971e-08\n",
            "Training: Batch 2/2, Loss: 8.175018706424453e-08\n",
            "Validation: Batch 1/1, Loss: 7.835217985530107e-08\n",
            "Epoch: 2156/10000\n",
            "Training: Batch 1/2, Loss: 8.143916119252026e-08\n",
            "Training: Batch 2/2, Loss: 8.333228151968797e-08\n",
            "Validation: Batch 1/1, Loss: 7.728522177785635e-08\n",
            "Epoch: 2157/10000\n",
            "Training: Batch 1/2, Loss: 7.835276960577175e-08\n",
            "Training: Batch 2/2, Loss: 8.416122199150777e-08\n",
            "Validation: Batch 1/1, Loss: 7.622962527875643e-08\n",
            "Epoch: 2158/10000\n",
            "Training: Batch 1/2, Loss: 7.69535120070941e-08\n",
            "Training: Batch 2/2, Loss: 8.333525158832344e-08\n",
            "Validation: Batch 1/1, Loss: 7.518010392004726e-08\n",
            "Epoch: 2159/10000\n",
            "Training: Batch 1/2, Loss: 7.848061756021707e-08\n",
            "Training: Batch 2/2, Loss: 7.961767778397189e-08\n",
            "Validation: Batch 1/1, Loss: 7.414370628566758e-08\n",
            "Epoch: 2160/10000\n",
            "Training: Batch 1/2, Loss: 7.509753885415194e-08\n",
            "Training: Batch 2/2, Loss: 8.080936453325194e-08\n",
            "Validation: Batch 1/1, Loss: 7.312841887596733e-08\n",
            "Epoch: 2161/10000\n",
            "Training: Batch 1/2, Loss: 7.33668628072337e-08\n",
            "Training: Batch 2/2, Loss: 8.040466781267241e-08\n",
            "Validation: Batch 1/1, Loss: 7.212533148504008e-08\n",
            "Epoch: 2162/10000\n",
            "Training: Batch 1/2, Loss: 7.641045129958002e-08\n",
            "Training: Batch 2/2, Loss: 7.526880807517955e-08\n",
            "Validation: Batch 1/1, Loss: 7.112723920954522e-08\n",
            "Epoch: 2163/10000\n",
            "Training: Batch 1/2, Loss: 7.389706269123053e-08\n",
            "Training: Batch 2/2, Loss: 7.567523852003433e-08\n",
            "Validation: Batch 1/1, Loss: 7.014016034645465e-08\n",
            "Epoch: 2164/10000\n",
            "Training: Batch 1/2, Loss: 7.306049809585602e-08\n",
            "Training: Batch 2/2, Loss: 7.444378979926114e-08\n",
            "Validation: Batch 1/1, Loss: 6.918003947475881e-08\n",
            "Epoch: 2165/10000\n",
            "Training: Batch 1/2, Loss: 7.365046172935763e-08\n",
            "Training: Batch 2/2, Loss: 7.183875538885331e-08\n",
            "Validation: Batch 1/1, Loss: 6.822646270165933e-08\n",
            "Epoch: 2166/10000\n",
            "Training: Batch 1/2, Loss: 7.445369476499764e-08\n",
            "Training: Batch 2/2, Loss: 6.903740512598233e-08\n",
            "Validation: Batch 1/1, Loss: 6.72761615305717e-08\n",
            "Epoch: 2167/10000\n",
            "Training: Batch 1/2, Loss: 6.782425998608232e-08\n",
            "Training: Batch 2/2, Loss: 7.363177445540714e-08\n",
            "Validation: Batch 1/1, Loss: 6.634079596778975e-08\n",
            "Epoch: 2168/10000\n",
            "Training: Batch 1/2, Loss: 7.385734335230154e-08\n",
            "Training: Batch 2/2, Loss: 6.567667298895685e-08\n",
            "Validation: Batch 1/1, Loss: 6.541936414805605e-08\n",
            "Epoch: 2169/10000\n",
            "Training: Batch 1/2, Loss: 7.119005829281377e-08\n",
            "Training: Batch 2/2, Loss: 6.639887573101078e-08\n",
            "Validation: Batch 1/1, Loss: 6.450997602769348e-08\n",
            "Epoch: 2170/10000\n",
            "Training: Batch 1/2, Loss: 7.101726140490427e-08\n",
            "Training: Batch 2/2, Loss: 6.466544988370515e-08\n",
            "Validation: Batch 1/1, Loss: 6.361167947943613e-08\n",
            "Epoch: 2171/10000\n",
            "Training: Batch 1/2, Loss: 6.795757201416563e-08\n",
            "Training: Batch 2/2, Loss: 6.582023814871718e-08\n",
            "Validation: Batch 1/1, Loss: 6.272163943776832e-08\n",
            "Epoch: 2172/10000\n",
            "Training: Batch 1/2, Loss: 6.428108179079572e-08\n",
            "Training: Batch 2/2, Loss: 6.761460014104159e-08\n",
            "Validation: Batch 1/1, Loss: 6.18593176682225e-08\n",
            "Epoch: 2173/10000\n",
            "Training: Batch 1/2, Loss: 6.62356782754614e-08\n",
            "Training: Batch 2/2, Loss: 6.385512563156226e-08\n",
            "Validation: Batch 1/1, Loss: 6.099521243640993e-08\n",
            "Epoch: 2174/10000\n",
            "Training: Batch 1/2, Loss: 6.558489218377872e-08\n",
            "Training: Batch 2/2, Loss: 6.269361563226994e-08\n",
            "Validation: Batch 1/1, Loss: 6.014392539555047e-08\n",
            "Epoch: 2175/10000\n",
            "Training: Batch 1/2, Loss: 6.136991004268566e-08\n",
            "Training: Batch 2/2, Loss: 6.509385741537699e-08\n",
            "Validation: Batch 1/1, Loss: 5.930263213826947e-08\n",
            "Epoch: 2176/10000\n",
            "Training: Batch 1/2, Loss: 6.24801685944476e-08\n",
            "Training: Batch 2/2, Loss: 6.222693826885006e-08\n",
            "Validation: Batch 1/1, Loss: 5.846815298582442e-08\n",
            "Epoch: 2177/10000\n",
            "Training: Batch 1/2, Loss: 6.194176194185275e-08\n",
            "Training: Batch 2/2, Loss: 6.10157471214734e-08\n",
            "Validation: Batch 1/1, Loss: 5.764968236121604e-08\n",
            "Epoch: 2178/10000\n",
            "Training: Batch 1/2, Loss: 6.030737154105736e-08\n",
            "Training: Batch 2/2, Loss: 6.09242292171075e-08\n",
            "Validation: Batch 1/1, Loss: 5.684346149337216e-08\n",
            "Epoch: 2179/10000\n",
            "Training: Batch 1/2, Loss: 5.92098530205476e-08\n",
            "Training: Batch 2/2, Loss: 6.032286137269693e-08\n",
            "Validation: Batch 1/1, Loss: 5.6043337082201106e-08\n",
            "Epoch: 2180/10000\n",
            "Training: Batch 1/2, Loss: 5.893181409533099e-08\n",
            "Training: Batch 2/2, Loss: 5.892439247645598e-08\n",
            "Validation: Batch 1/1, Loss: 5.525191326682943e-08\n",
            "Epoch: 2181/10000\n",
            "Training: Batch 1/2, Loss: 5.651686407759371e-08\n",
            "Training: Batch 2/2, Loss: 5.966410299151903e-08\n",
            "Validation: Batch 1/1, Loss: 5.4471076538220586e-08\n",
            "Epoch: 2182/10000\n",
            "Training: Batch 1/2, Loss: 5.595838459271363e-08\n",
            "Training: Batch 2/2, Loss: 5.858795404378725e-08\n",
            "Validation: Batch 1/1, Loss: 5.370914379909664e-08\n",
            "Epoch: 2183/10000\n",
            "Training: Batch 1/2, Loss: 5.824867344017548e-08\n",
            "Training: Batch 2/2, Loss: 5.471464348261179e-08\n",
            "Validation: Batch 1/1, Loss: 5.295306237940167e-08\n",
            "Epoch: 2184/10000\n",
            "Training: Batch 1/2, Loss: 5.638147015929462e-08\n",
            "Training: Batch 2/2, Loss: 5.4981107666662865e-08\n",
            "Validation: Batch 1/1, Loss: 5.220371690484171e-08\n",
            "Epoch: 2185/10000\n",
            "Training: Batch 1/2, Loss: 5.513071954510451e-08\n",
            "Training: Batch 2/2, Loss: 5.465295771500678e-08\n",
            "Validation: Batch 1/1, Loss: 5.1461771732874695e-08\n",
            "Epoch: 2186/10000\n",
            "Training: Batch 1/2, Loss: 5.55129986423708e-08\n",
            "Training: Batch 2/2, Loss: 5.272217151741643e-08\n",
            "Validation: Batch 1/1, Loss: 5.073676234701452e-08\n",
            "Epoch: 2187/10000\n",
            "Training: Batch 1/2, Loss: 5.1368626685643903e-08\n",
            "Training: Batch 2/2, Loss: 5.5318462699460724e-08\n",
            "Validation: Batch 1/1, Loss: 5.0025903419737006e-08\n",
            "Epoch: 2188/10000\n",
            "Training: Batch 1/2, Loss: 4.889269789032369e-08\n",
            "Training: Batch 2/2, Loss: 5.628257682133153e-08\n",
            "Validation: Batch 1/1, Loss: 4.931922958917312e-08\n",
            "Epoch: 2189/10000\n",
            "Training: Batch 1/2, Loss: 5.191287399952671e-08\n",
            "Training: Batch 2/2, Loss: 5.180026008133609e-08\n",
            "Validation: Batch 1/1, Loss: 4.8615426351261704e-08\n",
            "Epoch: 2190/10000\n",
            "Training: Batch 1/2, Loss: 5.090934962481697e-08\n",
            "Training: Batch 2/2, Loss: 5.1325347527608756e-08\n",
            "Validation: Batch 1/1, Loss: 4.79219615101556e-08\n",
            "Epoch: 2191/10000\n",
            "Training: Batch 1/2, Loss: 5.1763226593948275e-08\n",
            "Training: Batch 2/2, Loss: 4.902396000261433e-08\n",
            "Validation: Batch 1/1, Loss: 4.72390695449576e-08\n",
            "Epoch: 2192/10000\n",
            "Training: Batch 1/2, Loss: 5.015037629618746e-08\n",
            "Training: Batch 2/2, Loss: 4.919959550875319e-08\n",
            "Validation: Batch 1/1, Loss: 4.6568889189302354e-08\n",
            "Epoch: 2193/10000\n",
            "Training: Batch 1/2, Loss: 4.858868862811505e-08\n",
            "Training: Batch 2/2, Loss: 4.93447558369553e-08\n",
            "Validation: Batch 1/1, Loss: 4.590634716805653e-08\n",
            "Epoch: 2194/10000\n",
            "Training: Batch 1/2, Loss: 4.892038418802258e-08\n",
            "Training: Batch 2/2, Loss: 4.762650718248551e-08\n",
            "Validation: Batch 1/1, Loss: 4.524990870891088e-08\n",
            "Epoch: 2195/10000\n",
            "Training: Batch 1/2, Loss: 4.699229805282812e-08\n",
            "Training: Batch 2/2, Loss: 4.81628639192877e-08\n",
            "Validation: Batch 1/1, Loss: 4.4605187099477916e-08\n",
            "Epoch: 2196/10000\n",
            "Training: Batch 1/2, Loss: 4.701391276284994e-08\n",
            "Training: Batch 2/2, Loss: 4.679385057215768e-08\n",
            "Validation: Batch 1/1, Loss: 4.3972367080868935e-08\n",
            "Epoch: 2197/10000\n",
            "Training: Batch 1/2, Loss: 4.585153945413367e-08\n",
            "Training: Batch 2/2, Loss: 4.662075880901284e-08\n",
            "Validation: Batch 1/1, Loss: 4.334679104545103e-08\n",
            "Epoch: 2198/10000\n",
            "Training: Batch 1/2, Loss: 4.641136186478434e-08\n",
            "Training: Batch 2/2, Loss: 4.475259629543871e-08\n",
            "Validation: Batch 1/1, Loss: 4.272595433008064e-08\n",
            "Epoch: 2199/10000\n",
            "Training: Batch 1/2, Loss: 4.532108377475197e-08\n",
            "Training: Batch 2/2, Loss: 4.453110591384757e-08\n",
            "Validation: Batch 1/1, Loss: 4.211157644817831e-08\n",
            "Epoch: 2200/10000\n",
            "Training: Batch 1/2, Loss: 4.436893163983768e-08\n",
            "Training: Batch 2/2, Loss: 4.4190443304614746e-08\n",
            "Validation: Batch 1/1, Loss: 4.151005938979324e-08\n",
            "Epoch: 2201/10000\n",
            "Training: Batch 1/2, Loss: 4.503081996176661e-08\n",
            "Training: Batch 2/2, Loss: 4.227678473966989e-08\n",
            "Validation: Batch 1/1, Loss: 4.091843308628995e-08\n",
            "Epoch: 2202/10000\n",
            "Training: Batch 1/2, Loss: 4.341097437077224e-08\n",
            "Training: Batch 2/2, Loss: 4.2642479769483543e-08\n",
            "Validation: Batch 1/1, Loss: 4.033137201986392e-08\n",
            "Epoch: 2203/10000\n",
            "Training: Batch 1/2, Loss: 4.1274287099213325e-08\n",
            "Training: Batch 2/2, Loss: 4.3534015503610135e-08\n",
            "Validation: Batch 1/1, Loss: 3.975345563844712e-08\n",
            "Epoch: 2204/10000\n",
            "Training: Batch 1/2, Loss: 4.240504480890195e-08\n",
            "Training: Batch 2/2, Loss: 4.119664254176314e-08\n",
            "Validation: Batch 1/1, Loss: 3.9178257082994605e-08\n",
            "Epoch: 2205/10000\n",
            "Training: Batch 1/2, Loss: 4.260720842808041e-08\n",
            "Training: Batch 2/2, Loss: 3.97890538295087e-08\n",
            "Validation: Batch 1/1, Loss: 3.861161701479432e-08\n",
            "Epoch: 2206/10000\n",
            "Training: Batch 1/2, Loss: 3.938688308835481e-08\n",
            "Training: Batch 2/2, Loss: 4.180498081041151e-08\n",
            "Validation: Batch 1/1, Loss: 3.8059067009044156e-08\n",
            "Epoch: 2207/10000\n",
            "Training: Batch 1/2, Loss: 4.0439523729673965e-08\n",
            "Training: Batch 2/2, Loss: 3.959977945555693e-08\n",
            "Validation: Batch 1/1, Loss: 3.751226529402629e-08\n",
            "Epoch: 2208/10000\n",
            "Training: Batch 1/2, Loss: 3.957201855087078e-08\n",
            "Training: Batch 2/2, Loss: 3.9312485711207046e-08\n",
            "Validation: Batch 1/1, Loss: 3.696930406249521e-08\n",
            "Epoch: 2209/10000\n",
            "Training: Batch 1/2, Loss: 3.772587930939153e-08\n",
            "Training: Batch 2/2, Loss: 4.001223885552463e-08\n",
            "Validation: Batch 1/1, Loss: 3.643780388529194e-08\n",
            "Epoch: 2210/10000\n",
            "Training: Batch 1/2, Loss: 3.85572391792266e-08\n",
            "Training: Batch 2/2, Loss: 3.8068932894930185e-08\n",
            "Validation: Batch 1/1, Loss: 3.590802322150921e-08\n",
            "Epoch: 2211/10000\n",
            "Training: Batch 1/2, Loss: 3.795121372718313e-08\n",
            "Training: Batch 2/2, Loss: 3.756261435228225e-08\n",
            "Validation: Batch 1/1, Loss: 3.539174286970592e-08\n",
            "Epoch: 2212/10000\n",
            "Training: Batch 1/2, Loss: 3.5960937339041266e-08\n",
            "Training: Batch 2/2, Loss: 3.845685725423209e-08\n",
            "Validation: Batch 1/1, Loss: 3.4881267652053793e-08\n",
            "Epoch: 2213/10000\n",
            "Training: Batch 1/2, Loss: 3.705206452764287e-08\n",
            "Training: Batch 2/2, Loss: 3.630010780852899e-08\n",
            "Validation: Batch 1/1, Loss: 3.4374995294683686e-08\n",
            "Epoch: 2214/10000\n",
            "Training: Batch 1/2, Loss: 3.7569009236904094e-08\n",
            "Training: Batch 2/2, Loss: 3.472507970059269e-08\n",
            "Validation: Batch 1/1, Loss: 3.387263447507394e-08\n",
            "Epoch: 2215/10000\n",
            "Training: Batch 1/2, Loss: 3.614728072420803e-08\n",
            "Training: Batch 2/2, Loss: 3.5085061966810827e-08\n",
            "Validation: Batch 1/1, Loss: 3.337542864301213e-08\n",
            "Epoch: 2216/10000\n",
            "Training: Batch 1/2, Loss: 3.531276249191251e-08\n",
            "Training: Batch 2/2, Loss: 3.487170374683046e-08\n",
            "Validation: Batch 1/1, Loss: 3.2888159751109924e-08\n",
            "Epoch: 2217/10000\n",
            "Training: Batch 1/2, Loss: 3.616523969185437e-08\n",
            "Training: Batch 2/2, Loss: 3.300394624261571e-08\n",
            "Validation: Batch 1/1, Loss: 3.240766588419319e-08\n",
            "Epoch: 2218/10000\n",
            "Training: Batch 1/2, Loss: 3.404003123819166e-08\n",
            "Training: Batch 2/2, Loss: 3.411034654732248e-08\n",
            "Validation: Batch 1/1, Loss: 3.193496667108775e-08\n",
            "Epoch: 2219/10000\n",
            "Training: Batch 1/2, Loss: 3.2887033540873745e-08\n",
            "Training: Batch 2/2, Loss: 3.4262622961023226e-08\n",
            "Validation: Batch 1/1, Loss: 3.146690019661946e-08\n",
            "Epoch: 2220/10000\n",
            "Training: Batch 1/2, Loss: 3.198248066382803e-08\n",
            "Training: Batch 2/2, Loss: 3.418060501303444e-08\n",
            "Validation: Batch 1/1, Loss: 3.100388212828875e-08\n",
            "Epoch: 2221/10000\n",
            "Training: Batch 1/2, Loss: 3.4787074554287756e-08\n",
            "Training: Batch 2/2, Loss: 3.042239882233844e-08\n",
            "Validation: Batch 1/1, Loss: 3.054829633697409e-08\n",
            "Epoch: 2222/10000\n",
            "Training: Batch 1/2, Loss: 3.196127096316559e-08\n",
            "Training: Batch 2/2, Loss: 3.2278201445024024e-08\n",
            "Validation: Batch 1/1, Loss: 3.010432081396175e-08\n",
            "Epoch: 2223/10000\n",
            "Training: Batch 1/2, Loss: 3.135589921043902e-08\n",
            "Training: Batch 2/2, Loss: 3.194628206415473e-08\n",
            "Validation: Batch 1/1, Loss: 2.9663146605685142e-08\n",
            "Epoch: 2224/10000\n",
            "Training: Batch 1/2, Loss: 3.158996619845311e-08\n",
            "Training: Batch 2/2, Loss: 3.07884775452294e-08\n",
            "Validation: Batch 1/1, Loss: 2.9228823805738102e-08\n",
            "Epoch: 2225/10000\n",
            "Training: Batch 1/2, Loss: 3.0688227070641005e-08\n",
            "Training: Batch 2/2, Loss: 3.0772021375469194e-08\n",
            "Validation: Batch 1/1, Loss: 2.879631821883777e-08\n",
            "Epoch: 2226/10000\n",
            "Training: Batch 1/2, Loss: 2.999984616280926e-08\n",
            "Training: Batch 2/2, Loss: 3.055310315858151e-08\n",
            "Validation: Batch 1/1, Loss: 2.8369759874635747e-08\n",
            "Epoch: 2227/10000\n",
            "Training: Batch 1/2, Loss: 2.9660096600991892e-08\n",
            "Training: Batch 2/2, Loss: 3.000006998377103e-08\n",
            "Validation: Batch 1/1, Loss: 2.7956479797808242e-08\n",
            "Epoch: 2228/10000\n",
            "Training: Batch 1/2, Loss: 2.997713366426069e-08\n",
            "Training: Batch 2/2, Loss: 2.881633598406097e-08\n",
            "Validation: Batch 1/1, Loss: 2.754692829398664e-08\n",
            "Epoch: 2229/10000\n",
            "Training: Batch 1/2, Loss: 2.8054444101144327e-08\n",
            "Training: Batch 2/2, Loss: 2.986588754083641e-08\n",
            "Validation: Batch 1/1, Loss: 2.7141560110521823e-08\n",
            "Epoch: 2230/10000\n",
            "Training: Batch 1/2, Loss: 2.87084986894115e-08\n",
            "Training: Batch 2/2, Loss: 2.8365860771373264e-08\n",
            "Validation: Batch 1/1, Loss: 2.6739757075233683e-08\n",
            "Epoch: 2231/10000\n",
            "Training: Batch 1/2, Loss: 2.6447027678955237e-08\n",
            "Training: Batch 2/2, Loss: 2.976939406096335e-08\n",
            "Validation: Batch 1/1, Loss: 2.6342714676275136e-08\n",
            "Epoch: 2232/10000\n",
            "Training: Batch 1/2, Loss: 2.8182398637000006e-08\n",
            "Training: Batch 2/2, Loss: 2.7215332210062115e-08\n",
            "Validation: Batch 1/1, Loss: 2.5950539495056546e-08\n",
            "Epoch: 2233/10000\n",
            "Training: Batch 1/2, Loss: 2.7461641849413354e-08\n",
            "Training: Batch 2/2, Loss: 2.7111319411687873e-08\n",
            "Validation: Batch 1/1, Loss: 2.5568370531914297e-08\n",
            "Epoch: 2234/10000\n",
            "Training: Batch 1/2, Loss: 2.8554824282878144e-08\n",
            "Training: Batch 2/2, Loss: 2.5220506572054546e-08\n",
            "Validation: Batch 1/1, Loss: 2.5186531971144177e-08\n",
            "Epoch: 2235/10000\n",
            "Training: Batch 1/2, Loss: 2.613913885340935e-08\n",
            "Training: Batch 2/2, Loss: 2.6823091303640467e-08\n",
            "Validation: Batch 1/1, Loss: 2.4813704868620334e-08\n",
            "Epoch: 2236/10000\n",
            "Training: Batch 1/2, Loss: 2.5348020571414054e-08\n",
            "Training: Batch 2/2, Loss: 2.6825080823300596e-08\n",
            "Validation: Batch 1/1, Loss: 2.4444208435170367e-08\n",
            "Epoch: 2237/10000\n",
            "Training: Batch 1/2, Loss: 2.7159774873553033e-08\n",
            "Training: Batch 2/2, Loss: 2.4253823838193966e-08\n",
            "Validation: Batch 1/1, Loss: 2.407889176936351e-08\n",
            "Epoch: 2238/10000\n",
            "Training: Batch 1/2, Loss: 2.6778105066682656e-08\n",
            "Training: Batch 2/2, Loss: 2.3869107579344018e-08\n",
            "Validation: Batch 1/1, Loss: 2.372432739150554e-08\n",
            "Epoch: 2239/10000\n",
            "Training: Batch 1/2, Loss: 2.5282645310653606e-08\n",
            "Training: Batch 2/2, Loss: 2.4607789583797057e-08\n",
            "Validation: Batch 1/1, Loss: 2.337317539513606e-08\n",
            "Epoch: 2240/10000\n",
            "Training: Batch 1/2, Loss: 2.5143126691773432e-08\n",
            "Training: Batch 2/2, Loss: 2.40096724724026e-08\n",
            "Validation: Batch 1/1, Loss: 2.302693324907068e-08\n",
            "Epoch: 2241/10000\n",
            "Training: Batch 1/2, Loss: 2.4532528897225347e-08\n",
            "Training: Batch 2/2, Loss: 2.3889970890422774e-08\n",
            "Validation: Batch 1/1, Loss: 2.268427579110721e-08\n",
            "Epoch: 2242/10000\n",
            "Training: Batch 1/2, Loss: 2.326319936685195e-08\n",
            "Training: Batch 2/2, Loss: 2.4429326117569872e-08\n",
            "Validation: Batch 1/1, Loss: 2.234314067095511e-08\n",
            "Epoch: 2243/10000\n",
            "Training: Batch 1/2, Loss: 2.311478120020638e-08\n",
            "Training: Batch 2/2, Loss: 2.386208031168735e-08\n",
            "Validation: Batch 1/1, Loss: 2.2005909983136007e-08\n",
            "Epoch: 2244/10000\n",
            "Training: Batch 1/2, Loss: 2.2750956674144618e-08\n",
            "Training: Batch 2/2, Loss: 2.3518087033380652e-08\n",
            "Validation: Batch 1/1, Loss: 2.1674159356166456e-08\n",
            "Epoch: 2245/10000\n",
            "Training: Batch 1/2, Loss: 2.2956880840752092e-08\n",
            "Training: Batch 2/2, Loss: 2.262247100759396e-08\n",
            "Validation: Batch 1/1, Loss: 2.135290699811776e-08\n",
            "Epoch: 2246/10000\n",
            "Training: Batch 1/2, Loss: 2.1008554895729503e-08\n",
            "Training: Batch 2/2, Loss: 2.388124187291396e-08\n",
            "Validation: Batch 1/1, Loss: 2.1034699315691796e-08\n",
            "Epoch: 2247/10000\n",
            "Training: Batch 1/2, Loss: 2.2365041374428074e-08\n",
            "Training: Batch 2/2, Loss: 2.1866078725452098e-08\n",
            "Validation: Batch 1/1, Loss: 2.071827509553259e-08\n",
            "Epoch: 2248/10000\n",
            "Training: Batch 1/2, Loss: 2.1310606612701122e-08\n",
            "Training: Batch 2/2, Loss: 2.2250112863275717e-08\n",
            "Validation: Batch 1/1, Loss: 2.0407183498605264e-08\n",
            "Epoch: 2249/10000\n",
            "Training: Batch 1/2, Loss: 2.1700019559034445e-08\n",
            "Training: Batch 2/2, Loss: 2.1209222822449192e-08\n",
            "Validation: Batch 1/1, Loss: 2.0097289166187693e-08\n",
            "Epoch: 2250/10000\n",
            "Training: Batch 1/2, Loss: 2.0711393489136753e-08\n",
            "Training: Batch 2/2, Loss: 2.1541888273191034e-08\n",
            "Validation: Batch 1/1, Loss: 1.979152131070805e-08\n",
            "Epoch: 2251/10000\n",
            "Training: Batch 1/2, Loss: 2.1118234272421432e-08\n",
            "Training: Batch 2/2, Loss: 2.0500070974094342e-08\n",
            "Validation: Batch 1/1, Loss: 1.949568861903117e-08\n",
            "Epoch: 2252/10000\n",
            "Training: Batch 1/2, Loss: 2.0207483686363048e-08\n",
            "Training: Batch 2/2, Loss: 2.0783296861281997e-08\n",
            "Validation: Batch 1/1, Loss: 1.9204478007850412e-08\n",
            "Epoch: 2253/10000\n",
            "Training: Batch 1/2, Loss: 2.112564878586909e-08\n",
            "Training: Batch 2/2, Loss: 1.926173887056848e-08\n",
            "Validation: Batch 1/1, Loss: 1.891484124882936e-08\n",
            "Epoch: 2254/10000\n",
            "Training: Batch 1/2, Loss: 1.9236610526718323e-08\n",
            "Training: Batch 2/2, Loss: 2.0529324018525585e-08\n",
            "Validation: Batch 1/1, Loss: 1.8629082276788722e-08\n",
            "Epoch: 2255/10000\n",
            "Training: Batch 1/2, Loss: 1.9465495881831885e-08\n",
            "Training: Batch 2/2, Loss: 1.970343532775587e-08\n",
            "Validation: Batch 1/1, Loss: 1.8345192032143132e-08\n",
            "Epoch: 2256/10000\n",
            "Training: Batch 1/2, Loss: 1.9328441069887958e-08\n",
            "Training: Batch 2/2, Loss: 1.9243243443156643e-08\n",
            "Validation: Batch 1/1, Loss: 1.806373184365384e-08\n",
            "Epoch: 2257/10000\n",
            "Training: Batch 1/2, Loss: 1.8688586678194952e-08\n",
            "Training: Batch 2/2, Loss: 1.9288250996396528e-08\n",
            "Validation: Batch 1/1, Loss: 1.7787264994240104e-08\n",
            "Epoch: 2258/10000\n",
            "Training: Batch 1/2, Loss: 1.9075924839739855e-08\n",
            "Training: Batch 2/2, Loss: 1.832710339044752e-08\n",
            "Validation: Batch 1/1, Loss: 1.7519699468948602e-08\n",
            "Epoch: 2259/10000\n",
            "Training: Batch 1/2, Loss: 1.863059928552957e-08\n",
            "Training: Batch 2/2, Loss: 1.820448325418056e-08\n",
            "Validation: Batch 1/1, Loss: 1.7250899375653717e-08\n",
            "Epoch: 2260/10000\n",
            "Training: Batch 1/2, Loss: 1.7748819303164964e-08\n",
            "Training: Batch 2/2, Loss: 1.8519576983067054e-08\n",
            "Validation: Batch 1/1, Loss: 1.698788310022792e-08\n",
            "Epoch: 2261/10000\n",
            "Training: Batch 1/2, Loss: 1.8967494241906024e-08\n",
            "Training: Batch 2/2, Loss: 1.6758674448169586e-08\n",
            "Validation: Batch 1/1, Loss: 1.672868421564999e-08\n",
            "Epoch: 2262/10000\n",
            "Training: Batch 1/2, Loss: 1.791131687411962e-08\n",
            "Training: Batch 2/2, Loss: 1.7263637630549056e-08\n",
            "Validation: Batch 1/1, Loss: 1.6472124997335413e-08\n",
            "Epoch: 2263/10000\n",
            "Training: Batch 1/2, Loss: 1.7432816079576696e-08\n",
            "Training: Batch 2/2, Loss: 1.720210640598907e-08\n",
            "Validation: Batch 1/1, Loss: 1.621913803262487e-08\n",
            "Epoch: 2264/10000\n",
            "Training: Batch 1/2, Loss: 1.6757041976234177e-08\n",
            "Training: Batch 2/2, Loss: 1.7340113345198915e-08\n",
            "Validation: Batch 1/1, Loss: 1.5967158262242265e-08\n",
            "Epoch: 2265/10000\n",
            "Training: Batch 1/2, Loss: 1.654813708285019e-08\n",
            "Training: Batch 2/2, Loss: 1.702573193540502e-08\n",
            "Validation: Batch 1/1, Loss: 1.572711916253411e-08\n",
            "Epoch: 2266/10000\n",
            "Training: Batch 1/2, Loss: 1.704109919842267e-08\n",
            "Training: Batch 2/2, Loss: 1.6029140681439458e-08\n",
            "Validation: Batch 1/1, Loss: 1.5484967974543906e-08\n",
            "Epoch: 2267/10000\n",
            "Training: Batch 1/2, Loss: 1.6226046284373297e-08\n",
            "Training: Batch 2/2, Loss: 1.6333780550326082e-08\n",
            "Validation: Batch 1/1, Loss: 1.524845316680512e-08\n",
            "Epoch: 2268/10000\n",
            "Training: Batch 1/2, Loss: 1.627460832764882e-08\n",
            "Training: Batch 2/2, Loss: 1.578851005490378e-08\n",
            "Validation: Batch 1/1, Loss: 1.5013506882155525e-08\n",
            "Epoch: 2269/10000\n",
            "Training: Batch 1/2, Loss: 1.594416332295623e-08\n",
            "Training: Batch 2/2, Loss: 1.5622866555986548e-08\n",
            "Validation: Batch 1/1, Loss: 1.478086897321873e-08\n",
            "Epoch: 2270/10000\n",
            "Training: Batch 1/2, Loss: 1.5958327992393606e-08\n",
            "Training: Batch 2/2, Loss: 1.512183978036319e-08\n",
            "Validation: Batch 1/1, Loss: 1.455129705618674e-08\n",
            "Epoch: 2271/10000\n",
            "Training: Batch 1/2, Loss: 1.5561978372602425e-08\n",
            "Training: Batch 2/2, Loss: 1.503564917015865e-08\n",
            "Validation: Batch 1/1, Loss: 1.4325084229938057e-08\n",
            "Epoch: 2272/10000\n",
            "Training: Batch 1/2, Loss: 1.524258763652142e-08\n",
            "Training: Batch 2/2, Loss: 1.4882338028598951e-08\n",
            "Validation: Batch 1/1, Loss: 1.4107384593842198e-08\n",
            "Epoch: 2273/10000\n",
            "Training: Batch 1/2, Loss: 1.467634813678842e-08\n",
            "Training: Batch 2/2, Loss: 1.4986278884521198e-08\n",
            "Validation: Batch 1/1, Loss: 1.3891537697929834e-08\n",
            "Epoch: 2274/10000\n",
            "Training: Batch 1/2, Loss: 1.4470982634406937e-08\n",
            "Training: Batch 2/2, Loss: 1.473807209606548e-08\n",
            "Validation: Batch 1/1, Loss: 1.367827007214828e-08\n",
            "Epoch: 2275/10000\n",
            "Training: Batch 1/2, Loss: 1.4464298203620274e-08\n",
            "Training: Batch 2/2, Loss: 1.4295430617039528e-08\n",
            "Validation: Batch 1/1, Loss: 1.3467012394130506e-08\n",
            "Epoch: 2276/10000\n",
            "Training: Batch 1/2, Loss: 1.4447147478335864e-08\n",
            "Training: Batch 2/2, Loss: 1.3870170789687108e-08\n",
            "Validation: Batch 1/1, Loss: 1.325834286802774e-08\n",
            "Epoch: 2277/10000\n",
            "Training: Batch 1/2, Loss: 1.4811172732720479e-08\n",
            "Training: Batch 2/2, Loss: 1.307082797552539e-08\n",
            "Validation: Batch 1/1, Loss: 1.3050970082417734e-08\n",
            "Epoch: 2278/10000\n",
            "Training: Batch 1/2, Loss: 1.3922941022315172e-08\n",
            "Training: Batch 2/2, Loss: 1.3518999253392394e-08\n",
            "Validation: Batch 1/1, Loss: 1.2845908337055789e-08\n",
            "Epoch: 2279/10000\n",
            "Training: Batch 1/2, Loss: 1.3692433853407238e-08\n",
            "Training: Batch 2/2, Loss: 1.3318028457831588e-08\n",
            "Validation: Batch 1/1, Loss: 1.2646201419386216e-08\n",
            "Epoch: 2280/10000\n",
            "Training: Batch 1/2, Loss: 1.3305889723369546e-08\n",
            "Training: Batch 2/2, Loss: 1.3285010425079236e-08\n",
            "Validation: Batch 1/1, Loss: 1.245153580242686e-08\n",
            "Epoch: 2281/10000\n",
            "Training: Batch 1/2, Loss: 1.3089492156836968e-08\n",
            "Training: Batch 2/2, Loss: 1.3092873452080767e-08\n",
            "Validation: Batch 1/1, Loss: 1.2259973480865938e-08\n",
            "Epoch: 2282/10000\n",
            "Training: Batch 1/2, Loss: 1.2865848830756477e-08\n",
            "Training: Batch 2/2, Loss: 1.2912235725082155e-08\n",
            "Validation: Batch 1/1, Loss: 1.2070009880460475e-08\n",
            "Epoch: 2283/10000\n",
            "Training: Batch 1/2, Loss: 1.3515779606620981e-08\n",
            "Training: Batch 2/2, Loss: 1.1868667826320234e-08\n",
            "Validation: Batch 1/1, Loss: 1.1881042816241916e-08\n",
            "Epoch: 2284/10000\n",
            "Training: Batch 1/2, Loss: 1.2316792030730994e-08\n",
            "Training: Batch 2/2, Loss: 1.2662171755550844e-08\n",
            "Validation: Batch 1/1, Loss: 1.1694023527297759e-08\n",
            "Epoch: 2285/10000\n",
            "Training: Batch 1/2, Loss: 1.251660908252461e-08\n",
            "Training: Batch 2/2, Loss: 1.2072587374234445e-08\n",
            "Validation: Batch 1/1, Loss: 1.1510007169590608e-08\n",
            "Epoch: 2286/10000\n",
            "Training: Batch 1/2, Loss: 1.2273657645778258e-08\n",
            "Training: Batch 2/2, Loss: 1.1928522170023825e-08\n",
            "Validation: Batch 1/1, Loss: 1.1328358695550378e-08\n",
            "Epoch: 2287/10000\n",
            "Training: Batch 1/2, Loss: 1.1608339178792448e-08\n",
            "Training: Batch 2/2, Loss: 1.2207643784734046e-08\n",
            "Validation: Batch 1/1, Loss: 1.1150801171311286e-08\n",
            "Epoch: 2288/10000\n",
            "Training: Batch 1/2, Loss: 1.196642074319243e-08\n",
            "Training: Batch 2/2, Loss: 1.148047346077874e-08\n",
            "Validation: Batch 1/1, Loss: 1.097553781193028e-08\n",
            "Epoch: 2289/10000\n",
            "Training: Batch 1/2, Loss: 1.1908316999154067e-08\n",
            "Training: Batch 2/2, Loss: 1.117336090317167e-08\n",
            "Validation: Batch 1/1, Loss: 1.0804093619753985e-08\n",
            "Epoch: 2290/10000\n",
            "Training: Batch 1/2, Loss: 1.1218429740722513e-08\n",
            "Training: Batch 2/2, Loss: 1.1498743290871971e-08\n",
            "Validation: Batch 1/1, Loss: 1.0634943592435775e-08\n",
            "Epoch: 2291/10000\n",
            "Training: Batch 1/2, Loss: 1.1367333740963659e-08\n",
            "Training: Batch 2/2, Loss: 1.099467361598272e-08\n",
            "Validation: Batch 1/1, Loss: 1.0467265276759008e-08\n",
            "Epoch: 2292/10000\n",
            "Training: Batch 1/2, Loss: 1.0370024838834979e-08\n",
            "Training: Batch 2/2, Loss: 1.1633461305393666e-08\n",
            "Validation: Batch 1/1, Loss: 1.0301389075095813e-08\n",
            "Epoch: 2293/10000\n",
            "Training: Batch 1/2, Loss: 1.085869261174821e-08\n",
            "Training: Batch 2/2, Loss: 1.0801137761973223e-08\n",
            "Validation: Batch 1/1, Loss: 1.013736827815137e-08\n",
            "Epoch: 2294/10000\n",
            "Training: Batch 1/2, Loss: 1.0932167171517904e-08\n",
            "Training: Batch 2/2, Loss: 1.0384391124773629e-08\n",
            "Validation: Batch 1/1, Loss: 9.974513659471995e-09\n",
            "Epoch: 2295/10000\n",
            "Training: Batch 1/2, Loss: 1.0336180800152306e-08\n",
            "Training: Batch 2/2, Loss: 1.0636877156855462e-08\n",
            "Validation: Batch 1/1, Loss: 9.81950520895225e-09\n",
            "Epoch: 2296/10000\n",
            "Training: Batch 1/2, Loss: 9.99052396366551e-09\n",
            "Training: Batch 2/2, Loss: 1.0654857440783871e-08\n",
            "Validation: Batch 1/1, Loss: 9.667025402393392e-09\n",
            "Epoch: 2297/10000\n",
            "Training: Batch 1/2, Loss: 1.0298616182069509e-08\n",
            "Training: Batch 2/2, Loss: 1.0028724517496812e-08\n",
            "Validation: Batch 1/1, Loss: 9.515783716551596e-09\n",
            "Epoch: 2298/10000\n",
            "Training: Batch 1/2, Loss: 1.0042862541581599e-08\n",
            "Training: Batch 2/2, Loss: 9.964702840647988e-09\n",
            "Validation: Batch 1/1, Loss: 9.365750841539011e-09\n",
            "Epoch: 2299/10000\n",
            "Training: Batch 1/2, Loss: 9.767929576298684e-09\n",
            "Training: Batch 2/2, Loss: 9.922919375071615e-09\n",
            "Validation: Batch 1/1, Loss: 9.2171008603259e-09\n",
            "Epoch: 2300/10000\n",
            "Training: Batch 1/2, Loss: 9.613073892467128e-09\n",
            "Training: Batch 2/2, Loss: 9.764190345151746e-09\n",
            "Validation: Batch 1/1, Loss: 9.069251127868938e-09\n",
            "Epoch: 2301/10000\n",
            "Training: Batch 1/2, Loss: 9.446590176764857e-09\n",
            "Training: Batch 2/2, Loss: 9.61931512222236e-09\n",
            "Validation: Batch 1/1, Loss: 8.923897176771334e-09\n",
            "Epoch: 2302/10000\n",
            "Training: Batch 1/2, Loss: 9.092694597256923e-09\n",
            "Training: Batch 2/2, Loss: 9.667199485363653e-09\n",
            "Validation: Batch 1/1, Loss: 8.780933313801143e-09\n",
            "Epoch: 2303/10000\n",
            "Training: Batch 1/2, Loss: 9.183728444384087e-09\n",
            "Training: Batch 2/2, Loss: 9.277242973837474e-09\n",
            "Validation: Batch 1/1, Loss: 8.641372950535242e-09\n",
            "Epoch: 2304/10000\n",
            "Training: Batch 1/2, Loss: 8.96469121158816e-09\n",
            "Training: Batch 2/2, Loss: 9.204235595916543e-09\n",
            "Validation: Batch 1/1, Loss: 8.505270265857234e-09\n",
            "Epoch: 2305/10000\n",
            "Training: Batch 1/2, Loss: 8.762052416955157e-09\n",
            "Training: Batch 2/2, Loss: 9.119679234004252e-09\n",
            "Validation: Batch 1/1, Loss: 8.371207727009278e-09\n",
            "Epoch: 2306/10000\n",
            "Training: Batch 1/2, Loss: 8.929985639838378e-09\n",
            "Training: Batch 2/2, Loss: 8.670232531926558e-09\n",
            "Validation: Batch 1/1, Loss: 8.237196702509664e-09\n",
            "Epoch: 2307/10000\n",
            "Training: Batch 1/2, Loss: 8.906574144873503e-09\n",
            "Training: Batch 2/2, Loss: 8.413659990935685e-09\n",
            "Validation: Batch 1/1, Loss: 8.106072257874075e-09\n",
            "Epoch: 2308/10000\n",
            "Training: Batch 1/2, Loss: 8.67446470209643e-09\n",
            "Training: Batch 2/2, Loss: 8.369256399021197e-09\n",
            "Validation: Batch 1/1, Loss: 7.977056348806855e-09\n",
            "Epoch: 2309/10000\n",
            "Training: Batch 1/2, Loss: 8.345667268372381e-09\n",
            "Training: Batch 2/2, Loss: 8.424149378072343e-09\n",
            "Validation: Batch 1/1, Loss: 7.848454330883214e-09\n",
            "Epoch: 2310/10000\n",
            "Training: Batch 1/2, Loss: 7.933134149595844e-09\n",
            "Training: Batch 2/2, Loss: 8.564925657594813e-09\n",
            "Validation: Batch 1/1, Loss: 7.721780548308743e-09\n",
            "Epoch: 2311/10000\n",
            "Training: Batch 1/2, Loss: 8.02490429663294e-09\n",
            "Training: Batch 2/2, Loss: 8.207510227009607e-09\n",
            "Validation: Batch 1/1, Loss: 7.595475359778447e-09\n",
            "Epoch: 2312/10000\n",
            "Training: Batch 1/2, Loss: 8.285120145501423e-09\n",
            "Training: Batch 2/2, Loss: 7.686825398423025e-09\n",
            "Validation: Batch 1/1, Loss: 7.475053465100245e-09\n",
            "Epoch: 2313/10000\n",
            "Training: Batch 1/2, Loss: 7.898249165805282e-09\n",
            "Training: Batch 2/2, Loss: 7.818366398737453e-09\n",
            "Validation: Batch 1/1, Loss: 7.357506159877403e-09\n",
            "Epoch: 2314/10000\n",
            "Training: Batch 1/2, Loss: 7.795338596849888e-09\n",
            "Training: Batch 2/2, Loss: 7.672678492554041e-09\n",
            "Validation: Batch 1/1, Loss: 7.241017119241633e-09\n",
            "Epoch: 2315/10000\n",
            "Training: Batch 1/2, Loss: 7.79562814301471e-09\n",
            "Training: Batch 2/2, Loss: 7.42797734432088e-09\n",
            "Validation: Batch 1/1, Loss: 7.125504630778323e-09\n",
            "Epoch: 2316/10000\n",
            "Training: Batch 1/2, Loss: 7.257057621501417e-09\n",
            "Training: Batch 2/2, Loss: 7.720186268045381e-09\n",
            "Validation: Batch 1/1, Loss: 7.011173419613215e-09\n",
            "Epoch: 2317/10000\n",
            "Training: Batch 1/2, Loss: 7.487140685213944e-09\n",
            "Training: Batch 2/2, Loss: 7.252001665847274e-09\n",
            "Validation: Batch 1/1, Loss: 6.897633575420059e-09\n",
            "Epoch: 2318/10000\n",
            "Training: Batch 1/2, Loss: 7.186894190880366e-09\n",
            "Training: Batch 2/2, Loss: 7.31172367096633e-09\n",
            "Validation: Batch 1/1, Loss: 6.7849286189414215e-09\n",
            "Epoch: 2319/10000\n",
            "Training: Batch 1/2, Loss: 7.472913843287188e-09\n",
            "Training: Batch 2/2, Loss: 6.792049589421367e-09\n",
            "Validation: Batch 1/1, Loss: 6.6740168946921585e-09\n",
            "Epoch: 2320/10000\n",
            "Training: Batch 1/2, Loss: 6.882285852327641e-09\n",
            "Training: Batch 2/2, Loss: 7.146213842901261e-09\n",
            "Validation: Batch 1/1, Loss: 6.564993437763178e-09\n",
            "Epoch: 2321/10000\n",
            "Training: Batch 1/2, Loss: 6.682876474428667e-09\n",
            "Training: Batch 2/2, Loss: 7.114931754870213e-09\n",
            "Validation: Batch 1/1, Loss: 6.456294610046598e-09\n",
            "Epoch: 2322/10000\n",
            "Training: Batch 1/2, Loss: 6.5619678579764695e-09\n",
            "Training: Batch 2/2, Loss: 7.010335423274228e-09\n",
            "Validation: Batch 1/1, Loss: 6.353646053725015e-09\n",
            "Epoch: 2323/10000\n",
            "Training: Batch 1/2, Loss: 6.564105259343478e-09\n",
            "Training: Batch 2/2, Loss: 6.791355477986372e-09\n",
            "Validation: Batch 1/1, Loss: 6.251530848544462e-09\n",
            "Epoch: 2324/10000\n",
            "Training: Batch 1/2, Loss: 6.423183762649387e-09\n",
            "Training: Batch 2/2, Loss: 6.716700973186107e-09\n",
            "Validation: Batch 1/1, Loss: 6.150586706610284e-09\n",
            "Epoch: 2325/10000\n",
            "Training: Batch 1/2, Loss: 6.507276939515805e-09\n",
            "Training: Batch 2/2, Loss: 6.4211951311676785e-09\n",
            "Validation: Batch 1/1, Loss: 6.05122840724448e-09\n",
            "Epoch: 2326/10000\n",
            "Training: Batch 1/2, Loss: 6.314584410915813e-09\n",
            "Training: Batch 2/2, Loss: 6.404980101848423e-09\n",
            "Validation: Batch 1/1, Loss: 5.953535886504824e-09\n",
            "Epoch: 2327/10000\n",
            "Training: Batch 1/2, Loss: 6.034153621214955e-09\n",
            "Training: Batch 2/2, Loss: 6.478015457389574e-09\n",
            "Validation: Batch 1/1, Loss: 5.856137796911298e-09\n",
            "Epoch: 2328/10000\n",
            "Training: Batch 1/2, Loss: 6.099865057507259e-09\n",
            "Training: Batch 2/2, Loss: 6.208931591089595e-09\n",
            "Validation: Batch 1/1, Loss: 5.760155463718775e-09\n",
            "Epoch: 2329/10000\n",
            "Training: Batch 1/2, Loss: 6.112174766315093e-09\n",
            "Training: Batch 2/2, Loss: 5.9955946873913035e-09\n",
            "Validation: Batch 1/1, Loss: 5.664573254904326e-09\n",
            "Epoch: 2330/10000\n",
            "Training: Batch 1/2, Loss: 5.972570882306627e-09\n",
            "Training: Batch 2/2, Loss: 5.9344595904065045e-09\n",
            "Validation: Batch 1/1, Loss: 5.570091499151886e-09\n",
            "Epoch: 2331/10000\n",
            "Training: Batch 1/2, Loss: 5.798143298818559e-09\n",
            "Training: Batch 2/2, Loss: 5.9100599969497125e-09\n",
            "Validation: Batch 1/1, Loss: 5.477033493406225e-09\n",
            "Epoch: 2332/10000\n",
            "Training: Batch 1/2, Loss: 5.716680018252873e-09\n",
            "Training: Batch 2/2, Loss: 5.797928803730201e-09\n",
            "Validation: Batch 1/1, Loss: 5.38973310426627e-09\n",
            "Epoch: 2333/10000\n",
            "Training: Batch 1/2, Loss: 5.6366502576565836e-09\n",
            "Training: Batch 2/2, Loss: 5.694013704982126e-09\n",
            "Validation: Batch 1/1, Loss: 5.303204098083825e-09\n",
            "Epoch: 2334/10000\n",
            "Training: Batch 1/2, Loss: 5.488098420158849e-09\n",
            "Training: Batch 2/2, Loss: 5.658801427443905e-09\n",
            "Validation: Batch 1/1, Loss: 5.217035248250568e-09\n",
            "Epoch: 2335/10000\n",
            "Training: Batch 1/2, Loss: 5.320416995857613e-09\n",
            "Training: Batch 2/2, Loss: 5.644522627079596e-09\n",
            "Validation: Batch 1/1, Loss: 5.131789215795379e-09\n",
            "Epoch: 2336/10000\n",
            "Training: Batch 1/2, Loss: 5.546641368425753e-09\n",
            "Training: Batch 2/2, Loss: 5.2410564777005675e-09\n",
            "Validation: Batch 1/1, Loss: 5.047485096554283e-09\n",
            "Epoch: 2337/10000\n",
            "Training: Batch 1/2, Loss: 5.365667021806075e-09\n",
            "Training: Batch 2/2, Loss: 5.243709466640212e-09\n",
            "Validation: Batch 1/1, Loss: 4.964030964060839e-09\n",
            "Epoch: 2338/10000\n",
            "Training: Batch 1/2, Loss: 5.313094408876395e-09\n",
            "Training: Batch 2/2, Loss: 5.1204094297929714e-09\n",
            "Validation: Batch 1/1, Loss: 4.881696824554638e-09\n",
            "Epoch: 2339/10000\n",
            "Training: Batch 1/2, Loss: 5.327819518896604e-09\n",
            "Training: Batch 2/2, Loss: 4.933744968127485e-09\n",
            "Validation: Batch 1/1, Loss: 4.800260189341543e-09\n",
            "Epoch: 2340/10000\n",
            "Training: Batch 1/2, Loss: 5.119661139474374e-09\n",
            "Training: Batch 2/2, Loss: 4.970287292849207e-09\n",
            "Validation: Batch 1/1, Loss: 4.7200763297894355e-09\n",
            "Epoch: 2341/10000\n",
            "Training: Batch 1/2, Loss: 4.9780637390028915e-09\n",
            "Training: Batch 2/2, Loss: 4.943213838259908e-09\n",
            "Validation: Batch 1/1, Loss: 4.640600792527039e-09\n",
            "Epoch: 2342/10000\n",
            "Training: Batch 1/2, Loss: 4.921241636424156e-09\n",
            "Training: Batch 2/2, Loss: 4.833068611986846e-09\n",
            "Validation: Batch 1/1, Loss: 4.563640576549233e-09\n",
            "Epoch: 2343/10000\n",
            "Training: Batch 1/2, Loss: 4.800259301163123e-09\n",
            "Training: Batch 2/2, Loss: 4.793349717147066e-09\n",
            "Validation: Batch 1/1, Loss: 4.4894532535977305e-09\n",
            "Epoch: 2344/10000\n",
            "Training: Batch 1/2, Loss: 4.9717461259035645e-09\n",
            "Training: Batch 2/2, Loss: 4.465667391428951e-09\n",
            "Validation: Batch 1/1, Loss: 4.413850618334436e-09\n",
            "Epoch: 2345/10000\n",
            "Training: Batch 1/2, Loss: 4.670078102009256e-09\n",
            "Training: Batch 2/2, Loss: 4.6079771109930334e-09\n",
            "Validation: Batch 1/1, Loss: 4.340696690974255e-09\n",
            "Epoch: 2346/10000\n",
            "Training: Batch 1/2, Loss: 4.323405189410323e-09\n",
            "Training: Batch 2/2, Loss: 4.798723640675462e-09\n",
            "Validation: Batch 1/1, Loss: 4.269767650555423e-09\n",
            "Epoch: 2347/10000\n",
            "Training: Batch 1/2, Loss: 4.316104806889598e-09\n",
            "Training: Batch 2/2, Loss: 4.657482399750279e-09\n",
            "Validation: Batch 1/1, Loss: 4.199666392423751e-09\n",
            "Epoch: 2348/10000\n",
            "Training: Batch 1/2, Loss: 4.389015817451991e-09\n",
            "Training: Batch 2/2, Loss: 4.437764378195652e-09\n",
            "Validation: Batch 1/1, Loss: 4.129956376885957e-09\n",
            "Epoch: 2349/10000\n",
            "Training: Batch 1/2, Loss: 4.504358219747928e-09\n",
            "Training: Batch 2/2, Loss: 4.177214574241361e-09\n",
            "Validation: Batch 1/1, Loss: 4.060747738066084e-09\n",
            "Epoch: 2350/10000\n",
            "Training: Batch 1/2, Loss: 4.388665431065419e-09\n",
            "Training: Batch 2/2, Loss: 4.1470094025441995e-09\n",
            "Validation: Batch 1/1, Loss: 3.991840191730489e-09\n",
            "Epoch: 2351/10000\n",
            "Training: Batch 1/2, Loss: 4.151344601410756e-09\n",
            "Training: Batch 2/2, Loss: 4.238400741485293e-09\n",
            "Validation: Batch 1/1, Loss: 3.923822600171434e-09\n",
            "Epoch: 2352/10000\n",
            "Training: Batch 1/2, Loss: 4.140086495851847e-09\n",
            "Training: Batch 2/2, Loss: 4.107088891203148e-09\n",
            "Validation: Batch 1/1, Loss: 3.856463592910586e-09\n",
            "Epoch: 2353/10000\n",
            "Training: Batch 1/2, Loss: 3.9943386376251055e-09\n",
            "Training: Batch 2/2, Loss: 4.110622064956715e-09\n",
            "Validation: Batch 1/1, Loss: 3.790268099379546e-09\n",
            "Epoch: 2354/10000\n",
            "Training: Batch 1/2, Loss: 3.8761482912264e-09\n",
            "Training: Batch 2/2, Loss: 4.09174116811073e-09\n",
            "Validation: Batch 1/1, Loss: 3.728751085674276e-09\n",
            "Epoch: 2355/10000\n",
            "Training: Batch 1/2, Loss: 3.841690521255714e-09\n",
            "Training: Batch 2/2, Loss: 3.996642572445808e-09\n",
            "Validation: Batch 1/1, Loss: 3.667445680477499e-09\n",
            "Epoch: 2356/10000\n",
            "Training: Batch 1/2, Loss: 3.9573659904590386e-09\n",
            "Training: Batch 2/2, Loss: 3.752944177648487e-09\n",
            "Validation: Batch 1/1, Loss: 3.606946963330415e-09\n",
            "Epoch: 2357/10000\n",
            "Training: Batch 1/2, Loss: 3.832259842795338e-09\n",
            "Training: Batch 2/2, Loss: 3.750072252728387e-09\n",
            "Validation: Batch 1/1, Loss: 3.54716322981119e-09\n",
            "Epoch: 2358/10000\n",
            "Training: Batch 1/2, Loss: 3.788194202769546e-09\n",
            "Training: Batch 2/2, Loss: 3.6680187776028106e-09\n",
            "Validation: Batch 1/1, Loss: 3.4876408427919614e-09\n",
            "Epoch: 2359/10000\n",
            "Training: Batch 1/2, Loss: 3.690065586425817e-09\n",
            "Training: Batch 2/2, Loss: 3.6405956027607544e-09\n",
            "Validation: Batch 1/1, Loss: 3.4286851136045016e-09\n",
            "Epoch: 2360/10000\n",
            "Training: Batch 1/2, Loss: 3.643028101407708e-09\n",
            "Training: Batch 2/2, Loss: 3.5633866968254324e-09\n",
            "Validation: Batch 1/1, Loss: 3.3701947899089646e-09\n",
            "Epoch: 2361/10000\n",
            "Training: Batch 1/2, Loss: 3.597200093352626e-09\n",
            "Training: Batch 2/2, Loss: 3.48647533066071e-09\n",
            "Validation: Batch 1/1, Loss: 3.3131497545468847e-09\n",
            "Epoch: 2362/10000\n",
            "Training: Batch 1/2, Loss: 3.5355607330700423e-09\n",
            "Training: Batch 2/2, Loss: 3.428236583502553e-09\n",
            "Validation: Batch 1/1, Loss: 3.256646285976217e-09\n",
            "Epoch: 2363/10000\n",
            "Training: Batch 1/2, Loss: 3.395694836427765e-09\n",
            "Training: Batch 2/2, Loss: 3.4488820688238775e-09\n",
            "Validation: Batch 1/1, Loss: 3.2009681572020554e-09\n",
            "Epoch: 2364/10000\n",
            "Training: Batch 1/2, Loss: 3.318411545549793e-09\n",
            "Training: Batch 2/2, Loss: 3.409147852906358e-09\n",
            "Validation: Batch 1/1, Loss: 3.145761207079545e-09\n",
            "Epoch: 2365/10000\n",
            "Training: Batch 1/2, Loss: 3.2886737777459984e-09\n",
            "Training: Batch 2/2, Loss: 3.3229863305450635e-09\n",
            "Validation: Batch 1/1, Loss: 3.090795397397983e-09\n",
            "Epoch: 2366/10000\n",
            "Training: Batch 1/2, Loss: 3.30356675348753e-09\n",
            "Training: Batch 2/2, Loss: 3.19383097746595e-09\n",
            "Validation: Batch 1/1, Loss: 3.0382443227949807e-09\n",
            "Epoch: 2367/10000\n",
            "Training: Batch 1/2, Loss: 3.1851490334133814e-09\n",
            "Training: Batch 2/2, Loss: 3.202032639038066e-09\n",
            "Validation: Batch 1/1, Loss: 2.9875064644357963e-09\n",
            "Epoch: 2368/10000\n",
            "Training: Batch 1/2, Loss: 3.12258463530668e-09\n",
            "Training: Batch 2/2, Loss: 3.1572486847153414e-09\n",
            "Validation: Batch 1/1, Loss: 2.937292187255025e-09\n",
            "Epoch: 2369/10000\n",
            "Training: Batch 1/2, Loss: 3.152762495517436e-09\n",
            "Training: Batch 2/2, Loss: 3.021669359171142e-09\n",
            "Validation: Batch 1/1, Loss: 2.8876023794310868e-09\n",
            "Epoch: 2370/10000\n",
            "Training: Batch 1/2, Loss: 3.0052147437231724e-09\n",
            "Training: Batch 2/2, Loss: 3.063120201929337e-09\n",
            "Validation: Batch 1/1, Loss: 2.838745905009432e-09\n",
            "Epoch: 2371/10000\n",
            "Training: Batch 1/2, Loss: 2.9886186858618657e-09\n",
            "Training: Batch 2/2, Loss: 2.9773081777761945e-09\n",
            "Validation: Batch 1/1, Loss: 2.790534026075875e-09\n",
            "Epoch: 2372/10000\n",
            "Training: Batch 1/2, Loss: 3.050306895957533e-09\n",
            "Training: Batch 2/2, Loss: 2.815425004243366e-09\n",
            "Validation: Batch 1/1, Loss: 2.74312972337043e-09\n",
            "Epoch: 2373/10000\n",
            "Training: Batch 1/2, Loss: 2.8594100420775703e-09\n",
            "Training: Batch 2/2, Loss: 2.9050564176458238e-09\n",
            "Validation: Batch 1/1, Loss: 2.6961561871985396e-09\n",
            "Epoch: 2374/10000\n",
            "Training: Batch 1/2, Loss: 2.7710160832583597e-09\n",
            "Training: Batch 2/2, Loss: 2.8944033836353356e-09\n",
            "Validation: Batch 1/1, Loss: 2.649665820086966e-09\n",
            "Epoch: 2375/10000\n",
            "Training: Batch 1/2, Loss: 2.807395205195462e-09\n",
            "Training: Batch 2/2, Loss: 2.7609170505371594e-09\n",
            "Validation: Batch 1/1, Loss: 2.603595117278701e-09\n",
            "Epoch: 2376/10000\n",
            "Training: Batch 1/2, Loss: 2.828528522513807e-09\n",
            "Training: Batch 2/2, Loss: 2.643632424081943e-09\n",
            "Validation: Batch 1/1, Loss: 2.5578716922325384e-09\n",
            "Epoch: 2377/10000\n",
            "Training: Batch 1/2, Loss: 2.7820816761447986e-09\n",
            "Training: Batch 2/2, Loss: 2.5942374914933453e-09\n",
            "Validation: Batch 1/1, Loss: 2.5125550529025986e-09\n",
            "Epoch: 2378/10000\n",
            "Training: Batch 1/2, Loss: 2.6741910907901456e-09\n",
            "Training: Batch 2/2, Loss: 2.6063209368487605e-09\n",
            "Validation: Batch 1/1, Loss: 2.468250270837302e-09\n",
            "Epoch: 2379/10000\n",
            "Training: Batch 1/2, Loss: 2.658735676064339e-09\n",
            "Training: Batch 2/2, Loss: 2.5303390494002542e-09\n",
            "Validation: Batch 1/1, Loss: 2.4270083720523417e-09\n",
            "Epoch: 2380/10000\n",
            "Training: Batch 1/2, Loss: 2.5793767122195277e-09\n",
            "Training: Batch 2/2, Loss: 2.5226170041747764e-09\n",
            "Validation: Batch 1/1, Loss: 2.3862218867520824e-09\n",
            "Epoch: 2381/10000\n",
            "Training: Batch 1/2, Loss: 2.5802810998953873e-09\n",
            "Training: Batch 2/2, Loss: 2.436571167052648e-09\n",
            "Validation: Batch 1/1, Loss: 2.3462254361561463e-09\n",
            "Epoch: 2382/10000\n",
            "Training: Batch 1/2, Loss: 2.526842290961895e-09\n",
            "Training: Batch 2/2, Loss: 2.4054256364536286e-09\n",
            "Validation: Batch 1/1, Loss: 2.3066732968146653e-09\n",
            "Epoch: 2383/10000\n",
            "Training: Batch 1/2, Loss: 2.4877830906433474e-09\n",
            "Training: Batch 2/2, Loss: 2.360615258822918e-09\n",
            "Validation: Batch 1/1, Loss: 2.2673778410364775e-09\n",
            "Epoch: 2384/10000\n",
            "Training: Batch 1/2, Loss: 2.3635537971244958e-09\n",
            "Training: Batch 2/2, Loss: 2.4009296772931066e-09\n",
            "Validation: Batch 1/1, Loss: 2.2283586087468166e-09\n",
            "Epoch: 2385/10000\n",
            "Training: Batch 1/2, Loss: 2.4360606865059253e-09\n",
            "Training: Batch 2/2, Loss: 2.2469890392784464e-09\n",
            "Validation: Batch 1/1, Loss: 2.1896828794609746e-09\n",
            "Epoch: 2386/10000\n",
            "Training: Batch 1/2, Loss: 2.232409368474464e-09\n",
            "Training: Batch 2/2, Loss: 2.3676394178551163e-09\n",
            "Validation: Batch 1/1, Loss: 2.1514787729159934e-09\n",
            "Epoch: 2387/10000\n",
            "Training: Batch 1/2, Loss: 2.219093131472505e-09\n",
            "Training: Batch 2/2, Loss: 2.3004982363517e-09\n",
            "Validation: Batch 1/1, Loss: 2.1140624806292863e-09\n",
            "Epoch: 2388/10000\n",
            "Training: Batch 1/2, Loss: 2.247668495769517e-09\n",
            "Training: Batch 2/2, Loss: 2.1946000572370394e-09\n",
            "Validation: Batch 1/1, Loss: 2.077374938735943e-09\n",
            "Epoch: 2389/10000\n",
            "Training: Batch 1/2, Loss: 2.160706280562863e-09\n",
            "Training: Batch 2/2, Loss: 2.2042045966230717e-09\n",
            "Validation: Batch 1/1, Loss: 2.0410293455341844e-09\n",
            "Epoch: 2390/10000\n",
            "Training: Batch 1/2, Loss: 2.1714148257245824e-09\n",
            "Training: Batch 2/2, Loss: 2.117735764528561e-09\n",
            "Validation: Batch 1/1, Loss: 2.005133392657399e-09\n",
            "Epoch: 2391/10000\n",
            "Training: Batch 1/2, Loss: 2.113043962026495e-09\n",
            "Training: Batch 2/2, Loss: 2.1007624528834867e-09\n",
            "Validation: Batch 1/1, Loss: 1.969496343789956e-09\n",
            "Epoch: 2392/10000\n",
            "Training: Batch 1/2, Loss: 2.04838701556298e-09\n",
            "Training: Batch 2/2, Loss: 2.090140505117688e-09\n",
            "Validation: Batch 1/1, Loss: 1.9341488410873353e-09\n",
            "Epoch: 2393/10000\n",
            "Training: Batch 1/2, Loss: 2.079469485494201e-09\n",
            "Training: Batch 2/2, Loss: 1.985901887380237e-09\n",
            "Validation: Batch 1/1, Loss: 1.9004069429229276e-09\n",
            "Epoch: 2394/10000\n",
            "Training: Batch 1/2, Loss: 1.9814516694083295e-09\n",
            "Training: Batch 2/2, Loss: 2.0134536260485447e-09\n",
            "Validation: Batch 1/1, Loss: 1.8681047819768537e-09\n",
            "Epoch: 2395/10000\n",
            "Training: Batch 1/2, Loss: 1.9549493135428975e-09\n",
            "Training: Batch 2/2, Loss: 1.9719268440354654e-09\n",
            "Validation: Batch 1/1, Loss: 1.8363021103695587e-09\n",
            "Epoch: 2396/10000\n",
            "Training: Batch 1/2, Loss: 1.974353125433481e-09\n",
            "Training: Batch 2/2, Loss: 1.8850256910951657e-09\n",
            "Validation: Batch 1/1, Loss: 1.8044907790226716e-09\n",
            "Epoch: 2397/10000\n",
            "Training: Batch 1/2, Loss: 1.838556640265665e-09\n",
            "Training: Batch 2/2, Loss: 1.953277539712417e-09\n",
            "Validation: Batch 1/1, Loss: 1.7734838042571255e-09\n",
            "Epoch: 2398/10000\n",
            "Training: Batch 1/2, Loss: 1.9188417521576184e-09\n",
            "Training: Batch 2/2, Loss: 1.808712957185321e-09\n",
            "Validation: Batch 1/1, Loss: 1.7431690535474331e-09\n",
            "Epoch: 2399/10000\n",
            "Training: Batch 1/2, Loss: 1.8399813894731665e-09\n",
            "Training: Batch 2/2, Loss: 1.8229766585164953e-09\n",
            "Validation: Batch 1/1, Loss: 1.7130693530376107e-09\n",
            "Epoch: 2400/10000\n",
            "Training: Batch 1/2, Loss: 1.7124827111913987e-09\n",
            "Training: Batch 2/2, Loss: 1.886256484340265e-09\n",
            "Validation: Batch 1/1, Loss: 1.6834322824621495e-09\n",
            "Epoch: 2401/10000\n",
            "Training: Batch 1/2, Loss: 1.76942727136975e-09\n",
            "Training: Batch 2/2, Loss: 1.7671500929239414e-09\n",
            "Validation: Batch 1/1, Loss: 1.6537733404931032e-09\n",
            "Epoch: 2402/10000\n",
            "Training: Batch 1/2, Loss: 1.7828457599122771e-09\n",
            "Training: Batch 2/2, Loss: 1.6918360046247471e-09\n",
            "Validation: Batch 1/1, Loss: 1.6247460044027662e-09\n",
            "Epoch: 2403/10000\n",
            "Training: Batch 1/2, Loss: 1.7185971534772193e-09\n",
            "Training: Batch 2/2, Loss: 1.6945516101429803e-09\n",
            "Validation: Batch 1/1, Loss: 1.5957555277168467e-09\n",
            "Epoch: 2404/10000\n",
            "Training: Batch 1/2, Loss: 1.7364319981894027e-09\n",
            "Training: Batch 2/2, Loss: 1.6166932237382525e-09\n",
            "Validation: Batch 1/1, Loss: 1.5673167208518635e-09\n",
            "Epoch: 2405/10000\n",
            "Training: Batch 1/2, Loss: 1.6910425282290475e-09\n",
            "Training: Batch 2/2, Loss: 1.6021659554610324e-09\n",
            "Validation: Batch 1/1, Loss: 1.5389209906402357e-09\n",
            "Epoch: 2406/10000\n",
            "Training: Batch 1/2, Loss: 1.6499164257055554e-09\n",
            "Training: Batch 2/2, Loss: 1.583749020817038e-09\n",
            "Validation: Batch 1/1, Loss: 1.510868541387822e-09\n",
            "Epoch: 2407/10000\n",
            "Training: Batch 1/2, Loss: 1.57989976656836e-09\n",
            "Training: Batch 2/2, Loss: 1.5946017839496562e-09\n",
            "Validation: Batch 1/1, Loss: 1.4830399130971728e-09\n",
            "Epoch: 2408/10000\n",
            "Training: Batch 1/2, Loss: 1.5201666592190577e-09\n",
            "Training: Batch 2/2, Loss: 1.59572366431604e-09\n",
            "Validation: Batch 1/1, Loss: 1.4559127237134817e-09\n",
            "Epoch: 2409/10000\n",
            "Training: Batch 1/2, Loss: 1.515985004196807e-09\n",
            "Training: Batch 2/2, Loss: 1.5436141254099311e-09\n",
            "Validation: Batch 1/1, Loss: 1.430530249812989e-09\n",
            "Epoch: 2410/10000\n",
            "Training: Batch 1/2, Loss: 1.506541558171648e-09\n",
            "Training: Batch 2/2, Loss: 1.5006055287258846e-09\n",
            "Validation: Batch 1/1, Loss: 1.406187277730453e-09\n",
            "Epoch: 2411/10000\n",
            "Training: Batch 1/2, Loss: 1.5003470688057519e-09\n",
            "Training: Batch 2/2, Loss: 1.4558749761306444e-09\n",
            "Validation: Batch 1/1, Loss: 1.3823228117715303e-09\n",
            "Epoch: 2412/10000\n",
            "Training: Batch 1/2, Loss: 1.4772454370870491e-09\n",
            "Training: Batch 2/2, Loss: 1.4283898508438142e-09\n",
            "Validation: Batch 1/1, Loss: 1.3585051972242468e-09\n",
            "Epoch: 2413/10000\n",
            "Training: Batch 1/2, Loss: 1.4397324443748971e-09\n",
            "Training: Batch 2/2, Loss: 1.415571659890702e-09\n",
            "Validation: Batch 1/1, Loss: 1.3348102623211844e-09\n",
            "Epoch: 2414/10000\n",
            "Training: Batch 1/2, Loss: 1.4106216195131083e-09\n",
            "Training: Batch 2/2, Loss: 1.394423909673037e-09\n",
            "Validation: Batch 1/1, Loss: 1.3115932784302231e-09\n",
            "Epoch: 2415/10000\n",
            "Training: Batch 1/2, Loss: 1.3316052704936965e-09\n",
            "Training: Batch 2/2, Loss: 1.4237686585261145e-09\n",
            "Validation: Batch 1/1, Loss: 1.2882571676087196e-09\n",
            "Epoch: 2416/10000\n",
            "Training: Batch 1/2, Loss: 1.3135731391500372e-09\n",
            "Training: Batch 2/2, Loss: 1.3929187803185528e-09\n",
            "Validation: Batch 1/1, Loss: 1.265492599600293e-09\n",
            "Epoch: 2417/10000\n",
            "Training: Batch 1/2, Loss: 1.3507331919626608e-09\n",
            "Training: Batch 2/2, Loss: 1.3079111127467513e-09\n",
            "Validation: Batch 1/1, Loss: 1.242736802353761e-09\n",
            "Epoch: 2418/10000\n",
            "Training: Batch 1/2, Loss: 1.2780249081245643e-09\n",
            "Training: Batch 2/2, Loss: 1.3320035074926295e-09\n",
            "Validation: Batch 1/1, Loss: 1.2200437327081204e-09\n",
            "Epoch: 2419/10000\n",
            "Training: Batch 1/2, Loss: 1.2759311385224237e-09\n",
            "Training: Batch 2/2, Loss: 1.2868643928243273e-09\n",
            "Validation: Batch 1/1, Loss: 1.1982448366865128e-09\n",
            "Epoch: 2420/10000\n",
            "Training: Batch 1/2, Loss: 1.2942560356776767e-09\n",
            "Training: Batch 2/2, Loss: 1.2227708845458096e-09\n",
            "Validation: Batch 1/1, Loss: 1.176714503614562e-09\n",
            "Epoch: 2421/10000\n",
            "Training: Batch 1/2, Loss: 1.1728534810018232e-09\n",
            "Training: Batch 2/2, Loss: 1.2981220542940264e-09\n",
            "Validation: Batch 1/1, Loss: 1.1555491008508056e-09\n",
            "Epoch: 2422/10000\n",
            "Training: Batch 1/2, Loss: 1.2470016130805561e-09\n",
            "Training: Batch 2/2, Loss: 1.180405106993021e-09\n",
            "Validation: Batch 1/1, Loss: 1.134415339443251e-09\n",
            "Epoch: 2423/10000\n",
            "Training: Batch 1/2, Loss: 1.2214074196492675e-09\n",
            "Training: Batch 2/2, Loss: 1.1621468232192456e-09\n",
            "Validation: Batch 1/1, Loss: 1.1137504252189956e-09\n",
            "Epoch: 2424/10000\n",
            "Training: Batch 1/2, Loss: 1.1461168680781952e-09\n",
            "Training: Batch 2/2, Loss: 1.1935626931247612e-09\n",
            "Validation: Batch 1/1, Loss: 1.0932568184074398e-09\n",
            "Epoch: 2425/10000\n",
            "Training: Batch 1/2, Loss: 1.2162042484220592e-09\n",
            "Training: Batch 2/2, Loss: 1.081129075153342e-09\n",
            "Validation: Batch 1/1, Loss: 1.0728948840466046e-09\n",
            "Epoch: 2426/10000\n",
            "Training: Batch 1/2, Loss: 1.1752046003010719e-09\n",
            "Training: Batch 2/2, Loss: 1.079313638463475e-09\n",
            "Validation: Batch 1/1, Loss: 1.0526920446451982e-09\n",
            "Epoch: 2427/10000\n",
            "Training: Batch 1/2, Loss: 1.1347949246953704e-09\n",
            "Training: Batch 2/2, Loss: 1.0772805980607814e-09\n",
            "Validation: Batch 1/1, Loss: 1.0336606015570737e-09\n",
            "Epoch: 2428/10000\n",
            "Training: Batch 1/2, Loss: 1.0974802178154164e-09\n",
            "Training: Batch 2/2, Loss: 1.0753252732698115e-09\n",
            "Validation: Batch 1/1, Loss: 1.0156924190596328e-09\n",
            "Epoch: 2429/10000\n",
            "Training: Batch 1/2, Loss: 1.060583065815024e-09\n",
            "Training: Batch 2/2, Loss: 1.0740826006383486e-09\n",
            "Validation: Batch 1/1, Loss: 9.978972093094285e-10\n",
            "Epoch: 2430/10000\n",
            "Training: Batch 1/2, Loss: 1.0814453776930577e-09\n",
            "Training: Batch 2/2, Loss: 1.0157370500252227e-09\n",
            "Validation: Batch 1/1, Loss: 9.801612854687392e-10\n",
            "Epoch: 2431/10000\n",
            "Training: Batch 1/2, Loss: 1.0434736408271306e-09\n",
            "Training: Batch 2/2, Loss: 1.0161657071350305e-09\n",
            "Validation: Batch 1/1, Loss: 9.627481034613083e-10\n",
            "Epoch: 2432/10000\n",
            "Training: Batch 1/2, Loss: 1.0166743003026113e-09\n",
            "Training: Batch 2/2, Loss: 1.0058520683031702e-09\n",
            "Validation: Batch 1/1, Loss: 9.457411520585879e-10\n",
            "Epoch: 2433/10000\n",
            "Training: Batch 1/2, Loss: 1.0149333595776966e-09\n",
            "Training: Batch 2/2, Loss: 9.722318505822614e-10\n",
            "Validation: Batch 1/1, Loss: 9.290765379255106e-10\n",
            "Epoch: 2434/10000\n",
            "Training: Batch 1/2, Loss: 9.82313452801975e-10\n",
            "Training: Batch 2/2, Loss: 9.696384806190395e-10\n",
            "Validation: Batch 1/1, Loss: 9.127510969264563e-10\n",
            "Epoch: 2435/10000\n",
            "Training: Batch 1/2, Loss: 9.535423561857215e-10\n",
            "Training: Batch 2/2, Loss: 9.635990894096835e-10\n",
            "Validation: Batch 1/1, Loss: 8.963769726477722e-10\n",
            "Epoch: 2436/10000\n",
            "Training: Batch 1/2, Loss: 9.125972200152432e-10\n",
            "Training: Batch 2/2, Loss: 9.699332448320774e-10\n",
            "Validation: Batch 1/1, Loss: 8.802161222121185e-10\n",
            "Epoch: 2437/10000\n",
            "Training: Batch 1/2, Loss: 9.603907669131218e-10\n",
            "Training: Batch 2/2, Loss: 8.887932057000114e-10\n",
            "Validation: Batch 1/1, Loss: 8.642956905724475e-10\n",
            "Epoch: 2438/10000\n",
            "Training: Batch 1/2, Loss: 9.28306598257933e-10\n",
            "Training: Batch 2/2, Loss: 8.869808221234621e-10\n",
            "Validation: Batch 1/1, Loss: 8.483828639604951e-10\n",
            "Epoch: 2439/10000\n",
            "Training: Batch 1/2, Loss: 9.008488399686598e-10\n",
            "Training: Batch 2/2, Loss: 8.809019069744295e-10\n",
            "Validation: Batch 1/1, Loss: 8.327333822499838e-10\n",
            "Epoch: 2440/10000\n",
            "Training: Batch 1/2, Loss: 8.979181842505568e-10\n",
            "Training: Batch 2/2, Loss: 8.507791693368461e-10\n",
            "Validation: Batch 1/1, Loss: 8.17097445260373e-10\n",
            "Epoch: 2441/10000\n",
            "Training: Batch 1/2, Loss: 8.401734863383581e-10\n",
            "Training: Batch 2/2, Loss: 8.754199587457379e-10\n",
            "Validation: Batch 1/1, Loss: 8.018780084384503e-10\n",
            "Epoch: 2442/10000\n",
            "Training: Batch 1/2, Loss: 8.620775759915489e-10\n",
            "Training: Batch 2/2, Loss: 8.218510871849105e-10\n",
            "Validation: Batch 1/1, Loss: 7.866420848046118e-10\n",
            "Epoch: 2443/10000\n",
            "Training: Batch 1/2, Loss: 7.80713382830811e-10\n",
            "Training: Batch 2/2, Loss: 8.707208842828607e-10\n",
            "Validation: Batch 1/1, Loss: 7.716707273175416e-10\n",
            "Epoch: 2444/10000\n",
            "Training: Batch 1/2, Loss: 8.153083208561895e-10\n",
            "Training: Batch 2/2, Loss: 8.05235378376068e-10\n",
            "Validation: Batch 1/1, Loss: 7.568645710165356e-10\n",
            "Epoch: 2445/10000\n",
            "Training: Batch 1/2, Loss: 7.64457386281947e-10\n",
            "Training: Batch 2/2, Loss: 8.246594518368511e-10\n",
            "Validation: Batch 1/1, Loss: 7.419583836210109e-10\n",
            "Epoch: 2446/10000\n",
            "Training: Batch 1/2, Loss: 7.654178957317015e-10\n",
            "Training: Batch 2/2, Loss: 7.929860101896224e-10\n",
            "Validation: Batch 1/1, Loss: 7.276805269462727e-10\n",
            "Epoch: 2447/10000\n",
            "Training: Batch 1/2, Loss: 7.433074711293841e-10\n",
            "Training: Batch 2/2, Loss: 7.851746475218135e-10\n",
            "Validation: Batch 1/1, Loss: 7.137448410077241e-10\n",
            "Epoch: 2448/10000\n",
            "Training: Batch 1/2, Loss: 7.110039779156807e-10\n",
            "Training: Batch 2/2, Loss: 7.890402775601046e-10\n",
            "Validation: Batch 1/1, Loss: 7.015115710551356e-10\n",
            "Epoch: 2449/10000\n",
            "Training: Batch 1/2, Loss: 7.283947334180141e-10\n",
            "Training: Batch 2/2, Loss: 7.460619899646304e-10\n",
            "Validation: Batch 1/1, Loss: 6.89340418080775e-10\n",
            "Epoch: 2450/10000\n",
            "Training: Batch 1/2, Loss: 7.176662597530026e-10\n",
            "Training: Batch 2/2, Loss: 7.311000138621182e-10\n",
            "Validation: Batch 1/1, Loss: 6.773187011255288e-10\n",
            "Epoch: 2451/10000\n",
            "Training: Batch 1/2, Loss: 7.11174952261473e-10\n",
            "Training: Batch 2/2, Loss: 7.121555567479732e-10\n",
            "Validation: Batch 1/1, Loss: 6.653140816048619e-10\n",
            "Epoch: 2452/10000\n",
            "Training: Batch 1/2, Loss: 6.79049427798617e-10\n",
            "Training: Batch 2/2, Loss: 7.189295270215723e-10\n",
            "Validation: Batch 1/1, Loss: 6.535310625999102e-10\n",
            "Epoch: 2453/10000\n",
            "Training: Batch 1/2, Loss: 6.713528066804031e-10\n",
            "Training: Batch 2/2, Loss: 7.015021896705775e-10\n",
            "Validation: Batch 1/1, Loss: 6.418551246056836e-10\n",
            "Epoch: 2454/10000\n",
            "Training: Batch 1/2, Loss: 6.635409444122331e-10\n",
            "Training: Batch 2/2, Loss: 6.846527789150514e-10\n",
            "Validation: Batch 1/1, Loss: 6.301990040924466e-10\n",
            "Epoch: 2455/10000\n",
            "Training: Batch 1/2, Loss: 6.793117179881847e-10\n",
            "Training: Batch 2/2, Loss: 6.444822453488541e-10\n",
            "Validation: Batch 1/1, Loss: 6.18747997283009e-10\n",
            "Epoch: 2456/10000\n",
            "Training: Batch 1/2, Loss: 6.339642144581603e-10\n",
            "Training: Batch 2/2, Loss: 6.652242645621698e-10\n",
            "Validation: Batch 1/1, Loss: 6.073024860775433e-10\n",
            "Epoch: 2457/10000\n",
            "Training: Batch 1/2, Loss: 6.278280118010571e-10\n",
            "Training: Batch 2/2, Loss: 6.473170333087808e-10\n",
            "Validation: Batch 1/1, Loss: 5.959758242468638e-10\n",
            "Epoch: 2458/10000\n",
            "Training: Batch 1/2, Loss: 6.019749698715771e-10\n",
            "Training: Batch 2/2, Loss: 6.491421844501133e-10\n",
            "Validation: Batch 1/1, Loss: 5.8486537835023e-10\n",
            "Epoch: 2459/10000\n",
            "Training: Batch 1/2, Loss: 6.125573936976991e-10\n",
            "Training: Batch 2/2, Loss: 6.151552045530195e-10\n",
            "Validation: Batch 1/1, Loss: 5.737739727784685e-10\n",
            "Epoch: 2460/10000\n",
            "Training: Batch 1/2, Loss: 5.740793396213917e-10\n",
            "Training: Batch 2/2, Loss: 6.299260002506912e-10\n",
            "Validation: Batch 1/1, Loss: 5.62854651775524e-10\n",
            "Epoch: 2461/10000\n",
            "Training: Batch 1/2, Loss: 6.079234893263674e-10\n",
            "Training: Batch 2/2, Loss: 5.736232600028757e-10\n",
            "Validation: Batch 1/1, Loss: 5.523464463585981e-10\n",
            "Epoch: 2462/10000\n",
            "Training: Batch 1/2, Loss: 6.057812029780507e-10\n",
            "Training: Batch 2/2, Loss: 5.538287051187751e-10\n",
            "Validation: Batch 1/1, Loss: 5.420135451572605e-10\n",
            "Epoch: 2463/10000\n",
            "Training: Batch 1/2, Loss: 5.714752560059821e-10\n",
            "Training: Batch 2/2, Loss: 5.661551782942809e-10\n",
            "Validation: Batch 1/1, Loss: 5.31701294104181e-10\n",
            "Epoch: 2464/10000\n",
            "Training: Batch 1/2, Loss: 5.656236035100903e-10\n",
            "Training: Batch 2/2, Loss: 5.50340661931159e-10\n",
            "Validation: Batch 1/1, Loss: 5.21465759462103e-10\n",
            "Epoch: 2465/10000\n",
            "Training: Batch 1/2, Loss: 5.438752226361032e-10\n",
            "Training: Batch 2/2, Loss: 5.50713918912038e-10\n",
            "Validation: Batch 1/1, Loss: 5.115566859004161e-10\n",
            "Epoch: 2466/10000\n",
            "Training: Batch 1/2, Loss: 4.999117231285766e-10\n",
            "Training: Batch 2/2, Loss: 5.735139030349501e-10\n",
            "Validation: Batch 1/1, Loss: 5.017610216206947e-10\n",
            "Epoch: 2467/10000\n",
            "Training: Batch 1/2, Loss: 5.218630527714652e-10\n",
            "Training: Batch 2/2, Loss: 5.313667283957102e-10\n",
            "Validation: Batch 1/1, Loss: 4.920337470792902e-10\n",
            "Epoch: 2468/10000\n",
            "Training: Batch 1/2, Loss: 5.145628922953449e-10\n",
            "Training: Batch 2/2, Loss: 5.184725981877136e-10\n",
            "Validation: Batch 1/1, Loss: 4.824389221447234e-10\n",
            "Epoch: 2469/10000\n",
            "Training: Batch 1/2, Loss: 5.10676667619947e-10\n",
            "Training: Batch 2/2, Loss: 5.022617322048006e-10\n",
            "Validation: Batch 1/1, Loss: 4.727672697768526e-10\n",
            "Epoch: 2470/10000\n",
            "Training: Batch 1/2, Loss: 5.047187667805986e-10\n",
            "Training: Batch 2/2, Loss: 4.883814463951808e-10\n",
            "Validation: Batch 1/1, Loss: 4.6329684533219506e-10\n",
            "Epoch: 2471/10000\n",
            "Training: Batch 1/2, Loss: 4.90571860911615e-10\n",
            "Training: Batch 2/2, Loss: 4.833745625987262e-10\n",
            "Validation: Batch 1/1, Loss: 4.545940290867634e-10\n",
            "Epoch: 2472/10000\n",
            "Training: Batch 1/2, Loss: 4.785412621721719e-10\n",
            "Training: Batch 2/2, Loss: 4.769503680890352e-10\n",
            "Validation: Batch 1/1, Loss: 4.4658937659036724e-10\n",
            "Epoch: 2473/10000\n",
            "Training: Batch 1/2, Loss: 4.789628693657733e-10\n",
            "Training: Batch 2/2, Loss: 4.594596925144856e-10\n",
            "Validation: Batch 1/1, Loss: 4.3859194054363115e-10\n",
            "Epoch: 2474/10000\n",
            "Training: Batch 1/2, Loss: 4.744062365169555e-10\n",
            "Training: Batch 2/2, Loss: 4.4722853198564394e-10\n",
            "Validation: Batch 1/1, Loss: 4.3069800503836575e-10\n",
            "Epoch: 2475/10000\n",
            "Training: Batch 1/2, Loss: 4.50449039179901e-10\n",
            "Training: Batch 2/2, Loss: 4.5426756800637236e-10\n",
            "Validation: Batch 1/1, Loss: 4.2287245927141726e-10\n",
            "Epoch: 2476/10000\n",
            "Training: Batch 1/2, Loss: 4.549929599750868e-10\n",
            "Training: Batch 2/2, Loss: 4.331697778248156e-10\n",
            "Validation: Batch 1/1, Loss: 4.151181620670741e-10\n",
            "Epoch: 2477/10000\n",
            "Training: Batch 1/2, Loss: 4.550617938026136e-10\n",
            "Training: Batch 2/2, Loss: 4.1676487261277373e-10\n",
            "Validation: Batch 1/1, Loss: 4.0740996687382847e-10\n",
            "Epoch: 2478/10000\n",
            "Training: Batch 1/2, Loss: 4.3585005049529e-10\n",
            "Training: Batch 2/2, Loss: 4.195207237156495e-10\n",
            "Validation: Batch 1/1, Loss: 4.0003916845776644e-10\n",
            "Epoch: 2479/10000\n",
            "Training: Batch 1/2, Loss: 4.285675425652613e-10\n",
            "Training: Batch 2/2, Loss: 4.1137251938216934e-10\n",
            "Validation: Batch 1/1, Loss: 3.927793090774401e-10\n",
            "Epoch: 2480/10000\n",
            "Training: Batch 1/2, Loss: 4.190106595025611e-10\n",
            "Training: Batch 2/2, Loss: 4.056758540205152e-10\n",
            "Validation: Batch 1/1, Loss: 3.8558184423109765e-10\n",
            "Epoch: 2481/10000\n",
            "Training: Batch 1/2, Loss: 4.1875714007488796e-10\n",
            "Training: Batch 2/2, Loss: 3.9082861946759806e-10\n",
            "Validation: Batch 1/1, Loss: 3.7855743539871867e-10\n",
            "Epoch: 2482/10000\n",
            "Training: Batch 1/2, Loss: 4.16139511738578e-10\n",
            "Training: Batch 2/2, Loss: 3.78675979462173e-10\n",
            "Validation: Batch 1/1, Loss: 3.715477925325672e-10\n",
            "Epoch: 2483/10000\n",
            "Training: Batch 1/2, Loss: 3.806336912326458e-10\n",
            "Training: Batch 2/2, Loss: 3.991374453171659e-10\n",
            "Validation: Batch 1/1, Loss: 3.646241919508242e-10\n",
            "Epoch: 2484/10000\n",
            "Training: Batch 1/2, Loss: 3.855208374758945e-10\n",
            "Training: Batch 2/2, Loss: 3.7978761802115457e-10\n",
            "Validation: Batch 1/1, Loss: 3.5777802942504877e-10\n",
            "Epoch: 2485/10000\n",
            "Training: Batch 1/2, Loss: 3.714815122179971e-10\n",
            "Training: Batch 2/2, Loss: 3.7932371133031495e-10\n",
            "Validation: Batch 1/1, Loss: 3.5095512607163926e-10\n",
            "Epoch: 2486/10000\n",
            "Training: Batch 1/2, Loss: 3.7774461336681497e-10\n",
            "Training: Batch 2/2, Loss: 3.5893052419133653e-10\n",
            "Validation: Batch 1/1, Loss: 3.4433980666825903e-10\n",
            "Epoch: 2487/10000\n",
            "Training: Batch 1/2, Loss: 3.7422862031455395e-10\n",
            "Training: Batch 2/2, Loss: 3.4825767269985874e-10\n",
            "Validation: Batch 1/1, Loss: 3.3762082018995443e-10\n",
            "Epoch: 2488/10000\n",
            "Training: Batch 1/2, Loss: 3.466966991272358e-10\n",
            "Training: Batch 2/2, Loss: 3.616169030884464e-10\n",
            "Validation: Batch 1/1, Loss: 3.3109742725301317e-10\n",
            "Epoch: 2489/10000\n",
            "Training: Batch 1/2, Loss: 3.6576353057427013e-10\n",
            "Training: Batch 2/2, Loss: 3.288795347167195e-10\n",
            "Validation: Batch 1/1, Loss: 3.2458583043570854e-10\n",
            "Epoch: 2490/10000\n",
            "Training: Batch 1/2, Loss: 3.404201642798199e-10\n",
            "Training: Batch 2/2, Loss: 3.402464421320417e-10\n",
            "Validation: Batch 1/1, Loss: 3.1807903533298543e-10\n",
            "Epoch: 2491/10000\n",
            "Training: Batch 1/2, Loss: 3.2089281232217104e-10\n",
            "Training: Batch 2/2, Loss: 3.4605501797457805e-10\n",
            "Validation: Batch 1/1, Loss: 3.11626058291381e-10\n",
            "Epoch: 2492/10000\n",
            "Training: Batch 1/2, Loss: 3.32335631236802e-10\n",
            "Training: Batch 2/2, Loss: 3.213709576233015e-10\n",
            "Validation: Batch 1/1, Loss: 3.053204633562956e-10\n",
            "Epoch: 2493/10000\n",
            "Training: Batch 1/2, Loss: 3.1401523048479874e-10\n",
            "Training: Batch 2/2, Loss: 3.26138754891403e-10\n",
            "Validation: Batch 1/1, Loss: 2.9908467369388347e-10\n",
            "Epoch: 2494/10000\n",
            "Training: Batch 1/2, Loss: 3.163464212807554e-10\n",
            "Training: Batch 2/2, Loss: 3.10774628253796e-10\n",
            "Validation: Batch 1/1, Loss: 2.928897957499288e-10\n",
            "Epoch: 2495/10000\n",
            "Training: Batch 1/2, Loss: 3.137406168196577e-10\n",
            "Training: Batch 2/2, Loss: 3.006579707420798e-10\n",
            "Validation: Batch 1/1, Loss: 2.8673047269833773e-10\n",
            "Epoch: 2496/10000\n",
            "Training: Batch 1/2, Loss: 2.9917701649395667e-10\n",
            "Training: Batch 2/2, Loss: 3.0240146497995113e-10\n",
            "Validation: Batch 1/1, Loss: 2.810535415509463e-10\n",
            "Epoch: 2497/10000\n",
            "Training: Batch 1/2, Loss: 2.986730307519281e-10\n",
            "Training: Batch 2/2, Loss: 2.9110189259107244e-10\n",
            "Validation: Batch 1/1, Loss: 2.753564321000823e-10\n",
            "Epoch: 2498/10000\n",
            "Training: Batch 1/2, Loss: 2.8301938570507446e-10\n",
            "Training: Batch 2/2, Loss: 2.949972766064235e-10\n",
            "Validation: Batch 1/1, Loss: 2.6982296952304807e-10\n",
            "Epoch: 2499/10000\n",
            "Training: Batch 1/2, Loss: 2.815236099795726e-10\n",
            "Training: Batch 2/2, Loss: 2.850297775580657e-10\n",
            "Validation: Batch 1/1, Loss: 2.643361363130481e-10\n",
            "Epoch: 2500/10000\n",
            "Training: Batch 1/2, Loss: 2.807405696803045e-10\n",
            "Training: Batch 2/2, Loss: 2.744615368310832e-10\n",
            "Validation: Batch 1/1, Loss: 2.5884810961329663e-10\n",
            "Epoch: 2501/10000\n",
            "Training: Batch 1/2, Loss: 2.7579208361494523e-10\n",
            "Training: Batch 2/2, Loss: 2.686438571597449e-10\n",
            "Validation: Batch 1/1, Loss: 2.544102706281137e-10\n",
            "Epoch: 2502/10000\n",
            "Training: Batch 1/2, Loss: 2.6018687204754087e-10\n",
            "Training: Batch 2/2, Loss: 2.7454527540271556e-10\n",
            "Validation: Batch 1/1, Loss: 2.499883633433342e-10\n",
            "Epoch: 2503/10000\n",
            "Training: Batch 1/2, Loss: 2.617969729890035e-10\n",
            "Training: Batch 2/2, Loss: 2.6368943140120393e-10\n",
            "Validation: Batch 1/1, Loss: 2.4564297818052694e-10\n",
            "Epoch: 2504/10000\n",
            "Training: Batch 1/2, Loss: 2.558247225170618e-10\n",
            "Training: Batch 2/2, Loss: 2.6027080490820254e-10\n",
            "Validation: Batch 1/1, Loss: 2.412240407423383e-10\n",
            "Epoch: 2505/10000\n",
            "Training: Batch 1/2, Loss: 2.505701479638134e-10\n",
            "Training: Batch 2/2, Loss: 2.563317613724081e-10\n",
            "Validation: Batch 1/1, Loss: 2.3694743389590656e-10\n",
            "Epoch: 2506/10000\n",
            "Training: Batch 1/2, Loss: 2.607106475149834e-10\n",
            "Training: Batch 2/2, Loss: 2.3718010888629237e-10\n",
            "Validation: Batch 1/1, Loss: 2.3267690552053466e-10\n",
            "Epoch: 2507/10000\n",
            "Training: Batch 1/2, Loss: 2.4214905081088034e-10\n",
            "Training: Batch 2/2, Loss: 2.465319337563443e-10\n",
            "Validation: Batch 1/1, Loss: 2.283888633769493e-10\n",
            "Epoch: 2508/10000\n",
            "Training: Batch 1/2, Loss: 2.4297980294463173e-10\n",
            "Training: Batch 2/2, Loss: 2.3682775185385196e-10\n",
            "Validation: Batch 1/1, Loss: 2.242003666053094e-10\n",
            "Epoch: 2509/10000\n",
            "Training: Batch 1/2, Loss: 2.501784057695744e-10\n",
            "Training: Batch 2/2, Loss: 2.2080687278602795e-10\n",
            "Validation: Batch 1/1, Loss: 2.2003562860639647e-10\n",
            "Epoch: 2510/10000\n",
            "Training: Batch 1/2, Loss: 2.318971820125526e-10\n",
            "Training: Batch 2/2, Loss: 2.3015379879698372e-10\n",
            "Validation: Batch 1/1, Loss: 2.15896564514928e-10\n",
            "Epoch: 2511/10000\n",
            "Training: Batch 1/2, Loss: 2.2174968805632744e-10\n",
            "Training: Batch 2/2, Loss: 2.3147261885014814e-10\n",
            "Validation: Batch 1/1, Loss: 2.1181700837757944e-10\n",
            "Epoch: 2512/10000\n",
            "Training: Batch 1/2, Loss: 2.290906769841783e-10\n",
            "Training: Batch 2/2, Loss: 2.1568459518395144e-10\n",
            "Validation: Batch 1/1, Loss: 2.0786544707718235e-10\n",
            "Epoch: 2513/10000\n",
            "Training: Batch 1/2, Loss: 2.1195246946437152e-10\n",
            "Training: Batch 2/2, Loss: 2.2417789846684855e-10\n",
            "Validation: Batch 1/1, Loss: 2.0379119225477638e-10\n",
            "Epoch: 2514/10000\n",
            "Training: Batch 1/2, Loss: 2.1210259937287645e-10\n",
            "Training: Batch 2/2, Loss: 2.1557362839264016e-10\n",
            "Validation: Batch 1/1, Loss: 1.9985554877699485e-10\n",
            "Epoch: 2515/10000\n",
            "Training: Batch 1/2, Loss: 2.1081661416566533e-10\n",
            "Training: Batch 2/2, Loss: 2.0843729520159116e-10\n",
            "Validation: Batch 1/1, Loss: 1.959550438579427e-10\n",
            "Epoch: 2516/10000\n",
            "Training: Batch 1/2, Loss: 2.121113840125588e-10\n",
            "Training: Batch 2/2, Loss: 1.989396009038913e-10\n",
            "Validation: Batch 1/1, Loss: 1.9203301449000065e-10\n",
            "Epoch: 2517/10000\n",
            "Training: Batch 1/2, Loss: 2.0209289797179508e-10\n",
            "Training: Batch 2/2, Loss: 2.006837335200018e-10\n",
            "Validation: Batch 1/1, Loss: 1.8825142833467368e-10\n",
            "Epoch: 2518/10000\n",
            "Training: Batch 1/2, Loss: 1.943748079158425e-10\n",
            "Training: Batch 2/2, Loss: 2.0021920232871082e-10\n",
            "Validation: Batch 1/1, Loss: 1.8457280148709287e-10\n",
            "Epoch: 2519/10000\n",
            "Training: Batch 1/2, Loss: 2.0792896571197872e-10\n",
            "Training: Batch 2/2, Loss: 1.7913638628019868e-10\n",
            "Validation: Batch 1/1, Loss: 1.8089393871711934e-10\n",
            "Epoch: 2520/10000\n",
            "Training: Batch 1/2, Loss: 1.8525293798976605e-10\n",
            "Training: Batch 2/2, Loss: 1.9412890739367583e-10\n",
            "Validation: Batch 1/1, Loss: 1.7741624835920788e-10\n",
            "Epoch: 2521/10000\n",
            "Training: Batch 1/2, Loss: 1.837186236475219e-10\n",
            "Training: Batch 2/2, Loss: 1.8829937609154968e-10\n",
            "Validation: Batch 1/1, Loss: 1.739733357375428e-10\n",
            "Epoch: 2522/10000\n",
            "Training: Batch 1/2, Loss: 1.8508027055386123e-10\n",
            "Training: Batch 2/2, Loss: 1.7971925336812689e-10\n",
            "Validation: Batch 1/1, Loss: 1.7056986378882755e-10\n",
            "Epoch: 2523/10000\n",
            "Training: Batch 1/2, Loss: 1.8976122062586143e-10\n",
            "Training: Batch 2/2, Loss: 1.6796365687188342e-10\n",
            "Validation: Batch 1/1, Loss: 1.6717045803193997e-10\n",
            "Epoch: 2524/10000\n",
            "Training: Batch 1/2, Loss: 1.7036987098872913e-10\n",
            "Training: Batch 2/2, Loss: 1.8010901103870935e-10\n",
            "Validation: Batch 1/1, Loss: 1.6381464240655674e-10\n",
            "Epoch: 2525/10000\n",
            "Training: Batch 1/2, Loss: 1.7592545475952903e-10\n",
            "Training: Batch 2/2, Loss: 1.6762269350323322e-10\n",
            "Validation: Batch 1/1, Loss: 1.6049221673863912e-10\n",
            "Epoch: 2526/10000\n",
            "Training: Batch 1/2, Loss: 1.679883177008179e-10\n",
            "Training: Batch 2/2, Loss: 1.6848097916799532e-10\n",
            "Validation: Batch 1/1, Loss: 1.57236765274682e-10\n",
            "Epoch: 2527/10000\n",
            "Training: Batch 1/2, Loss: 1.6359623378203736e-10\n",
            "Training: Batch 2/2, Loss: 1.6608124597805585e-10\n",
            "Validation: Batch 1/1, Loss: 1.5400720976277427e-10\n",
            "Epoch: 2528/10000\n",
            "Training: Batch 1/2, Loss: 1.5689442800503883e-10\n",
            "Training: Batch 2/2, Loss: 1.658733844722704e-10\n",
            "Validation: Batch 1/1, Loss: 1.5069906433851088e-10\n",
            "Epoch: 2529/10000\n",
            "Training: Batch 1/2, Loss: 1.6728567142632045e-10\n",
            "Training: Batch 2/2, Loss: 1.4872797438059138e-10\n",
            "Validation: Batch 1/1, Loss: 1.475276706353057e-10\n",
            "Epoch: 2530/10000\n",
            "Training: Batch 1/2, Loss: 1.5460192848149035e-10\n",
            "Training: Batch 2/2, Loss: 1.5462205127381168e-10\n",
            "Validation: Batch 1/1, Loss: 1.4434876904889649e-10\n",
            "Epoch: 2531/10000\n",
            "Training: Batch 1/2, Loss: 1.47502302039193e-10\n",
            "Training: Batch 2/2, Loss: 1.551555689482953e-10\n",
            "Validation: Batch 1/1, Loss: 1.4134178549785048e-10\n",
            "Epoch: 2532/10000\n",
            "Training: Batch 1/2, Loss: 1.531008236854703e-10\n",
            "Training: Batch 2/2, Loss: 1.4341161591602258e-10\n",
            "Validation: Batch 1/1, Loss: 1.3830031564410206e-10\n",
            "Epoch: 2533/10000\n",
            "Training: Batch 1/2, Loss: 1.5308217193865659e-10\n",
            "Training: Batch 2/2, Loss: 1.372405106225827e-10\n",
            "Validation: Batch 1/1, Loss: 1.3541674726003095e-10\n",
            "Epoch: 2534/10000\n",
            "Training: Batch 1/2, Loss: 1.4143076987327419e-10\n",
            "Training: Batch 2/2, Loss: 1.4264761594162678e-10\n",
            "Validation: Batch 1/1, Loss: 1.3238807272664133e-10\n",
            "Epoch: 2535/10000\n",
            "Training: Batch 1/2, Loss: 1.3794358710850219e-10\n",
            "Training: Batch 2/2, Loss: 1.3999439107958978e-10\n",
            "Validation: Batch 1/1, Loss: 1.295080154228856e-10\n",
            "Epoch: 2536/10000\n",
            "Training: Batch 1/2, Loss: 1.3278773913771857e-10\n",
            "Training: Batch 2/2, Loss: 1.3914168428552642e-10\n",
            "Validation: Batch 1/1, Loss: 1.26658419863368e-10\n",
            "Epoch: 2537/10000\n",
            "Training: Batch 1/2, Loss: 1.3505800644519894e-10\n",
            "Training: Batch 2/2, Loss: 1.3128648168603263e-10\n",
            "Validation: Batch 1/1, Loss: 1.2444149877222088e-10\n",
            "Epoch: 2538/10000\n",
            "Training: Batch 1/2, Loss: 1.3627954320583058e-10\n",
            "Training: Batch 2/2, Loss: 1.2535990301376643e-10\n",
            "Validation: Batch 1/1, Loss: 1.2222692302721327e-10\n",
            "Epoch: 2539/10000\n",
            "Training: Batch 1/2, Loss: 1.278006728222536e-10\n",
            "Training: Batch 2/2, Loss: 1.2904063928509402e-10\n",
            "Validation: Batch 1/1, Loss: 1.2004244820396082e-10\n",
            "Epoch: 2540/10000\n",
            "Training: Batch 1/2, Loss: 1.2176464003754717e-10\n",
            "Training: Batch 2/2, Loss: 1.3047379843200702e-10\n",
            "Validation: Batch 1/1, Loss: 1.1792351817252467e-10\n",
            "Epoch: 2541/10000\n",
            "Training: Batch 1/2, Loss: 1.258821519245501e-10\n",
            "Training: Batch 2/2, Loss: 1.217874134873398e-10\n",
            "Validation: Batch 1/1, Loss: 1.1577054592759595e-10\n",
            "Epoch: 2542/10000\n",
            "Training: Batch 1/2, Loss: 1.221603790346748e-10\n",
            "Training: Batch 2/2, Loss: 1.2098833046536583e-10\n",
            "Validation: Batch 1/1, Loss: 1.136264068946069e-10\n",
            "Epoch: 2543/10000\n",
            "Training: Batch 1/2, Loss: 1.2192198639571217e-10\n",
            "Training: Batch 2/2, Loss: 1.1671337507568325e-10\n",
            "Validation: Batch 1/1, Loss: 1.1157064161437802e-10\n",
            "Epoch: 2544/10000\n",
            "Training: Batch 1/2, Loss: 1.1186385151518152e-10\n",
            "Training: Batch 2/2, Loss: 1.2227248380458633e-10\n",
            "Validation: Batch 1/1, Loss: 1.094958013148073e-10\n",
            "Epoch: 2545/10000\n",
            "Training: Batch 1/2, Loss: 1.1481436634763753e-10\n",
            "Training: Batch 2/2, Loss: 1.1501082031184495e-10\n",
            "Validation: Batch 1/1, Loss: 1.0742844114286498e-10\n",
            "Epoch: 2546/10000\n",
            "Training: Batch 1/2, Loss: 1.1319696568978799e-10\n",
            "Training: Batch 2/2, Loss: 1.122504172945682e-10\n",
            "Validation: Batch 1/1, Loss: 1.0554224161296588e-10\n",
            "Epoch: 2547/10000\n",
            "Training: Batch 1/2, Loss: 1.1398493954262179e-10\n",
            "Training: Batch 2/2, Loss: 1.0734939326351167e-10\n",
            "Validation: Batch 1/1, Loss: 1.0355648977222742e-10\n",
            "Epoch: 2548/10000\n",
            "Training: Batch 1/2, Loss: 1.0898210112131324e-10\n",
            "Training: Batch 2/2, Loss: 1.0844616177285715e-10\n",
            "Validation: Batch 1/1, Loss: 1.0177279158307684e-10\n",
            "Epoch: 2549/10000\n",
            "Training: Batch 1/2, Loss: 1.0326365457169473e-10\n",
            "Training: Batch 2/2, Loss: 1.1027202068136788e-10\n",
            "Validation: Batch 1/1, Loss: 9.999188976816953e-11\n",
            "Epoch: 2550/10000\n",
            "Training: Batch 1/2, Loss: 1.026990437136277e-10\n",
            "Training: Batch 2/2, Loss: 1.0706457248543799e-10\n",
            "Validation: Batch 1/1, Loss: 9.819113577780314e-11\n",
            "Epoch: 2551/10000\n",
            "Training: Batch 1/2, Loss: 1.0426763619175716e-10\n",
            "Training: Batch 2/2, Loss: 1.0181877563297803e-10\n",
            "Validation: Batch 1/1, Loss: 9.648330051570397e-11\n",
            "Epoch: 2552/10000\n",
            "Training: Batch 1/2, Loss: 1.0126224164741515e-10\n",
            "Training: Batch 2/2, Loss: 1.0105142417282664e-10\n",
            "Validation: Batch 1/1, Loss: 9.46843575766465e-11\n",
            "Epoch: 2553/10000\n",
            "Training: Batch 1/2, Loss: 9.741894790860073e-11\n",
            "Training: Batch 2/2, Loss: 1.0116901760781616e-10\n",
            "Validation: Batch 1/1, Loss: 9.297523861917512e-11\n",
            "Epoch: 2554/10000\n",
            "Training: Batch 1/2, Loss: 9.873131479043451e-11\n",
            "Training: Batch 2/2, Loss: 9.626469760215528e-11\n",
            "Validation: Batch 1/1, Loss: 9.128479222519914e-11\n",
            "Epoch: 2555/10000\n",
            "Training: Batch 1/2, Loss: 9.648479237789331e-11\n",
            "Training: Batch 2/2, Loss: 9.489786040317583e-11\n",
            "Validation: Batch 1/1, Loss: 8.956702046702958e-11\n",
            "Epoch: 2556/10000\n",
            "Training: Batch 1/2, Loss: 9.347857210517674e-11\n",
            "Training: Batch 2/2, Loss: 9.431291858597035e-11\n",
            "Validation: Batch 1/1, Loss: 8.790335820352269e-11\n",
            "Epoch: 2557/10000\n",
            "Training: Batch 1/2, Loss: 9.1331955887064e-11\n",
            "Training: Batch 2/2, Loss: 9.292840802421765e-11\n",
            "Validation: Batch 1/1, Loss: 8.623653874328951e-11\n",
            "Epoch: 2558/10000\n",
            "Training: Batch 1/2, Loss: 9.212965113025717e-11\n",
            "Training: Batch 2/2, Loss: 8.863113021284619e-11\n",
            "Validation: Batch 1/1, Loss: 8.455310063215649e-11\n",
            "Epoch: 2559/10000\n",
            "Training: Batch 1/2, Loss: 9.061478650762567e-11\n",
            "Training: Batch 2/2, Loss: 8.668999545990985e-11\n",
            "Validation: Batch 1/1, Loss: 8.295525377732815e-11\n",
            "Epoch: 2560/10000\n",
            "Training: Batch 1/2, Loss: 8.660461237042227e-11\n",
            "Training: Batch 2/2, Loss: 8.723273492439176e-11\n",
            "Validation: Batch 1/1, Loss: 8.135172396839252e-11\n",
            "Epoch: 2561/10000\n",
            "Training: Batch 1/2, Loss: 8.488937330852764e-11\n",
            "Training: Batch 2/2, Loss: 8.552495517344383e-11\n",
            "Validation: Batch 1/1, Loss: 7.971969612219354e-11\n",
            "Epoch: 2562/10000\n",
            "Training: Batch 1/2, Loss: 8.257532158051362e-11\n",
            "Training: Batch 2/2, Loss: 8.433716225386689e-11\n",
            "Validation: Batch 1/1, Loss: 7.804427243351952e-11\n",
            "Epoch: 2563/10000\n",
            "Training: Batch 1/2, Loss: 8.498328429862312e-11\n",
            "Training: Batch 2/2, Loss: 7.857335615479855e-11\n",
            "Validation: Batch 1/1, Loss: 7.649485211924656e-11\n",
            "Epoch: 2564/10000\n",
            "Training: Batch 1/2, Loss: 7.977084964805314e-11\n",
            "Training: Batch 2/2, Loss: 8.043683774605626e-11\n",
            "Validation: Batch 1/1, Loss: 7.494661835583116e-11\n",
            "Epoch: 2565/10000\n",
            "Training: Batch 1/2, Loss: 7.951760777613615e-11\n",
            "Training: Batch 2/2, Loss: 7.745531299674369e-11\n",
            "Validation: Batch 1/1, Loss: 7.343691627026416e-11\n",
            "Epoch: 2566/10000\n",
            "Training: Batch 1/2, Loss: 7.824155906499541e-11\n",
            "Training: Batch 2/2, Loss: 7.550494951491515e-11\n",
            "Validation: Batch 1/1, Loss: 7.188614287167994e-11\n",
            "Epoch: 2567/10000\n",
            "Training: Batch 1/2, Loss: 7.632360715659203e-11\n",
            "Training: Batch 2/2, Loss: 7.422055331440802e-11\n",
            "Validation: Batch 1/1, Loss: 7.03904018406476e-11\n",
            "Epoch: 2568/10000\n",
            "Training: Batch 1/2, Loss: 7.264437662524159e-11\n",
            "Training: Batch 2/2, Loss: 7.468852619707533e-11\n",
            "Validation: Batch 1/1, Loss: 6.893206144775732e-11\n",
            "Epoch: 2569/10000\n",
            "Training: Batch 1/2, Loss: 7.083048730871511e-11\n",
            "Training: Batch 2/2, Loss: 7.339589352950426e-11\n",
            "Validation: Batch 1/1, Loss: 6.745293906762484e-11\n",
            "Epoch: 2570/10000\n",
            "Training: Batch 1/2, Loss: 7.205008117905365e-11\n",
            "Training: Batch 2/2, Loss: 6.911906463846762e-11\n",
            "Validation: Batch 1/1, Loss: 6.598888796505165e-11\n",
            "Epoch: 2571/10000\n",
            "Training: Batch 1/2, Loss: 7.263891571573922e-11\n",
            "Training: Batch 2/2, Loss: 6.548397241123993e-11\n",
            "Validation: Batch 1/1, Loss: 6.457719081698343e-11\n",
            "Epoch: 2572/10000\n",
            "Training: Batch 1/2, Loss: 6.557630133352532e-11\n",
            "Training: Batch 2/2, Loss: 6.947079717045668e-11\n",
            "Validation: Batch 1/1, Loss: 6.314727213352356e-11\n",
            "Epoch: 2573/10000\n",
            "Training: Batch 1/2, Loss: 6.584913170293305e-11\n",
            "Training: Batch 2/2, Loss: 6.623581544351609e-11\n",
            "Validation: Batch 1/1, Loss: 6.176039540894962e-11\n",
            "Epoch: 2574/10000\n",
            "Training: Batch 1/2, Loss: 6.5846030017358e-11\n",
            "Training: Batch 2/2, Loss: 6.328392671006711e-11\n",
            "Validation: Batch 1/1, Loss: 6.03817204569701e-11\n",
            "Epoch: 2575/10000\n",
            "Training: Batch 1/2, Loss: 6.473734048828561e-11\n",
            "Training: Batch 2/2, Loss: 6.148646175541117e-11\n",
            "Validation: Batch 1/1, Loss: 5.899911115214707e-11\n",
            "Epoch: 2576/10000\n",
            "Training: Batch 1/2, Loss: 6.327506574255182e-11\n",
            "Training: Batch 2/2, Loss: 6.006119906976082e-11\n",
            "Validation: Batch 1/1, Loss: 5.763045943241174e-11\n",
            "Epoch: 2577/10000\n",
            "Training: Batch 1/2, Loss: 5.918911194502385e-11\n",
            "Training: Batch 2/2, Loss: 6.124817319985709e-11\n",
            "Validation: Batch 1/1, Loss: 5.6336022652425655e-11\n",
            "Epoch: 2578/10000\n",
            "Training: Batch 1/2, Loss: 5.888501491968512e-11\n",
            "Training: Batch 2/2, Loss: 5.877280606636504e-11\n",
            "Validation: Batch 1/1, Loss: 5.499353403215501e-11\n",
            "Epoch: 2579/10000\n",
            "Training: Batch 1/2, Loss: 6.091594589863192e-11\n",
            "Training: Batch 2/2, Loss: 5.402225194961474e-11\n",
            "Validation: Batch 1/1, Loss: 5.368549702011727e-11\n",
            "Epoch: 2580/10000\n",
            "Training: Batch 1/2, Loss: 5.639241504318271e-11\n",
            "Training: Batch 2/2, Loss: 5.5670072718339014e-11\n",
            "Validation: Batch 1/1, Loss: 5.239987957428305e-11\n",
            "Epoch: 2581/10000\n",
            "Training: Batch 1/2, Loss: 5.582213510879619e-11\n",
            "Training: Batch 2/2, Loss: 5.370373937219064e-11\n",
            "Validation: Batch 1/1, Loss: 5.124890095631329e-11\n",
            "Epoch: 2582/10000\n",
            "Training: Batch 1/2, Loss: 5.531578320394637e-11\n",
            "Training: Batch 2/2, Loss: 5.173700357019584e-11\n",
            "Validation: Batch 1/1, Loss: 5.004021502719169e-11\n",
            "Epoch: 2583/10000\n",
            "Training: Batch 1/2, Loss: 5.1792709010456406e-11\n",
            "Training: Batch 2/2, Loss: 5.283906298614305e-11\n",
            "Validation: Batch 1/1, Loss: 4.89208673570829e-11\n",
            "Epoch: 2584/10000\n",
            "Training: Batch 1/2, Loss: 5.073775427577587e-11\n",
            "Training: Batch 2/2, Loss: 5.154027205023226e-11\n",
            "Validation: Batch 1/1, Loss: 4.783207510627996e-11\n",
            "Epoch: 2585/10000\n",
            "Training: Batch 1/2, Loss: 4.892717481164155e-11\n",
            "Training: Batch 2/2, Loss: 5.1132816730747876e-11\n",
            "Validation: Batch 1/1, Loss: 4.67792957148383e-11\n",
            "Epoch: 2586/10000\n",
            "Training: Batch 1/2, Loss: 5.018123069855385e-11\n",
            "Training: Batch 2/2, Loss: 4.776161063868578e-11\n",
            "Validation: Batch 1/1, Loss: 4.5709127455273446e-11\n",
            "Epoch: 2587/10000\n",
            "Training: Batch 1/2, Loss: 4.6538335685131216e-11\n",
            "Training: Batch 2/2, Loss: 4.9232572879587266e-11\n",
            "Validation: Batch 1/1, Loss: 4.470460807093346e-11\n",
            "Epoch: 2588/10000\n",
            "Training: Batch 1/2, Loss: 4.808891132523918e-11\n",
            "Training: Batch 2/2, Loss: 4.56337433119014e-11\n",
            "Validation: Batch 1/1, Loss: 4.369473186049966e-11\n",
            "Epoch: 2589/10000\n",
            "Training: Batch 1/2, Loss: 4.6831784977774404e-11\n",
            "Training: Batch 2/2, Loss: 4.488558830173517e-11\n",
            "Validation: Batch 1/1, Loss: 4.27074903219804e-11\n",
            "Epoch: 2590/10000\n",
            "Training: Batch 1/2, Loss: 4.7283593707092564e-11\n",
            "Training: Batch 2/2, Loss: 4.2431006624932266e-11\n",
            "Validation: Batch 1/1, Loss: 4.189091087902774e-11\n",
            "Epoch: 2591/10000\n",
            "Training: Batch 1/2, Loss: 4.412589391100674e-11\n",
            "Training: Batch 2/2, Loss: 4.398861830345879e-11\n",
            "Validation: Batch 1/1, Loss: 4.127718999935581e-11\n",
            "Epoch: 2592/10000\n",
            "Training: Batch 1/2, Loss: 4.4069282945091715e-11\n",
            "Training: Batch 2/2, Loss: 4.274685813654422e-11\n",
            "Validation: Batch 1/1, Loss: 4.063390249275933e-11\n",
            "Epoch: 2593/10000\n",
            "Training: Batch 1/2, Loss: 4.215532090068308e-11\n",
            "Training: Batch 2/2, Loss: 4.331234676469009e-11\n",
            "Validation: Batch 1/1, Loss: 4.0033292653118835e-11\n",
            "Epoch: 2594/10000\n",
            "Training: Batch 1/2, Loss: 4.270524905924944e-11\n",
            "Training: Batch 2/2, Loss: 4.145801410504468e-11\n",
            "Validation: Batch 1/1, Loss: 3.941577619848147e-11\n",
            "Epoch: 2595/10000\n",
            "Training: Batch 1/2, Loss: 4.159713198892412e-11\n",
            "Training: Batch 2/2, Loss: 4.126043604002483e-11\n",
            "Validation: Batch 1/1, Loss: 3.879327067912719e-11\n",
            "Epoch: 2596/10000\n",
            "Training: Batch 1/2, Loss: 4.1564862662824e-11\n",
            "Training: Batch 2/2, Loss: 3.998760697565551e-11\n",
            "Validation: Batch 1/1, Loss: 3.8209001934630393e-11\n",
            "Epoch: 2597/10000\n",
            "Training: Batch 1/2, Loss: 4.076620638282513e-11\n",
            "Training: Batch 2/2, Loss: 3.9500611115350637e-11\n",
            "Validation: Batch 1/1, Loss: 3.7616812442742287e-11\n",
            "Epoch: 2598/10000\n",
            "Training: Batch 1/2, Loss: 4.113418633489019e-11\n",
            "Training: Batch 2/2, Loss: 3.784692698127756e-11\n",
            "Validation: Batch 1/1, Loss: 3.704170997709255e-11\n",
            "Epoch: 2599/10000\n",
            "Training: Batch 1/2, Loss: 4.0890704017249035e-11\n",
            "Training: Batch 2/2, Loss: 3.6783891904645927e-11\n",
            "Validation: Batch 1/1, Loss: 3.6349270815527746e-11\n",
            "Epoch: 2600/10000\n",
            "Training: Batch 1/2, Loss: 3.7546483283579235e-11\n",
            "Training: Batch 2/2, Loss: 3.876932802571176e-11\n",
            "Validation: Batch 1/1, Loss: 3.577739493554333e-11\n",
            "Epoch: 2601/10000\n",
            "Training: Batch 1/2, Loss: 3.635444376093311e-11\n",
            "Training: Batch 2/2, Loss: 3.871911471997613e-11\n",
            "Validation: Batch 1/1, Loss: 3.520771868492645e-11\n",
            "Epoch: 2602/10000\n",
            "Training: Batch 1/2, Loss: 3.693282832339939e-11\n",
            "Training: Batch 2/2, Loss: 3.69254765653082e-11\n",
            "Validation: Batch 1/1, Loss: 3.464612624570762e-11\n",
            "Epoch: 2603/10000\n",
            "Training: Batch 1/2, Loss: 3.5234540979312e-11\n",
            "Training: Batch 2/2, Loss: 3.7392033913619116e-11\n",
            "Validation: Batch 1/1, Loss: 3.406369283864841e-11\n",
            "Epoch: 2604/10000\n",
            "Training: Batch 1/2, Loss: 3.539256041018568e-11\n",
            "Training: Batch 2/2, Loss: 3.6036871403632986e-11\n",
            "Validation: Batch 1/1, Loss: 3.35283259489394e-11\n",
            "Epoch: 2605/10000\n",
            "Training: Batch 1/2, Loss: 3.5933707398516646e-11\n",
            "Training: Batch 2/2, Loss: 3.4312015034787535e-11\n",
            "Validation: Batch 1/1, Loss: 3.295563474892127e-11\n",
            "Epoch: 2606/10000\n",
            "Training: Batch 1/2, Loss: 3.4829673867253774e-11\n",
            "Training: Batch 2/2, Loss: 3.4231888157432167e-11\n",
            "Validation: Batch 1/1, Loss: 3.240549495409084e-11\n",
            "Epoch: 2607/10000\n",
            "Training: Batch 1/2, Loss: 3.3474355232154807e-11\n",
            "Training: Batch 2/2, Loss: 3.441824256156245e-11\n",
            "Validation: Batch 1/1, Loss: 3.185762764701394e-11\n",
            "Epoch: 2608/10000\n",
            "Training: Batch 1/2, Loss: 3.347004964848743e-11\n",
            "Training: Batch 2/2, Loss: 3.32631838739772e-11\n",
            "Validation: Batch 1/1, Loss: 3.132322179411062e-11\n",
            "Epoch: 2609/10000\n",
            "Training: Batch 1/2, Loss: 3.334703346791201e-11\n",
            "Training: Batch 2/2, Loss: 3.2252311932268185e-11\n",
            "Validation: Batch 1/1, Loss: 3.0794432975822517e-11\n",
            "Epoch: 2610/10000\n",
            "Training: Batch 1/2, Loss: 3.1299979968979486e-11\n",
            "Training: Batch 2/2, Loss: 3.3165255264311355e-11\n",
            "Validation: Batch 1/1, Loss: 3.0263520855999815e-11\n",
            "Epoch: 2611/10000\n",
            "Training: Batch 1/2, Loss: 3.1467765893022914e-11\n",
            "Training: Batch 2/2, Loss: 3.186105199115552e-11\n",
            "Validation: Batch 1/1, Loss: 2.976431601187102e-11\n",
            "Epoch: 2612/10000\n",
            "Training: Batch 1/2, Loss: 3.1304046160807175e-11\n",
            "Training: Batch 2/2, Loss: 3.092613665156563e-11\n",
            "Validation: Batch 1/1, Loss: 2.923156855461073e-11\n",
            "Epoch: 2613/10000\n",
            "Training: Batch 1/2, Loss: 3.042437482503324e-11\n",
            "Training: Batch 2/2, Loss: 3.069358309182313e-11\n",
            "Validation: Batch 1/1, Loss: 2.871654997127493e-11\n",
            "Epoch: 2614/10000\n",
            "Training: Batch 1/2, Loss: 3.0586845556346276e-11\n",
            "Training: Batch 2/2, Loss: 2.946573887663284e-11\n",
            "Validation: Batch 1/1, Loss: 2.8229425741432834e-11\n",
            "Epoch: 2615/10000\n",
            "Training: Batch 1/2, Loss: 2.8506572102848793e-11\n",
            "Training: Batch 2/2, Loss: 3.041478180421109e-11\n",
            "Validation: Batch 1/1, Loss: 2.7692837606396736e-11\n",
            "Epoch: 2616/10000\n",
            "Training: Batch 1/2, Loss: 2.7618459602640755e-11\n",
            "Training: Batch 2/2, Loss: 3.023863451301345e-11\n",
            "Validation: Batch 1/1, Loss: 2.7208893124686107e-11\n",
            "Epoch: 2617/10000\n",
            "Training: Batch 1/2, Loss: 2.7783803036030008e-11\n",
            "Training: Batch 2/2, Loss: 2.9001287482621763e-11\n",
            "Validation: Batch 1/1, Loss: 2.662942782949429e-11\n",
            "Epoch: 2618/10000\n",
            "Training: Batch 1/2, Loss: 2.8523607087382885e-11\n",
            "Training: Batch 2/2, Loss: 2.7094791688053732e-11\n",
            "Validation: Batch 1/1, Loss: 2.616543440248087e-11\n",
            "Epoch: 2619/10000\n",
            "Training: Batch 1/2, Loss: 2.8550889083489572e-11\n",
            "Training: Batch 2/2, Loss: 2.6024096072552183e-11\n",
            "Validation: Batch 1/1, Loss: 2.568286035231626e-11\n",
            "Epoch: 2620/10000\n",
            "Training: Batch 1/2, Loss: 2.6636578359662266e-11\n",
            "Training: Batch 2/2, Loss: 2.6905618358274985e-11\n",
            "Validation: Batch 1/1, Loss: 2.5194105482406748e-11\n",
            "Epoch: 2621/10000\n",
            "Training: Batch 1/2, Loss: 2.5946833223655652e-11\n",
            "Training: Batch 2/2, Loss: 2.6585495957465177e-11\n",
            "Validation: Batch 1/1, Loss: 2.4723920827307566e-11\n",
            "Epoch: 2622/10000\n",
            "Training: Batch 1/2, Loss: 2.5585502466674015e-11\n",
            "Training: Batch 2/2, Loss: 2.59411468001014e-11\n",
            "Validation: Batch 1/1, Loss: 2.4271411269705112e-11\n",
            "Epoch: 2623/10000\n",
            "Training: Batch 1/2, Loss: 2.6352775864268985e-11\n",
            "Training: Batch 2/2, Loss: 2.4208340193565547e-11\n",
            "Validation: Batch 1/1, Loss: 2.3795439577090072e-11\n",
            "Epoch: 2624/10000\n",
            "Training: Batch 1/2, Loss: 2.4571584350541187e-11\n",
            "Training: Batch 2/2, Loss: 2.5018175031643608e-11\n",
            "Validation: Batch 1/1, Loss: 2.336343098152671e-11\n",
            "Epoch: 2625/10000\n",
            "Training: Batch 1/2, Loss: 2.3106358840174757e-11\n",
            "Training: Batch 2/2, Loss: 2.5494616834320638e-11\n",
            "Validation: Batch 1/1, Loss: 2.2945175273680896e-11\n",
            "Epoch: 2626/10000\n",
            "Training: Batch 1/2, Loss: 2.382877055395749e-11\n",
            "Training: Batch 2/2, Loss: 2.3956959679938272e-11\n",
            "Validation: Batch 1/1, Loss: 2.2571252158987143e-11\n",
            "Epoch: 2627/10000\n",
            "Training: Batch 1/2, Loss: 2.4356650377144184e-11\n",
            "Training: Batch 2/2, Loss: 2.265529951139822e-11\n",
            "Validation: Batch 1/1, Loss: 2.218388493735457e-11\n",
            "Epoch: 2628/10000\n",
            "Training: Batch 1/2, Loss: 2.3139774818492498e-11\n",
            "Training: Batch 2/2, Loss: 2.310631373736438e-11\n",
            "Validation: Batch 1/1, Loss: 2.1830422886059964e-11\n",
            "Epoch: 2629/10000\n",
            "Training: Batch 1/2, Loss: 2.2407643449073866e-11\n",
            "Training: Batch 2/2, Loss: 2.3063939647016696e-11\n",
            "Validation: Batch 1/1, Loss: 2.1464620011957258e-11\n",
            "Epoch: 2630/10000\n",
            "Training: Batch 1/2, Loss: 2.2720655218355645e-11\n",
            "Training: Batch 2/2, Loss: 2.2003811966930797e-11\n",
            "Validation: Batch 1/1, Loss: 2.1099153674208893e-11\n",
            "Epoch: 2631/10000\n",
            "Training: Batch 1/2, Loss: 2.1632065930199396e-11\n",
            "Training: Batch 2/2, Loss: 2.2331570620481855e-11\n",
            "Validation: Batch 1/1, Loss: 2.0752620108477338e-11\n",
            "Epoch: 2632/10000\n",
            "Training: Batch 1/2, Loss: 2.221502495847183e-11\n",
            "Training: Batch 2/2, Loss: 2.101883077310074e-11\n",
            "Validation: Batch 1/1, Loss: 2.039515778484713e-11\n",
            "Epoch: 2633/10000\n",
            "Training: Batch 1/2, Loss: 2.1402962735450615e-11\n",
            "Training: Batch 2/2, Loss: 2.1088400123381312e-11\n",
            "Validation: Batch 1/1, Loss: 2.0043248311063522e-11\n",
            "Epoch: 2634/10000\n",
            "Training: Batch 1/2, Loss: 2.0658119312400025e-11\n",
            "Training: Batch 2/2, Loss: 2.1108564549066067e-11\n",
            "Validation: Batch 1/1, Loss: 1.9695533398644827e-11\n",
            "Epoch: 2635/10000\n",
            "Training: Batch 1/2, Loss: 2.125173648170886e-11\n",
            "Training: Batch 2/2, Loss: 1.979779014338323e-11\n",
            "Validation: Batch 1/1, Loss: 1.9362001585365718e-11\n",
            "Epoch: 2636/10000\n",
            "Training: Batch 1/2, Loss: 1.9907926002149523e-11\n",
            "Training: Batch 2/2, Loss: 2.033917305410693e-11\n",
            "Validation: Batch 1/1, Loss: 1.8947130323021533e-11\n",
            "Epoch: 2637/10000\n",
            "Training: Batch 1/2, Loss: 2.0106151119025917e-11\n",
            "Training: Batch 2/2, Loss: 1.9383109700621404e-11\n",
            "Validation: Batch 1/1, Loss: 1.860711931755965e-11\n",
            "Epoch: 2638/10000\n",
            "Training: Batch 1/2, Loss: 1.9849686130890554e-11\n",
            "Training: Batch 2/2, Loss: 1.893079443204826e-11\n",
            "Validation: Batch 1/1, Loss: 1.8283111136163654e-11\n",
            "Epoch: 2639/10000\n",
            "Training: Batch 1/2, Loss: 1.831744304847671e-11\n",
            "Training: Batch 2/2, Loss: 1.976977262452273e-11\n",
            "Validation: Batch 1/1, Loss: 1.7962796181048013e-11\n",
            "Epoch: 2640/10000\n",
            "Training: Batch 1/2, Loss: 1.8785108885088775e-11\n",
            "Training: Batch 2/2, Loss: 1.8635553170054386e-11\n",
            "Validation: Batch 1/1, Loss: 1.7624806128435644e-11\n",
            "Epoch: 2641/10000\n",
            "Training: Batch 1/2, Loss: 1.849683253785095e-11\n",
            "Training: Batch 2/2, Loss: 1.8235229298779743e-11\n",
            "Validation: Batch 1/1, Loss: 1.7305780072862653e-11\n",
            "Epoch: 2642/10000\n",
            "Training: Batch 1/2, Loss: 1.8628851933266688e-11\n",
            "Training: Batch 2/2, Loss: 1.7463230514436212e-11\n",
            "Validation: Batch 1/1, Loss: 1.700107936375428e-11\n",
            "Epoch: 2643/10000\n",
            "Training: Batch 1/2, Loss: 1.7049878869856983e-11\n",
            "Training: Batch 2/2, Loss: 1.8357869044360875e-11\n",
            "Validation: Batch 1/1, Loss: 1.6689906401357035e-11\n",
            "Epoch: 2644/10000\n",
            "Training: Batch 1/2, Loss: 1.7670000879155268e-11\n",
            "Training: Batch 2/2, Loss: 1.7107997310472634e-11\n",
            "Validation: Batch 1/1, Loss: 1.6369617467093533e-11\n",
            "Epoch: 2645/10000\n",
            "Training: Batch 1/2, Loss: 1.734530401253931e-11\n",
            "Training: Batch 2/2, Loss: 1.6796938839824804e-11\n",
            "Validation: Batch 1/1, Loss: 1.6086704884843428e-11\n",
            "Epoch: 2646/10000\n",
            "Training: Batch 1/2, Loss: 1.6881562120429905e-11\n",
            "Training: Batch 2/2, Loss: 1.663037589583194e-11\n",
            "Validation: Batch 1/1, Loss: 1.577934484464638e-11\n",
            "Epoch: 2647/10000\n",
            "Training: Batch 1/2, Loss: 1.6739108363306165e-11\n",
            "Training: Batch 2/2, Loss: 1.61329647557773e-11\n",
            "Validation: Batch 1/1, Loss: 1.5482910092901037e-11\n",
            "Epoch: 2648/10000\n",
            "Training: Batch 1/2, Loss: 1.5873063280436028e-11\n",
            "Training: Batch 2/2, Loss: 1.638207104692757e-11\n",
            "Validation: Batch 1/1, Loss: 1.5203045419798222e-11\n",
            "Epoch: 2649/10000\n",
            "Training: Batch 1/2, Loss: 1.5464032832035457e-11\n",
            "Training: Batch 2/2, Loss: 1.6178483899786933e-11\n",
            "Validation: Batch 1/1, Loss: 1.488781840863762e-11\n",
            "Epoch: 2650/10000\n",
            "Training: Batch 1/2, Loss: 1.613197422867252e-11\n",
            "Training: Batch 2/2, Loss: 1.4920053040268222e-11\n",
            "Validation: Batch 1/1, Loss: 1.4599148279170748e-11\n",
            "Epoch: 2651/10000\n",
            "Training: Batch 1/2, Loss: 1.5252393098519335e-11\n",
            "Training: Batch 2/2, Loss: 1.5180294521410787e-11\n",
            "Validation: Batch 1/1, Loss: 1.4326252857632671e-11\n",
            "Epoch: 2652/10000\n",
            "Training: Batch 1/2, Loss: 1.494768891996401e-11\n",
            "Training: Batch 2/2, Loss: 1.4925461908066318e-11\n",
            "Validation: Batch 1/1, Loss: 1.4033811439329025e-11\n",
            "Epoch: 2653/10000\n",
            "Training: Batch 1/2, Loss: 1.44051116521271e-11\n",
            "Training: Batch 2/2, Loss: 1.4887395136109483e-11\n",
            "Validation: Batch 1/1, Loss: 1.3769450857292753e-11\n",
            "Epoch: 2654/10000\n",
            "Training: Batch 1/2, Loss: 1.4372136293572257e-11\n",
            "Training: Batch 2/2, Loss: 1.4263660565172476e-11\n",
            "Validation: Batch 1/1, Loss: 1.3430235222544606e-11\n",
            "Epoch: 2655/10000\n",
            "Training: Batch 1/2, Loss: 1.4127790951001806e-11\n",
            "Training: Batch 2/2, Loss: 1.3872799610459285e-11\n",
            "Validation: Batch 1/1, Loss: 1.3164347216487737e-11\n",
            "Epoch: 2656/10000\n",
            "Training: Batch 1/2, Loss: 1.3619333959213886e-11\n",
            "Training: Batch 2/2, Loss: 1.3796232385676621e-11\n",
            "Validation: Batch 1/1, Loss: 1.2899157436629949e-11\n",
            "Epoch: 2657/10000\n",
            "Training: Batch 1/2, Loss: 1.348923663740953e-11\n",
            "Training: Batch 2/2, Loss: 1.3389220288040349e-11\n",
            "Validation: Batch 1/1, Loss: 1.2632393395217711e-11\n",
            "Epoch: 2658/10000\n",
            "Training: Batch 1/2, Loss: 1.346686651082507e-11\n",
            "Training: Batch 2/2, Loss: 1.2863512338645577e-11\n",
            "Validation: Batch 1/1, Loss: 1.2373344536464881e-11\n",
            "Epoch: 2659/10000\n",
            "Training: Batch 1/2, Loss: 1.3360557452046784e-11\n",
            "Training: Batch 2/2, Loss: 1.2441049926370518e-11\n",
            "Validation: Batch 1/1, Loss: 1.2118373145242334e-11\n",
            "Epoch: 2660/10000\n",
            "Training: Batch 1/2, Loss: 1.3078714326819618e-11\n",
            "Training: Batch 2/2, Loss: 1.220083842984332e-11\n",
            "Validation: Batch 1/1, Loss: 1.1887379101904738e-11\n",
            "Epoch: 2661/10000\n",
            "Training: Batch 1/2, Loss: 1.2805106801294652e-11\n",
            "Training: Batch 2/2, Loss: 1.1974914983226004e-11\n",
            "Validation: Batch 1/1, Loss: 1.1620875269013897e-11\n",
            "Epoch: 2662/10000\n",
            "Training: Batch 1/2, Loss: 1.2205450192204204e-11\n",
            "Training: Batch 2/2, Loss: 1.2051799662404772e-11\n",
            "Validation: Batch 1/1, Loss: 1.1395009935633027e-11\n",
            "Epoch: 2663/10000\n",
            "Training: Batch 1/2, Loss: 1.188109159666606e-11\n",
            "Training: Batch 2/2, Loss: 1.1886531689486723e-11\n",
            "Validation: Batch 1/1, Loss: 1.1148250725345754e-11\n",
            "Epoch: 2664/10000\n",
            "Training: Batch 1/2, Loss: 1.1534049756956044e-11\n",
            "Training: Batch 2/2, Loss: 1.1741430744338643e-11\n",
            "Validation: Batch 1/1, Loss: 1.0912643184923798e-11\n",
            "Epoch: 2665/10000\n",
            "Training: Batch 1/2, Loss: 1.1697002741395401e-11\n",
            "Training: Batch 2/2, Loss: 1.1080849779410151e-11\n",
            "Validation: Batch 1/1, Loss: 1.0689072023339907e-11\n",
            "Epoch: 2666/10000\n",
            "Training: Batch 1/2, Loss: 1.1237351327242351e-11\n",
            "Training: Batch 2/2, Loss: 1.1059325330520231e-11\n",
            "Validation: Batch 1/1, Loss: 1.0457307762778978e-11\n",
            "Epoch: 2667/10000\n",
            "Training: Batch 1/2, Loss: 1.0578284775908386e-11\n",
            "Training: Batch 2/2, Loss: 1.1249414594294294e-11\n",
            "Validation: Batch 1/1, Loss: 1.024731341447982e-11\n",
            "Epoch: 2668/10000\n",
            "Training: Batch 1/2, Loss: 1.0549363466116901e-11\n",
            "Training: Batch 2/2, Loss: 1.0802999120262946e-11\n",
            "Validation: Batch 1/1, Loss: 1.0020328317095206e-11\n",
            "Epoch: 2669/10000\n",
            "Training: Batch 1/2, Loss: 1.0549807555326751e-11\n",
            "Training: Batch 2/2, Loss: 1.035741110932964e-11\n",
            "Validation: Batch 1/1, Loss: 9.812617732252171e-12\n",
            "Epoch: 2670/10000\n",
            "Training: Batch 1/2, Loss: 1.0228730189243418e-11\n",
            "Training: Batch 2/2, Loss: 1.0215642567978911e-11\n",
            "Validation: Batch 1/1, Loss: 9.600958916777813e-12\n",
            "Epoch: 2671/10000\n",
            "Training: Batch 1/2, Loss: 1.0010470750942968e-11\n",
            "Training: Batch 2/2, Loss: 1.0003094706723115e-11\n",
            "Validation: Batch 1/1, Loss: 9.387226239387925e-12\n",
            "Epoch: 2672/10000\n",
            "Training: Batch 1/2, Loss: 9.337986980884061e-12\n",
            "Training: Batch 2/2, Loss: 1.0243107577412314e-11\n",
            "Validation: Batch 1/1, Loss: 9.202059253476946e-12\n",
            "Epoch: 2673/10000\n",
            "Training: Batch 1/2, Loss: 9.818661508842474e-12\n",
            "Training: Batch 2/2, Loss: 9.270612923162336e-12\n",
            "Validation: Batch 1/1, Loss: 8.917034645394839e-12\n",
            "Epoch: 2674/10000\n",
            "Training: Batch 1/2, Loss: 9.552972995985343e-12\n",
            "Training: Batch 2/2, Loss: 9.059525699073312e-12\n",
            "Validation: Batch 1/1, Loss: 8.736436053757846e-12\n",
            "Epoch: 2675/10000\n",
            "Training: Batch 1/2, Loss: 9.221608719689467e-12\n",
            "Training: Batch 2/2, Loss: 9.02447994804989e-12\n",
            "Validation: Batch 1/1, Loss: 8.548325242108135e-12\n",
            "Epoch: 2676/10000\n",
            "Training: Batch 1/2, Loss: 8.91663305691015e-12\n",
            "Training: Batch 2/2, Loss: 8.959446899658996e-12\n",
            "Validation: Batch 1/1, Loss: 8.365069054105945e-12\n",
            "Epoch: 2677/10000\n",
            "Training: Batch 1/2, Loss: 8.89407384546681e-12\n",
            "Training: Batch 2/2, Loss: 8.610131704833712e-12\n",
            "Validation: Batch 1/1, Loss: 8.183797389760272e-12\n",
            "Epoch: 2678/10000\n",
            "Training: Batch 1/2, Loss: 8.415624100366337e-12\n",
            "Training: Batch 2/2, Loss: 8.71989806749962e-12\n",
            "Validation: Batch 1/1, Loss: 8.053798079832308e-12\n",
            "Epoch: 2679/10000\n",
            "Training: Batch 1/2, Loss: 8.578049728868997e-12\n",
            "Training: Batch 2/2, Loss: 8.351434994946505e-12\n",
            "Validation: Batch 1/1, Loss: 7.98473960561541e-12\n",
            "Epoch: 2680/10000\n",
            "Training: Batch 1/2, Loss: 8.400366340033383e-12\n",
            "Training: Batch 2/2, Loss: 8.338675236418958e-12\n",
            "Validation: Batch 1/1, Loss: 7.89850736898634e-12\n",
            "Epoch: 2681/10000\n",
            "Training: Batch 1/2, Loss: 8.20104140847322e-12\n",
            "Training: Batch 2/2, Loss: 8.334672361998141e-12\n",
            "Validation: Batch 1/1, Loss: 7.827010740923956e-12\n",
            "Epoch: 2682/10000\n",
            "Training: Batch 1/2, Loss: 8.057680390971544e-12\n",
            "Training: Batch 2/2, Loss: 8.315403920988729e-12\n",
            "Validation: Batch 1/1, Loss: 7.765933729420027e-12\n",
            "Epoch: 2683/10000\n",
            "Training: Batch 1/2, Loss: 8.109544286094561e-12\n",
            "Training: Batch 2/2, Loss: 8.113352871486068e-12\n",
            "Validation: Batch 1/1, Loss: 7.695179563005361e-12\n",
            "Epoch: 2684/10000\n",
            "Training: Batch 1/2, Loss: 8.387773982321267e-12\n",
            "Training: Batch 2/2, Loss: 7.689079407902089e-12\n",
            "Validation: Batch 1/1, Loss: 7.634687153312836e-12\n",
            "Epoch: 2685/10000\n",
            "Training: Batch 1/2, Loss: 7.99653659261379e-12\n",
            "Training: Batch 2/2, Loss: 7.937777171673766e-12\n",
            "Validation: Batch 1/1, Loss: 7.567931524510296e-12\n",
            "Epoch: 2686/10000\n",
            "Training: Batch 1/2, Loss: 7.542515223502022e-12\n",
            "Training: Batch 2/2, Loss: 8.238996290765854e-12\n",
            "Validation: Batch 1/1, Loss: 7.510318755787893e-12\n",
            "Epoch: 2687/10000\n",
            "Training: Batch 1/2, Loss: 7.862275067105351e-12\n",
            "Training: Batch 2/2, Loss: 7.79502417475042e-12\n",
            "Validation: Batch 1/1, Loss: 7.45520659095611e-12\n",
            "Epoch: 2688/10000\n",
            "Training: Batch 1/2, Loss: 7.79815448326282e-12\n",
            "Training: Batch 2/2, Loss: 7.720060701821296e-12\n",
            "Validation: Batch 1/1, Loss: 7.40929453207917e-12\n",
            "Epoch: 2689/10000\n",
            "Training: Batch 1/2, Loss: 7.413205466155759e-12\n",
            "Training: Batch 2/2, Loss: 7.983923418219963e-12\n",
            "Validation: Batch 1/1, Loss: 7.352190904696965e-12\n",
            "Epoch: 2690/10000\n",
            "Training: Batch 1/2, Loss: 7.733914203500447e-12\n",
            "Training: Batch 2/2, Loss: 7.552090029727676e-12\n",
            "Validation: Batch 1/1, Loss: 7.307845301118832e-12\n",
            "Epoch: 2691/10000\n",
            "Training: Batch 1/2, Loss: 7.493825004978305e-12\n",
            "Training: Batch 2/2, Loss: 7.62444447854893e-12\n",
            "Validation: Batch 1/1, Loss: 7.130028772256436e-12\n",
            "Epoch: 2692/10000\n",
            "Training: Batch 1/2, Loss: 7.488791704812758e-12\n",
            "Training: Batch 2/2, Loss: 7.316346313512856e-12\n",
            "Validation: Batch 1/1, Loss: 7.012459623395584e-12\n",
            "Epoch: 2693/10000\n",
            "Training: Batch 1/2, Loss: 7.399077010805666e-12\n",
            "Training: Batch 2/2, Loss: 7.097285866647374e-12\n",
            "Validation: Batch 1/1, Loss: 6.762252216518938e-12\n",
            "Epoch: 2694/10000\n",
            "Training: Batch 1/2, Loss: 6.929705073654979e-12\n",
            "Training: Batch 2/2, Loss: 7.065093735741934e-12\n",
            "Validation: Batch 1/1, Loss: 6.645932935922527e-12\n",
            "Epoch: 2695/10000\n",
            "Training: Batch 1/2, Loss: 7.011872853179835e-12\n",
            "Training: Batch 2/2, Loss: 6.80105754699567e-12\n",
            "Validation: Batch 1/1, Loss: 6.594943341431403e-12\n",
            "Epoch: 2696/10000\n",
            "Training: Batch 1/2, Loss: 6.80869379973692e-12\n",
            "Training: Batch 2/2, Loss: 6.860008221198921e-12\n",
            "Validation: Batch 1/1, Loss: 6.548686939944481e-12\n",
            "Epoch: 2697/10000\n",
            "Training: Batch 1/2, Loss: 6.8408347562998184e-12\n",
            "Training: Batch 2/2, Loss: 6.7080624908955055e-12\n",
            "Validation: Batch 1/1, Loss: 6.443092153557428e-12\n",
            "Epoch: 2698/10000\n",
            "Training: Batch 1/2, Loss: 6.59383225104504e-12\n",
            "Training: Batch 2/2, Loss: 6.6866759525219255e-12\n",
            "Validation: Batch 1/1, Loss: 6.2914621429266404e-12\n",
            "Epoch: 2699/10000\n",
            "Training: Batch 1/2, Loss: 6.587188693812918e-12\n",
            "Training: Batch 2/2, Loss: 6.3868983886383734e-12\n",
            "Validation: Batch 1/1, Loss: 6.187382637495853e-12\n",
            "Epoch: 2700/10000\n",
            "Training: Batch 1/2, Loss: 6.521729337127802e-12\n",
            "Training: Batch 2/2, Loss: 6.301913418188532e-12\n",
            "Validation: Batch 1/1, Loss: 6.1389478570039735e-12\n",
            "Epoch: 2701/10000\n",
            "Training: Batch 1/2, Loss: 6.385385709767322e-12\n",
            "Training: Batch 2/2, Loss: 6.329801266469204e-12\n",
            "Validation: Batch 1/1, Loss: 6.0976631729992015e-12\n",
            "Epoch: 2702/10000\n",
            "Training: Batch 1/2, Loss: 6.468557894184768e-12\n",
            "Training: Batch 2/2, Loss: 6.118845014002616e-12\n",
            "Validation: Batch 1/1, Loss: 6.045028626971982e-12\n",
            "Epoch: 2703/10000\n",
            "Training: Batch 1/2, Loss: 6.350192507248442e-12\n",
            "Training: Batch 2/2, Loss: 6.1209249474503125e-12\n",
            "Validation: Batch 1/1, Loss: 6.007898172011306e-12\n",
            "Epoch: 2704/10000\n",
            "Training: Batch 1/2, Loss: 6.083186905592175e-12\n",
            "Training: Batch 2/2, Loss: 6.3005794158355055e-12\n",
            "Validation: Batch 1/1, Loss: 5.915551902491156e-12\n",
            "Epoch: 2705/10000\n",
            "Training: Batch 1/2, Loss: 6.1814329696541215e-12\n",
            "Training: Batch 2/2, Loss: 5.950523927766849e-12\n",
            "Validation: Batch 1/1, Loss: 5.7577944148623494e-12\n",
            "Epoch: 2706/10000\n",
            "Training: Batch 1/2, Loss: 6.108855175185335e-12\n",
            "Training: Batch 2/2, Loss: 5.712173789529373e-12\n",
            "Validation: Batch 1/1, Loss: 5.611514291431474e-12\n",
            "Epoch: 2707/10000\n",
            "Training: Batch 1/2, Loss: 5.9223394417717845e-12\n",
            "Training: Batch 2/2, Loss: 5.6025835012962766e-12\n",
            "Validation: Batch 1/1, Loss: 5.5195678756725375e-12\n",
            "Epoch: 2708/10000\n",
            "Training: Batch 1/2, Loss: 5.703252106692425e-12\n",
            "Training: Batch 2/2, Loss: 5.674976981395741e-12\n",
            "Validation: Batch 1/1, Loss: 5.473278948120441e-12\n",
            "Epoch: 2709/10000\n",
            "Training: Batch 1/2, Loss: 5.607582106992304e-12\n",
            "Training: Batch 2/2, Loss: 5.6562176817265275e-12\n",
            "Validation: Batch 1/1, Loss: 5.434036901008632e-12\n",
            "Epoch: 2710/10000\n",
            "Training: Batch 1/2, Loss: 5.5888232410039596e-12\n",
            "Training: Batch 2/2, Loss: 5.579434917551973e-12\n",
            "Validation: Batch 1/1, Loss: 5.403374796209004e-12\n",
            "Epoch: 2711/10000\n",
            "Training: Batch 1/2, Loss: 5.5594344232356985e-12\n",
            "Training: Batch 2/2, Loss: 5.530421606780855e-12\n",
            "Validation: Batch 1/1, Loss: 5.369536846405731e-12\n",
            "Epoch: 2712/10000\n",
            "Training: Batch 1/2, Loss: 5.4983279214326775e-12\n",
            "Training: Batch 2/2, Loss: 5.509897225974836e-12\n",
            "Validation: Batch 1/1, Loss: 5.345809298701321e-12\n",
            "Epoch: 2713/10000\n",
            "Training: Batch 1/2, Loss: 5.40862406944731e-12\n",
            "Training: Batch 2/2, Loss: 5.413861633302153e-12\n",
            "Validation: Batch 1/1, Loss: 5.137339772337546e-12\n",
            "Epoch: 2714/10000\n",
            "Training: Batch 1/2, Loss: 5.146752815599065e-12\n",
            "Training: Batch 2/2, Loss: 5.3770772556749336e-12\n",
            "Validation: Batch 1/1, Loss: 5.0593500743045805e-12\n",
            "Epoch: 2715/10000\n",
            "Training: Batch 1/2, Loss: 5.172774968781324e-12\n",
            "Training: Batch 2/2, Loss: 5.144893625713687e-12\n",
            "Validation: Batch 1/1, Loss: 4.964083397812624e-12\n",
            "Epoch: 2716/10000\n",
            "Training: Batch 1/2, Loss: 5.120852960421862e-12\n",
            "Training: Batch 2/2, Loss: 5.045560757394041e-12\n",
            "Validation: Batch 1/1, Loss: 4.927151568689947e-12\n",
            "Epoch: 2717/10000\n",
            "Training: Batch 1/2, Loss: 5.110151017617692e-12\n",
            "Training: Batch 2/2, Loss: 4.923159969971724e-12\n",
            "Validation: Batch 1/1, Loss: 4.842481884231864e-12\n",
            "Epoch: 2718/10000\n",
            "Training: Batch 1/2, Loss: 4.987419331692333e-12\n",
            "Training: Batch 2/2, Loss: 4.839635203007786e-12\n",
            "Validation: Batch 1/1, Loss: 4.748467678089563e-12\n",
            "Epoch: 2719/10000\n",
            "Training: Batch 1/2, Loss: 4.927005418237096e-12\n",
            "Training: Batch 2/2, Loss: 4.771801009884058e-12\n",
            "Validation: Batch 1/1, Loss: 4.729094719990723e-12\n",
            "Epoch: 2720/10000\n",
            "Training: Batch 1/2, Loss: 4.811416022543202e-12\n",
            "Training: Batch 2/2, Loss: 4.8263632673739565e-12\n",
            "Validation: Batch 1/1, Loss: 4.646203560776385e-12\n",
            "Epoch: 2721/10000\n",
            "Training: Batch 1/2, Loss: 4.803915945594817e-12\n",
            "Training: Batch 2/2, Loss: 4.625344812020371e-12\n",
            "Validation: Batch 1/1, Loss: 4.572270947272861e-12\n",
            "Epoch: 2722/10000\n",
            "Training: Batch 1/2, Loss: 4.6264359530867605e-12\n",
            "Training: Batch 2/2, Loss: 4.6997323567554705e-12\n",
            "Validation: Batch 1/1, Loss: 4.55727035969522e-12\n",
            "Epoch: 2723/10000\n",
            "Training: Batch 1/2, Loss: 4.800674181099085e-12\n",
            "Training: Batch 2/2, Loss: 4.4514075585311286e-12\n",
            "Validation: Batch 1/1, Loss: 4.5042884352919366e-12\n",
            "Epoch: 2724/10000\n",
            "Training: Batch 1/2, Loss: 4.618719903065616e-12\n",
            "Training: Batch 2/2, Loss: 4.575632407688435e-12\n",
            "Validation: Batch 1/1, Loss: 4.4980039658193416e-12\n",
            "Epoch: 2725/10000\n",
            "Training: Batch 1/2, Loss: 4.489865076950927e-12\n",
            "Training: Batch 2/2, Loss: 4.6841033655986575e-12\n",
            "Validation: Batch 1/1, Loss: 4.49078664879754e-12\n",
            "Epoch: 2726/10000\n",
            "Training: Batch 1/2, Loss: 4.636419720371876e-12\n",
            "Training: Batch 2/2, Loss: 4.472116253706471e-12\n",
            "Validation: Batch 1/1, Loss: 4.4366797562200855e-12\n",
            "Epoch: 2727/10000\n",
            "Training: Batch 1/2, Loss: 4.639609009482459e-12\n",
            "Training: Batch 2/2, Loss: 4.406549777846713e-12\n",
            "Validation: Batch 1/1, Loss: 4.437003282148355e-12\n",
            "Epoch: 2727, Early stopping criteria invoked, stopping training\n",
            "Minimum validation loss: 4.4366797562200855e-12, Current loss: 4.437003282148355e-12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f26f111a3b0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGeCAYAAABGlgGHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABGrUlEQVR4nO3deVhU9eLH8fewgwqoKKDivu+GSmhpJYVZZnUrMlPzmv0yW262mJXacstWq1u2ea96723RrLRFsxS1RbmaW2m5obhUgjvIosDM9/eHOTqKyuDAGZjP63nOE3PWz5wHmo9nzmIzxhhERERELOJndQARERHxbSojIiIiYimVEREREbGUyoiIiIhYSmVERERELKUyIiIiIpZSGRERERFLqYyIiIiIpVRGRERExFIBVgcoDYfDwR9//EGNGjWw2WxWxxEREZFSMMZw+PBh6tWrh5/fWY5/GDd9++235uqrrzaxsbEGMLNnzz7nMosXLzZdunQxQUFBplmzZmbatGlubXPXrl0G0KBBgwYNGjRUwmHXrl1n/Zx3+8hIXl4enTp14q9//SvXX3/9OefPyMjgqquu4s477+T9998nNTWV22+/ndjYWJKTk0u1zRo1agCwa9cuwsPD3Y0sIiIiFsjJySEuLs75OX4mNmPK/qA8m83G7Nmzufbaa884z5gxY5g7dy7r1693jrv55ps5dOgQ8+fPL9V2cnJyiIiIIDs7W2VERESkkijt53e5n8CalpZGUlKSy7jk5GTS0tLOuMzRo0fJyclxGURERKRqKvcykpmZSXR0tMu46OhocnJyKCgoKHGZiRMnEhER4Rzi4uLKO6aIiIhYxCsv7R07dizZ2dnOYdeuXVZHEhERkXJS7pf2xsTEkJWV5TIuKyuL8PBwQkNDS1wmODiY4ODg8o4mIuKTjDEUFxdjt9utjiKVnL+/PwEBAed9241yLyOJiYnMmzfPZdyCBQtITEws702LiMgpCgsL2b17N/n5+VZHkSoiLCyM2NhYgoKCyrwOt8tIbm4u6enpztcZGRmsXbuWWrVq0bBhQ8aOHcvvv//Of/7zHwDuvPNO3njjDR5++GH++te/smjRIj766CPmzp1b5tAiIuI+h8NBRkYG/v7+1KtXj6CgIN1IUsrMGENhYSF79+4lIyODFi1anP3GZmfhdhlZuXIll156qfP16NGjARg6dCjTp09n9+7d7Ny50zm9SZMmzJ07l/vvv5/XXnuNBg0a8M9//rPU9xgRERHPKCwsxOFwEBcXR1hYmNVxpAoIDQ0lMDCQHTt2UFhYSEhISJnW43YZueSSSzjbrUmmT59e4jJr1qxxd1MiIlIOyvqvV5GSeOL3Sb+RIiIiYimVERER8TmNGzfm1VdfLfX8S5YswWazcejQoXLLBMe+XYiMjCzXbXgjlREREfFaNpvtrMMTTzxRpvX++OOP3HHHHaWev0ePHuzevZuIiIgybU/Ortwv7RURESmr3bt3O3+eOXMm48ePZ9OmTc5x1atXd/5sjMFutxMQcO6Ptjp16riVIygoiJiYGLeWkdLz6SMjb7wBI0bAli1WJxERkZLExMQ4h4iICGw2m/P1xo0bqVGjBl999RXx8fEEBwfzww8/sHXrVgYMGEB0dDTVq1enW7duLFy40GW9p35NY7PZ+Oc//8l1111HWFgYLVq04PPPP3dOP/VrmuNfp3z99de0adOG6tWr07dvX5fyVFxczL333ktkZCS1a9dmzJgxDB069KwPly3JW2+9RbNmzQgKCqJVq1b897//dU4zxvDEE0/QsGFDgoODqVevHvfee69z+ptvvkmLFi0ICQkhOjqaG264wa1tVxSfLiPvvWf45z9h3U8Oq6OIiFQ8YyAvz5qh7A+MP80jjzzCc889x4YNG+jYsSO5ubn069eP1NRU1qxZQ9++fenfv7/LbSdK8uSTT3LTTTfx888/069fPwYNGsSBAwfOOH9+fj4vvfQS//3vf/nuu+/YuXMnDz74oHP6888/z/vvv8+0adNYunQpOTk5zJkzx633Nnv2bO677z4eeOAB1q9fz//93/8xbNgwFi9eDMAnn3zCK6+8wjvvvMOWLVuYM2cOHTp0AI7diuPee+/lqaeeYtOmTcyfP59evXq5tf0KYyqB7OxsA5js7GzPrdThMANrfGHAmBfv3u659YqIeKmCggLz66+/moKCgmMjcnONOVYLKn7IzXU7/7Rp00xERITz9eLFiw1g5syZc85l27VrZ15//XXn60aNGplXXnnF+Rowjz/+uPN1bm6uAcxXX33lsq2DBw86swAmPT3duczkyZNNdHS083V0dLR58cUXna+Li4tNw4YNzYABA0r9Hnv06GFGjBjhMs+NN95o+vXrZ4wx5uWXXzYtW7Y0hYWFp63rk08+MeHh4SYnJ+eM2/OE036vTlLaz2/fPTJis9G0/hEAtq08c/MVERHv1rVrV5fXubm5PPjgg7Rp04bIyEiqV6/Ohg0bznlkpGPHjs6fq1WrRnh4OHv27Dnj/GFhYTRr1sz5OjY21jl/dnY2WVlZdO/e3Tnd39+f+Ph4t97bhg0b6Nmzp8u4nj17smHDBgBuvPFGCgoKaNq0KSNGjGD27NkUFxcDcPnll9OoUSOaNm3K4MGDef/99732MQC+W0aApjEFAGzbW8PiJCIiFggLg9xcawYP3gG2WrVqLq8ffPBBZs+ezbPPPsv333/P2rVr6dChA4WFhWddT2BgoMtrm82Gw3Hmr/FLmt948Oun0oiLi2PTpk28+eabhIaGctddd9GrVy+KioqoUaMGq1ev5sMPPyQ2Npbx48fTqVOncr88uSx8u4zUzgZg28FIa4OIiFjBZoNq1awZyvGZOEuXLuW2227juuuuo0OHDsTExLB9+/Zy215JIiIiiI6O5scff3SOs9vtrF692q31tGnThqVLl7qMW7p0KW3btnW+Dg0NpX///vzjH/9gyZIlpKWlsW7dOgACAgJISkrihRde4Oeff2b79u0sWrToPN5Z+fDpS3ubRuUAsP1AOPYiB/6BPt3NRESqhBYtWvDpp5/Sv39/bDYb48aNO+sRjvJyzz33MHHiRJo3b07r1q15/fXXOXjwoFsPJ3zooYe46aab6NKlC0lJSXzxxRd8+umnzquDpk+fjt1uJyEhgbCwMN577z1CQ0Np1KgRX375Jdu2baNXr17UrFmTefPm4XA4aNWqVXm95TLz6U/f+jXzCaSQIoL4/fNVVscREREPmDRpEjVr1qRHjx7079+f5ORkLrjgggrPMWbMGAYOHMiQIUNITEykevXqJCcnu/UwuWuvvZbXXnuNl156iXbt2vHOO+8wbdo0LrnkEgAiIyOZMmUKPXv2pGPHjixcuJAvvviC2rVrExkZyaeffspll11GmzZtePvtt/nwww9p165dOb3jsrOZiv6CqwxycnKIiIggOzub8PBwz634iSdo+eQtbKEli19cySUPdj33MiIildSRI0fIyMigSZMmZX66qpSdw+GgTZs23HTTTTz99NNWx/GYs/1elfbz26ePjODnR1O2AbDtF+88w1hERCqnHTt2MGXKFDZv3sy6desYOXIkGRkZ3HLLLVZH8zq+XUYcjhNlZPp3cI7LvkRERErLz8+P6dOn061bN3r27Mm6detYuHAhbdq0sTqa1/HpE1hdyghN4aKLVEhERMQj4uLiTrsSRkrm20dG7HbXMrJrl8WBREREfI9vl5FTj4yIiIhIhfP5MtKEDAD2UpfDVD/HAiIiIuJpPl9GIsihNvsAyKCJxYFERER8j8+XEUBf1YiIiFjIt8tIdDSgMiIiImIl3y4j99wD4eEqIyIiVdwll1zC3/72N+frxo0b8+qrr551GZvNxpw5c857255az9k88cQTdO7cuVy3UZ58u4yEhsLHH58oI8G6EY2IiDfp378/ffv2LXHa999/j81m4+eff3Z7vT/++CN33HHH+cZzcaZCsHv3bq688kqPbquq8e0yAhAc7CwjGUfrWRxGRERONnz4cBYsWMBvv/122rRp06bRtWtXOnbs6PZ669SpQ1hYmCcinlNMTAzBwcEVsq3KSmWEE+eMZNAER9Zei9OIiMhxV199NXXq1GH69Oku43Nzc5k1axbDhw9n//79DBw4kPr16xMWFkaHDh348MMPz7reU7+m2bJlC7169SIkJIS2bduyYMGC05YZM2YMLVu2JCwsjKZNmzJu3DiKiooAmD59Ok8++SQ//fQTNpsNm83mzHzq1zTr1q3jsssuIzQ0lNq1a3PHHXeQm5vrnH7bbbdx7bXX8tJLLxEbG0vt2rUZNWqUc1ul4XA4eOqpp2jQoAHBwcF07tyZ+fPnO6cXFhZy9913ExsbS0hICI0aNWLixIkAGGN44oknaNiwIcHBwdSrV49777231NsuC9++HTyAMTTgNwIo4igh7E5bR/1r61idSkSk3BkD+RY9IzQsDGy2c88XEBDAkCFDmD59Oo899hi2PxeaNWsWdrudgQMHkpubS3x8PGPGjCE8PJy5c+cyePBgmjVrRvfu3c+5DYfDwfXXX090dDTLly8nOzvb5fyS42rUqMH06dOpV68e69atY8SIEdSoUYOHH36YlJQU1q9fz/z581m4cCEAERERp60jLy+P5ORkEhMT+fHHH9mzZw+33347d999t0vhWrx4MbGxsSxevJj09HRSUlLo3LkzI0aMOPdOA1577TVefvll3nnnHbp06cLUqVO55ppr+OWXX2jRogX/+Mc/+Pzzz/noo49o2LAhu3btYtefdyH/5JNPeOWVV5gxYwbt2rUjMzOTn376qVTbLTNTCWRnZxvAZGdne37lS5YYA6YZWwwY891b6z2/DRERL1BQUGB+/fVXU1BQYIwxJjfXmGOVpOKH3NzS596wYYMBzOLFi53jLr74YnPrrbeecZmrrrrKPPDAA87XvXv3Nvfdd5/zdaNGjcwrr7xijDHm66+/NgEBAeb33393Tv/qq68MYGbPnn3Gbbz44osmPj7e+XrChAmmU6dOp8138nreffddU7NmTZN70g6YO3eu8fPzM5mZmcYYY4YOHWoaNWpkiouLnfPceOONJiUl5YxZTt12vXr1zDPPPOMyT7du3cxdd91ljDHmnnvuMZdddplxOBynrevll182LVu2NIWFhWfc3slO/b06WWk/v/U1TfPmwEmX985aZWUaERE5RevWrenRowdTp04FID09ne+//57hw4cDYLfbefrpp+nQoQO1atWievXqfP311+ws5YNPN2zYQFxcHPXqnThvMDEx8bT5Zs6cSc+ePYmJiaF69eo8/vjjpd7Gydvq1KkT1apVc47r2bMnDoeDTZs2Oce1a9cOf39/5+vY2Fj27NlTqm3k5OTwxx9/0LNnT5fxPXv2ZMOGDcCxr4LWrl1Lq1atuPfee/nmm2+c8914440UFBTQtGlTRowYwezZsykuLnbrfbpLZaR+fZg48UQZWZRhcSARkYoRFga5udYM7p47Onz4cD755BMOHz7MtGnTaNasGb179wbgxRdf5LXXXmPMmDEsXryYtWvXkpycTGFhocf2VVpaGoMGDaJfv358+eWXrFmzhscee8yj2zhZYGCgy2ubzYbjzxt1esIFF1xARkYGTz/9NAUFBdx0003ccMMNwLGnDW/atIk333yT0NBQ7rrrLnr16uXWOSvu0jkjAAkJNOUrQPcaERHfYbPBSf9A92o33XQT9913Hx988AH/+c9/GDlypPP8kaVLlzJgwABuvfVW4Ng5IJs3b6Zt27alWnebNm3YtWsXu3fvJjY2FoD//e9/LvMsW7aMRo0a8dhjjznH7dixw2WeoKAg7Hb7Obc1ffp08vLynEdHli5dip+fH61atSpV3nMJDw+nXr16LF261FnYjm/n5HNowsPDSUlJISUlhRtuuIG+ffty4MABatWqRWhoKP3796d///6MGjWK1q1bs27dOi644AKPZDyVygiAMc4H5qmMiIh4n+rVq5OSksLYsWPJycnhtttuc05r0aIFH3/8McuWLaNmzZpMmjSJrKysUpeRpKQkWrZsydChQ3nxxRfJyclxKR3Ht7Fz505mzJhBt27dmDt3LrNnz3aZp3HjxmRkZLB27VoaNGhAjRo1Trukd9CgQUyYMIGhQ4fyxBNPsHfvXu655x4GDx5M9J93BfeEhx56iAkTJtCsWTM6d+7MtGnTWLt2Le+//z4AkyZNIjY2li5duuDn58esWbOIiYkhMjKS6dOnY7fbSUhIICwsjPfee4/Q0FAaNWrksXyn0tc0AHFxrndh9eChMBER8Yzhw4dz8OBBkpOTXc7vePzxx7ngggtITk7mkksuISYmhmuvvbbU6/Xz82P27NkUFBTQvXt3br/9dp555hmXea655hruv/9+7r77bjp37syyZcsYN26cyzx/+ctf6Nu3L5deeil16tQp8fLisLAwvv76aw4cOEC3bt244YYb6NOnD2+88YZ7O+Mc7r33XkaPHs0DDzxAhw4dmD9/Pp9//jktWrQAjl0Z9MILL9C1a1e6devG9u3bmTdvHn5+fkRGRjJlyhR69uxJx44dWbhwIV988QW1a9f2aMaT2YwxptzW7iE5OTlERESQnZ1NeHh4uWzjoK0mtTgIQN7BQsIig8plOyIiVjly5AgZGRk0adKEkJAQq+NIFXG236vSfn7ryMifanKIyD/LSMaW8j1rWERERE5QGTmJ86uaqYstTiIiIuI7VEZO4iwjq7MtTiIiIuI7VEZO4iwjh3U7eBERkYqiMnISZxkpirM4iYiIiO9QGTmJjoyIiC+oBBdRSiXiid8nlZGTOMtIVhjGrnuNiEjVcvwW4/lWPapXqqTjv0+n3sLeHboD63Hz5tGw3zX4YecIoWSu2E5sYmOrU4mIeIy/vz+RkZHOB66FhYU5b6ku4i5jDPn5+ezZs4fIyEiXB/u5S2XkuCuvJLBuLRru2cl2mrDtuY+I/exhq1OJiHhUTEwMQKmfACtyLpGRkc7fq7JSGTlZjx40nbON7TRh6+fr6XnuJUREKhWbzUZsbCx169Yt16ewim8IDAw8ryMix6mMnOydd2g2Zw6L6MO2xn2sTiMiUm78/f098iEi4gk6gfVkdevSvNWxfpYeEW9xGBEREd+gMnKK5g0LAUjfH2ltEBERER+hMnKK5jX3A5C+L9LaICIiIj5CZeQUzVZ9BMD+I9U5eNDiMCIiIj5AZeQU1bb+TCx/ALB1q8VhREREfIDKSAmakw5A+s+6S6GIiEh5UxkpQTOOHRJJX5BhcRIREZGqT2WkBM4jIzN+tDiJiIhI1acyUgJnGaG5xUlERESqPpWRU73xhsqIiIhIBVIZOdWoUc5zRrKI4fBhi/OIiIhUcSojJYgkmyj2Arq8V0REpLypjJyB86uadIuDiIiIVHEqI2fgLCP/mGdxEhERkaqtTGVk8uTJNG7cmJCQEBISElixYsVZ53/11Vdp1aoVoaGhxMXFcf/993PkyJEyBa4ozjLy/R8WJxEREana3C4jM2fOZPTo0UyYMIHVq1fTqVMnkpOT2bNnT4nzf/DBBzzyyCNMmDCBDRs28K9//YuZM2fy6KOPnnf48nS8jGylmcVJREREqja3y8ikSZMYMWIEw4YNo23btrz99tuEhYUxderUEudftmwZPXv25JZbbqFx48ZcccUVDBw48JxHUyxls7le3ltUZHEgERGRqsutMlJYWMiqVatISko6sQI/P5KSkkhLSytxmR49erBq1Spn+di2bRvz5s2jX79+Z9zO0aNHycnJcRkq1Ell5DfiKHjyhYrdvoiIiA9xq4zs27cPu91OdHS0y/jo6GgyMzNLXOaWW27hqaee4qKLLiIwMJBmzZpxySWXnPVrmokTJxIREeEc4uLi3InpEbU4QCQHAdg2/bsK376IiIivKPeraZYsWcKzzz7Lm2++yerVq/n000+ZO3cuTz/99BmXGTt2LNnZ2c5h165d5R3Tlc2GjZNOYi2oX7HbFxER8SEB7swcFRWFv78/WVlZLuOzsrKIiYkpcZlx48YxePBgbr/9dgA6dOhAXl4ed9xxB4899hh+fqf3oeDgYIKDg92J5lmTJ8Odd9KcdFbSjfTCij8yIyIi4ivcOjISFBREfHw8qampznEOh4PU1FQSExNLXCY/P/+0wuHv7w+AMcbdvBXj//4P6tQ5cWSksKHFgURERKout46MAIwePZqhQ4fStWtXunfvzquvvkpeXh7Dhg0DYMiQIdSvX5+JEycC0L9/fyZNmkSXLl1ISEggPT2dcePG0b9/f2cp8Urr1tE85mEA0osbW5tFRESkCnO7jKSkpLB3717Gjx9PZmYmnTt3Zv78+c6TWnfu3OlyJOTxxx/HZrPx+OOP8/vvv1OnTh369+/PM88847l3UR6io08cGXE0tTiMiIhI1WUzXvtdyQk5OTlERESQnZ1NeHh4hW030xZDLJn4YSf/iD9WnsYiIiJS2ZT281vPpjmLaLKoRi4O/Nm+zWF1HBERkSpJZeQsbA0a0IItAGx+8F2L04iIiFRNKiNns3s3LdkMwJZ5my0OIyIiUjWpjJxNRISzjGyilcVhREREqiaVkbOZPdtZRjb7tbY4jIiISNWkMnI2vXrRKvzYM3c2+7exOIyIiEjVpDJyDi3efRCAP4rqkptrcRgREZEqSGXkHGpGGOqwB4AtWywOIyIiUgWpjJyLn9+Jk1g3WZxFRESkClIZORd//xMnserqXhEREY9TGTkXf39aceyQyOb3VlgcRkREpOpRGTmXwsITR0Z0zoiIiIjHqYycS1HRiTJCS7z/sYIiIiKVi8rIuRQV0Yyt2HCQTSR79lgdSEREpGpRGTmXoiJCOEojdgA6iVVERMTTVEbOpagI4MRJrJv0PY2IiIgnqYycS+/eACfOG1EZERER8SiVkXOpXx84UUY2bXRYmUZERKTKURkpjRUrThwZ2agjIyIiIp6kMlIa3brRslEhAOnpBvsPaRYHEhERqTpURkqpYfE2gjlCEUHsuHiQ1XFERESqDJWRUvK7/lpacOwWrJtpaXEaERGRqkNlpLQOHDhxEiutLA4jIiJSdaiMlNaVV7rcFl5EREQ8Q2WktC6//MSNz1RGREREPEZlpLTCw12/ptET80RERDxCZaS0goNpzUYAdtGQ3L0FFgcSERGpGlRGSstmoxYHqUsWAJt+tVscSEREpGpQGXHT8aMjGz5YY3ESERGRqkFlxE1t2ADAhinfW5xERESkalAZcUd4+IkyQhuLw4iIiFQNKiPu2LHDtYwcOWJxIBERkcpPZcQdkZHOMpJOc4r+/YHFgURERCo/lRE3NeA3qpFLMYFs3WazOo6IiEilpzLiJhsnXVGTHmhtGBERkSpAZaQMnOeNbLA4iIiISBWgMlIGJ8qIbgkvIiJyvlRGykCX94qIiHiOykgZHC8jG2mNw2FxGBERkUpOZaQMmrGVAIrIozq/T3jX6jgiIiKVmspIGQRSTAu2ALDh7x/r5mciIiLnQWWkjJyX99IGioosTiMiIlJ5qYyUkctJrCojIiIiZaYy4q60NLj7btcyUlhocSgREZHKS2XEXRdeCK+/rjIiIiLiISojZdR6QGsA9lKX/VnFFqcRERGpvFRGyqjaTVfRkB0AbNykB+aJiIiUlcpIWaWkOL+q+XWzv8VhREREKi+VkbLy96ddjV0A/LI5yOIwIiIilZfKyHloH7oVgPVz0sFutziNiIhI5aQych7a70kFYP3R5vD11xanERERqZxURs5DW34FIIsY9u01FqcRERGpnFRGzkM18mnCNgB+yYqyOI2IiEjlpDJyntqzHoD128IsTiIiIlI5qYycJ2cZ2RhgcRIREZHKSWXkfHzzzYkykh5scRgREZHKSWXkfFx+Oe1GXQocO2fE6BxWERERt6mMnKdWLRz4U8zB4nB23/qQ1XFEREQqnTKVkcmTJ9O4cWNCQkJISEhgxYoVZ53/0KFDjBo1itjYWIKDg2nZsiXz5s0rU2BvExIRTAu2ALD+g5/A4bA4kYiISOXidhmZOXMmo0ePZsKECaxevZpOnTqRnJzMnj17Spy/sLCQyy+/nO3bt/Pxxx+zadMmpkyZQv369c87vFcw5sR5I7SHYj3BV0RExB1ul5FJkyYxYsQIhg0bRtu2bXn77bcJCwtj6tSpJc4/depUDhw4wJw5c+jZsyeNGzemd+/edOrU6bzDe4WCAtrxC6AyIiIiUhZulZHCwkJWrVpFUlLSiRX4+ZGUlERaWlqJy3z++eckJiYyatQooqOjad++Pc8++yz2qvIsl/x855GRX2gHRUUWBxIREalc3Lo5xr59+7Db7URHR7uMj46OZuPGjSUus23bNhYtWsSgQYOYN28e6enp3HXXXRQVFTFhwoQSlzl69ChHjx51vs7JyXEnZsVq25b2/BM4VkYchUd1VrCIiIgbyv1z0+FwULduXd59913i4+NJSUnhscce4+233z7jMhMnTiQiIsI5xMXFlXfMsrvySpq//RBBHCWP6uzI0AmsIiIi7nCrjERFReHv709WVpbL+KysLGJiYkpcJjY2lpYtW+Lv7+8c16ZNGzIzMyksLCxxmbFjx5Kdne0cdu3a5U7MimWzEfB/w2nNsSND678/aHEgERGRysWtMhIUFER8fDypqanOcQ6Hg9TUVBITE0tcpmfPnqSnp+M46ZLXzZs3ExsbS1BQUInLBAcHEx4e7jJ4O+d5Iy/MtTiJiIhI5eL21zSjR49mypQp/Pvf/2bDhg2MHDmSvLw8hg0bBsCQIUMYO3asc/6RI0dy4MAB7rvvPjZv3szcuXN59tlnGTVqlOfehRc4XkbW7alrcRIREZHKxe2nu6WkpLB3717Gjx9PZmYmnTt3Zv78+c6TWnfu3Imf34mOExcXx9dff839999Px44dqV+/Pvfddx9jxozx3LvwAh1YB8DPId0tTiIiIlK52Izx/ieq5OTkEBERQXZ2ttd+ZbOr9600/O49AigiN9dGcDU9xVdERHxbaT+/dRWqhzS4ujM1OUAxgfz6YMk3gBMREZHTqYx4iC0slE78BMBPU5ZbnEZERKTyUBnxlJCQE2XEVJFb3YuIiFQAlRFPyc8/qYx0tDiMiIhI5aEy4ikHDpxURjrg/acFi4iIeAeVEQ9qy6/4U8wBavP771anERERqRxURjwlKYkQjjpvC//T6iryVGIREZFypjLiKT17wjvvnPiqZsYGiwOJiIhUDiojnnTFFSfKyBo9vVdERKQ0VEY8qXHjE2VkY8kPARQRERFXKiMe1qnzsV26hRbk5+roiIiIyLmojHhYTI086pKFA3/WrzxidRwRERGvpzLiaUFBJ76q+WKnxWFERES8n8qIpxlzoozMybA4jIiIiPdTGfG0Q4foyM8A/GTrbG0WERGRSkBlxNPuuosurAFg7Y5IHDqHVURE5KxURjztr3+ldQsHoeSTWxzKlk9+tjqRiIiIV1MZ8TSbjYBa4c7zRlZ//pvFgURERLybykh5GDKEeFYBsCqrgcVhREREvJvKSHkYOZILWA3A6oxIa7OIiIh4OZWR8mCzEf+XJgCsTg/HvPe+xYFERES8l8pIOWm7/iOCOUI2kWwbPN7qOCIiIl5LZaScBG5a77zfyCriLU4jIiLivVRGystTT504b8Svm8VhREREvJfKSHl59FHiOx+749mqWkkWhxEREfFeKiPlxd+fC46fxLqvIebIUYsDiYiIeCeVkXLUvmk+gRRygNrseOAfVscRERHxSioj5Sg4zJ/2rAdg9YzNFqcRERHxTioj5al1a+edWFcfaGRxGBEREe+kMlKeWrd2XlGjy3tFRERKpjJSzuLrHntQ3qrgHhhjcRgREREvpDJSzjo+k0Ighew9GsHO9EKr44iIiHgdlZFyFtKqEZ34CYAVH++0OI2IiIj3URkpbz160J0VACxPc1gcRkRExPuojJQ3f3+6JwYAsOKLTCgqsjiQiIiId1EZqQDdOx27++oq4im++VaL04iIiHgXlZEK0KqNHzXIIZ9q/PrpBqvjiIiIeBWVkQrgF1efbvwIwHISLE4jIiLiXVRGKkK/fiSwHIAVdLc4jIiIiHdRGakIwcF0b7Yf+LOM/PCDxYFERES8h8pIBen+9wEArKc9eYPvtDiNiIiI91AZqSD1WodTn99w4M+qQ82sjiMiIuI1VEYqSvv2zpufrTjUwuIwIiIi3kNlpKIEBLiexLpggcWBREREvIPKSAXqPvoi4M8y8t13FqcRERHxDiojFSg+qRY2HOygMZmHq1kdR0RExCuojFSg8NhqtGc9AGnTN1qcRkRExDuojFSkhg3pwTIAlma3tziMiIiId1AZqUi1atGz1rEjIsvoYXEYERER76AyUsF6/HckcOwJvkfWbbE4jYiIiPVURipY074tqUsWhQSz6tFPrI4jIiJiOZWRCmbzs9EzZhsAy7bXsziNiIiI9VRGLNAjKQyAZb81tDiJiIiI9VRGLNCjSwEASw+1xRwttDiNiIiItVRGLBAft4cgjrKXumxdtMPqOCIiIpZSGbFAcHx7urISgGXf2y1OIyIiYi2VESs0bUrPoGNlZOlyf4vDiIiIWEtlxCI9etoAWPZtERhjcRoRERHrqIxYJLFfTQB+sbfm0DszLU4jIiJinTKVkcmTJ9O4cWNCQkJISEhgxYoVpVpuxowZ2Gw2rr322rJstkqJvvlSmrMFgx/Lvsq2Oo6IiIhl3C4jM2fOZPTo0UyYMIHVq1fTqVMnkpOT2bNnz1mX2759Ow8++CAXX3xxmcNWKfXr04vvAPhufzuLw4iIiFjH7TIyadIkRowYwbBhw2jbti1vv/02YWFhTJ069YzL2O12Bg0axJNPPknTpk3PK3CVYbPR64pQAL5dGmBxGBEREeu4VUYKCwtZtWoVSUlJJ1bg50dSUhJpaWlnXO6pp56ibt26DB8+vFTbOXr0KDk5OS5DVdR713sArCSevBW/WJxGRETEGm6VkX379mG324mOjnYZHx0dTWZmZonL/PDDD/zrX/9iypQppd7OxIkTiYiIcA5xcXHuxKw0GvvtpCE7KCaQtIT7rI4jIiJiiXK9mubw4cMMHjyYKVOmEBUVVerlxo4dS3Z2tnPYtWtXOaa00BVXOM8b+ZbeFocRERGxhlsnK0RFReHv709WVpbL+KysLGJiYk6bf+vWrWzfvp3+/fs7xzkcjmMbDghg06ZNNGvW7LTlgoODCQ4Odida5fTUU/R+5W+8x2CVERER8VluHRkJCgoiPj6e1NRU5ziHw0FqaiqJiYmnzd+6dWvWrVvH2rVrncM111zDpZdeytq1a6vs1y+lVr06vUd3BWA5CRw5YnEeERERC7h9Gcfo0aMZOnQoXbt2pXv37rz66qvk5eUxbNgwAIYMGUL9+vWZOHEiISEhtG/f3mX5yMhIgNPG+6rmlzUkZtJuMoll+X820fuOVlZHEhERqVBul5GUlBT27t3L+PHjyczMpHPnzsyfP995UuvOnTvx89ONXUvL1jCO3nzLTG7m20fmqYyIiIjPsRnj/Q9GycnJISIiguzsbMLDw62O43FvRY7lruyJ9GEhCzOaQ+PGVkcSERE5b6X9/NYhDC/Q6/54AJbRg8Lvl1ucRkREpGKpjHiBtokRRLGXAsJYOeQ1q+OIiIhUKJURL2C7PInefAvAYi61OI2IiEjFUhnxBjYbl7EIgFT6gN1ucSAREZGKozLiJZJYCMBSepK/Y6/FaURERCqOyoiXaDHsYhqwi0KCWTqs9M/xERERqexURryEbeq/nEdHFn4XpK9qRETEZ6iMeJE+TTKAP88bOXDA4jQiIiIVQ2XEi/S5phoAq7mAAz//ZnEaERGRiqEy4kVix91OW37B4Mfiu2ZZHUdERKRCqIx4k9q1SeLYE5EXbvbxJxqLiIjPUBnxMn3+PIk1lT4WJxEREakYKiNepvfN9fCnmC20ZOd2h9VxREREyp3KiJeJeOUJuvEjAAvHLLA4jYiISPlTGfE2MTFczrES8s1HBy0OIyIiUv5URrxQX+YD8A1XULzvkLVhREREypnKiBfqPqglNTnAQWqxYoRuDS8iIlWbyogXCnjhWedXNfPnFFicRkREpHypjHij2Fiu5CsAvuJKi8OIiIiUL5URb2SzkXx7QwBW0o09eyzOIyIiUo5URrxU7CsP05k1AHw985C1YURERMqRyoi3ql6dK+usAuCrVzZaHEZERKT8qIx4sSujjt387JuM5ti377I4jYiISPlQGfFiif4riOAQ+4liZa/RVscREREpFyojXiygeWOS/nxw3le72lmcRkREpHyojHiz11+nH/MA+NL/WmuziIiIlBOVEW/WoAFX/TsFGw5W2Tvz26w0qxOJiIh4nMqIl4tuF8WF/A+AL276j8VpREREPE9lxNs1asQAPgPgc66xOIyIiIjnqYx4u6gorpnYA4BFXMbhj76yOJCIiIhnqYxUAq0fvJoWbKaQYL5O+ZfVcURERDxKZaQSsAX4cw2fA/qqRkREqh6VkUpiwGW5AMzlKoqz9lucRkRExHNURiqJxGeupjb7OEBtll73ktVxREREPEZlpJIIuLArV/MlAJ+l1bE4jYiIiOeojFQi18SuBGA212H2H7A4jYiIiGeojFQifQfWJIw8ttOEVTdMtDqOiIiIR6iMVCJhNfy5irkAzFreEIqLLU4kIiJy/lRGKpN77uFGZgEwq+AqzAMPWhxIRETk/KmMVCa1a9PvtymEkk8GTVn9j++tTiQiInLeVEYqmWr1I+nHPABmcaPFaURERM6fykgldGPNVOBYGTHG4jAiIiLnSWWkErrqszsIoYBtNGPNR1usjiMiInJeVEYqoeqtG5z4qubmjy1OIyIicn5URiqjOnW4MfF3AGaSgjlw0OJAIiIiZacyUkn1/+YeqpFLBk1JG/Cc1XFERETKTGWkkqpW3cb1fArAez80sjiNiIhI2amMVGK3xn0HHPuqpjBfd2MVEZHKSWWkErvsq4eIYTcHqM38++ZZHUdERKRMVEYqsYB2rRjIhwC8988jcOSIxYlERETcpzJSyd06pj4AX9Cf7EeftziNiIiI+1RGKrku4/rThl85QiifvrLd6jgiIiJuUxmp5GzVwriV9wD4N0PBbrc4kYiIiHtURqqAWz++DhsOvuUS0p/9yOo4IiIiblEZqQIa/qUbyXwNwNTxGRanERERcY/KSBVx++U7AZjGMIr36vbwIiJSeaiMVBH9p11PHfaQSSzz7tY9R0REpPJQGakigurXYeiFmwD41zcNLE4jIiJSeiojVcjwJxoCMPdQT/54b5HFaUREREqnTGVk8uTJNG7cmJCQEBISElixYsUZ550yZQoXX3wxNWvWpGbNmiQlJZ11fim71pfH0ZMfsBPA9MELweGwOpKIiMg5uV1GZs6cyejRo5kwYQKrV6+mU6dOJCcns2fPnhLnX7JkCQMHDmTx4sWkpaURFxfHFVdcwe+//37e4eUUfn7czr8AmMII7Fu2WRxIRETk3GzGGOPOAgkJCXTr1o033ngDAIfDQVxcHPfccw+PPPLIOZe32+3UrFmTN954gyFDhpRqmzk5OURERJCdnU14eLg7cX1OQb6hQbUDHKA2cx78gQEvXmR1JBER8VGl/fx268hIYWEhq1atIikp6cQK/PxISkoiLS2tVOvIz8+nqKiIWrVqnXGeo0ePkpOT4zJI6YSG2bi90UIAXn/pCBQUWJxIRETk7NwqI/v27cNutxMdHe0yPjo6mszMzFKtY8yYMdSrV8+l0Jxq4sSJREREOIe4uDh3Yvq8u4YfxQ87qSTx63WPWR1HRETkrCr0aprnnnuOGTNmMHv2bEJCQs4439ixY8nOznYOu3btqsCUlV+jxwdzDZ8D8MbXzSEvz+JEIiIiZ+ZWGYmKisLf35+srCyX8VlZWcTExJx12ZdeeonnnnuOb775ho4dO5513uDgYMLDw10GcYPNxj2PRQDwH4aQ/e5MiwOJiIicmVtlJCgoiPj4eFJTU53jHA4HqampJCYmnnG5F154gaeffpr58+fTtWvXsqeVUrv04W60Yz15VGfa64etjiMiInJGbn9NM3r0aKZMmcK///1vNmzYwMiRI8nLy2PYsGEADBkyhLFjxzrnf/755xk3bhxTp06lcePGZGZmkpmZSW5urufehZzGFl6Du+vPAeC1jGsonv2FtYFERETOwO0ykpKSwksvvcT48ePp3Lkza9euZf78+c6TWnfu3Mnu3bud87/11lsUFhZyww03EBsb6xxeeuklz70LKdGQtaOJYi/bacKsR9dYHUdERKREbt9nxAq6z0jZPf2XNYz/tAudWMua3JbYqoVZHUlERHxEudxnRCqfUW+0JYw8fqIzC179xeo4IiIip1EZqeJqxQYzovliAJ5/PEfPqxEREa+jMuIDRk8IJ4AiFtGHH19YbHUcERERFyojPqDhoIsZyIcA/H1sLhw4YHEiERGRE1RGfIHNxqNTmuKHnc8ZwKrHPrE6kYiIiJPKiI9oPSieQbwPwISP2oH3X0QlIiI+QmXEV4SGMu6t+vhTzNwDPVg+WreIFxER76Ay4kNa3NKNwfwXgAmvRsKRI9YGEhERQWXEt4SHM+7vIfhTzNf05YendWWNiIhYT2XExzR9bCB/DfkAgIeejcQU2y1OJCIivk5lxAc9+cBhwsjjfyTy8eA5VscREREfpzLig2Kf+D8e5gUAHpnRmaMbtlmcSEREfJnKiC8KCODBTXcQyx9soxlvtn3d6kQiIuLDVEZ8VLWW9XmacQA8zTj2bz1kbSAREfFZKiM+7LYtj9ORnzhILca21F1ZRUTEGiojPsy/eRMmd/4nAFMcw/nf3xdanEhERHyRyoiPu2jWfdzGNADuGleL4mKLA4mIiM9RGfF1zZvzwugsanKANVzAW3essTqRiIj4GJURoc7Lj/DsxV8B8Ni0Zuxcvc/iRCIi4ktURgSAEbOSSWQZhwnn9vjVeqiviIhUGJURAcA/Oopp135OCAUs4Ar+FTXG6kgiIuIjVEbEqdXUMfydxwEYfeAxdi7cbHEiERHxBSojckLNmvwt9Rp6sJTDhDPs8l04flhmdSoREaniVEbEhf9lvZnW+CnCyGMRfXj+plVWRxIRkSpOZURO03LrV7zB3QCM2z2SZRO/tTiRiIhUZSojcjo/P2479Bq38AF2Ahj4aGMObt5rdSoREamiVEakRLaIcN6aWYtmpLOTRgxpvRyHw+pUIiJSFamMyBmF39SXmR2eIYQCvjRXM97/76iRiIiIp6mMyFnFr3qXKbXHAvAMjzNroJ7uKyIinqUyImcXGMitu1/kAV4C4LaP+rH2I91/REREPEdlRM4tMJDnFnbjcr4hn2pclVKNHd9utzqViIhUESojUioBfXoz8wMH7VjPH9Sn7yUF7F+9w+pYIiJSBaiMSKnVHNiX+e0epAG72Egb+sf/Tv6SFVbHEhGRSk5lRNzSYPknzG/7AJEcJI0eXHfpQY5kZVsdS0REKjGVEXFPtWq0++Ujvqw5hGrk8g3JXFd/OUfydcmviIiUjcqIlEnP/Z8z19afMPKYb7+C65us5kjmIatjiYhIJaQyImVjs9Hbvoi5t3xAKPl8tacrV8WuIufX36xOJiIilYzKiJSdzcYl/x3OvMS/U53DLKIPvdvtJXNrntXJRESkElEZkfPj58clS5/h2+a3U5cs1tKFHs2z2Pz4f6xOJiIilYTKiJw/m40Ltsxk2WPzaEY6GTSl+zPX8NUds61OJiIilYDKiHhMs78PY+mCAhJZRjaRXDVlAM8FT8A4jNXRRETEi6mMiEdFJ3Vg8fxCRvAuBj/GFj7JTXUWc+i3XKujiYiIl1IZEY8LTr6Ed4/extv8HwEU8fGBy+gUt58fXv3R6mgiIuKFVEakfAQF8X/mHX4Y+QFN2cpOGtH7/gsYHzuFwr26Y6uIiJygMiLlKuHNoaydvZ2hTMeBP09njiC+3h+kvf2T1dFERMRLqIxIuatxbR+m77+GmdxEHfawvrgNPUd2YJRtMtlzf7A6noiIWExlRCpGrVrcZD5iw1c7uI1pGPx4k1G0uLolb/X9jOIiXXEjIuKrVEakQtXu241p2y8j1ZZESzaxl7rc9fUAOgRt5Mub38Ook4iI+ByVEal4jRpxmWMh6/fF8jp3U5t9bKQN/WfeyoVhP/HlpM0qJSIiPkRlRCwTWDucu80bbH3qAx7meULJZ8WRTvR/oCUX+K1hVudnKD5SbHVMEREpZzZjvP/foDk5OURERJCdnU14eLjVcaQ8GEPWvFVMuieDyRlXkkd1ABqwizt5mxFpw6l7YVOLQ4qIiDtK+/mtIyPiHWw2oq/qyvPbbmTHq3MYz5PUJYvfiONxniEusT631PiC+a9soFgHS0REqhQdGRGvdfSnjczq/Hfe4G6Wc6FzfAy7uYUPGHR7GF3euRObn83ClCIicial/fxWGRHvt38/K+/5N/9e1oIPdySynyjnpIbsYACfce0d0Vz84jUEhodaGFRERE6mMiJVUuHipcxPmcZ/9vZlHv0oIMw5LZKD9CGVPkMa0Of2JrS4KBqbDpqIiFhGZUSqvPy0n1jYYxyfMYDPuYZ91HGZ3oBdXBaxmsQrI0noGUCHv3YjICzIorQiIr5HZUR8iv1wPiv+9gGpU7eTGtSPZYXxFBLsMk8o+cQHriOh6Hs6X1abDqMvp3Wf+gSH6PCJiEh5UBkRn1awbTdLb5vCtxkNWX6gBSvy25FN5Gnz+VNMy+CddIjZS/vWxbToUp1mF0TQrHttajWqUfHBRUSqkHItI5MnT+bFF18kMzOTTp068frrr9O9e/czzj9r1izGjRvH9u3badGiBc8//zz9+vUr9fZURuR8OQ7lsHnBDpZPXMSKNQH8TEfW0aHEgnJcTb9DNAv9g2a1DtKoXjH14/yo3ziABm0jqN+lLjFtaxEQpKvjRUTOpNzKyMyZMxkyZAhvv/02CQkJvPrqq8yaNYtNmzZRt27d0+ZftmwZvXr1YuLEiVx99dV88MEHPP/886xevZr27dt79M2IuMMcLeT31I2s+/YA65dm88v2MLbui2BrYRy7Tew5l/fDTrTfXmKCD1InLI+oGkeJirQTFQVR0f5E1QuiTr1AatULIaJBDcIbhFMjOoyAYP8KeHciItYrtzKSkJBAt27deOONNwBwOBzExcVxzz338Mgjj5w2f0pKCnl5eXz55ZfOcRdeeCGdO3fm7bff9uibEfGU/Iwstv3wB+lrDrN1UzG7dsHv+4P5/VA1fj9Smz8cMRQTWKZ1h5FHuO0w4f55hAfkEx5YQHjQUaoHFxEa7CA0xEFoCISFQWiYjdBqfseGGgHHhvBAQmsEEBLmR2D1YILCAggM9iOwWhCBoQEEVgsiqFoggWGBx14H++EfoPNiRKTilfbzO8CdlRYWFrJq1SrGjh3rHOfn50dSUhJpaWklLpOWlsbo0aNdxiUnJzNnzpwzbufo0aMcPXrU+TonJ8edmCLnLaxJNO2bRNN+cMnTHQVH2bNxN7//mk3Wtjz2/XaEfbuL2LfHwb6D/uzLDmBffhj7jtZgf3EEOY7qHOHYPVDyqUa+qUZmMVAMHCn/9+OHnUCKjg22YgIpJsBWjJ/N4I8DP5sDf5sDP8yJn23G+d+Tf/b/87WfzeDv5zhlmgMbBrBhsxlsADawATYbx6ad9JqTxru8trmu48Tr06e5rL+kaadsz9OOvd9Sz+zJ2dxybN94dgtuvffTF3Zn9Hlxfe8VscUy8IIYf3u9OY0vamDJtt0qI/v27cNutxMdHe0yPjo6mo0bN5a4TGZmZonzZ2ZmnnE7EydO5Mknn3QnmkiF8gsNJqZLLDFdzv11DgB2O4W5BRzeU0BOVgE5e4+Ss6+QnP1FHD5YTM4hO4ezDQV5dgryDAX5hvwCGwVH/hwK/SgoDKCgyJ+C4kAK7EEccQRR5PCn0ARSZAIpIoAiAk+7igjAgT9H8ecoITg/P7z+1HURqUg3b1xfOcpIRRk7dqzL0ZScnBzi4uIsTCRynvz9CYoIpXZEKLVblON2jMEUFWHPP0rREfuxoaCYoiN2CgvsJ8YdsWMvtGMvNjiKHdiLHDjsf/5cbHDYzYlxdoO92IHDzolpf/7XYTfY7eAwYLfbsDtsgME4wJhjw5+xMObY+GOvDcbYjuU1J81rDAabc35Omnbaulxec5Z5T7wub+Zs/7z1UIizbqNUKyhdjvPeTikylOs2XDfl3bwkZL32LS3btltlJCoqCn9/f7KyslzGZ2VlERMTU+IyMTExbs0PEBwcTHDw6f+6E5FzsNmwBQUSEBRIAKCb44tIZeDWdYlBQUHEx8eTmprqHOdwOEhNTSUxMbHEZRITE13mB1iwYMEZ5xcRERHf4vbXNKNHj2bo0KF07dqV7t278+qrr5KXl8ewYcMAGDJkCPXr12fixIkA3HffffTu3ZuXX36Zq666ihkzZrBy5Ureffddz74TERERqZTcLiMpKSns3buX8ePHk5mZSefOnZk/f77zJNWdO3fi53figEuPHj344IMPePzxx3n00Udp0aIFc+bMKfU9RkRERKRq0+3gRUREpFyU9vNb97IWERERS6mMiIiIiKVURkRERMRSKiMiIiJiKZURERERsZTKiIiIiFhKZUREREQspTIiIiIillIZEREREUu5fTt4Kxy/SWxOTo7FSURERKS0jn9un+tm75WijBw+fBiAuLg4i5OIiIiIuw4fPkxERMQZp1eKZ9M4HA7++OMPatSogc1m89h6c3JyiIuLY9euXXrmzXnSvvQc7UvP0H70HO1Lz/G1fWmM4fDhw9SrV8/lIbqnqhRHRvz8/GjQoEG5rT88PNwnfikqgval52hfeob2o+doX3qOL+3Lsx0ROU4nsIqIiIilVEZERETEUj5dRoKDg5kwYQLBwcFWR6n0tC89R/vSM7QfPUf70nO0L0tWKU5gFRERkarLp4+MiIiIiPVURkRERMRSKiMiIiJiKZURERERsZRPl5HJkyfTuHFjQkJCSEhIYMWKFVZH8ipPPPEENpvNZWjdurVz+pEjRxg1ahS1a9emevXq/OUvfyErK8tlHTt37uSqq64iLCyMunXr8tBDD1FcXFzRb6XCfffdd/Tv35969ephs9mYM2eOy3RjDOPHjyc2NpbQ0FCSkpLYsmWLyzwHDhxg0KBBhIeHExkZyfDhw8nNzXWZ5+eff+biiy8mJCSEuLg4XnjhhfJ+axXqXPvxtttuO+13tG/fvi7zaD8eM3HiRLp160aNGjWoW7cu1157LZs2bXKZx1N/00uWLOGCCy4gODiY5s2bM3369PJ+exWmNPvxkksuOe338s4773SZx9f342mMj5oxY4YJCgoyU6dONb/88osZMWKEiYyMNFlZWVZH8xoTJkww7dq1M7t373YOe/fudU6/8847TVxcnElNTTUrV640F154oenRo4dzenFxsWnfvr1JSkoya9asMfPmzTNRUVFm7NixVrydCjVv3jzz2GOPmU8//dQAZvbs2S7Tn3vuORMREWHmzJljfvrpJ3PNNdeYJk2amIKCAuc8ffv2NZ06dTL/+9//zPfff2+aN29uBg4c6JyenZ1toqOjzaBBg8z69evNhx9+aEJDQ80777xTUW+z3J1rPw4dOtT07dvX5Xf0wIEDLvNoPx6TnJxspk2bZtavX2/Wrl1r+vXrZxo2bGhyc3Od83jib3rbtm0mLCzMjB492vz666/m9ddfN/7+/mb+/PkV+n7LS2n2Y+/evc2IESNcfi+zs7Od07UfT+ezZaR79+5m1KhRztd2u93Uq1fPTJw40cJU3mXChAmmU6dOJU47dOiQCQwMNLNmzXKO27BhgwFMWlqaMebYB4mfn5/JzMx0zvPWW2+Z8PBwc/To0XLN7k1O/RB1OBwmJibGvPjii85xhw4dMsHBwebDDz80xhjz66+/GsD8+OOPznm++uorY7PZzO+//26MMebNN980NWvWdNmXY8aMMa1atSrnd2SNM5WRAQMGnHEZ7ccz27NnjwHMt99+a4zx3N/0ww8/bNq1a+eyrZSUFJOcnFzeb8kSp+5HY46Vkfvuu++My2g/ns4nv6YpLCxk1apVJCUlOcf5+fmRlJREWlqahcm8z5YtW6hXrx5NmzZl0KBB7Ny5E4BVq1ZRVFTksg9bt25Nw4YNnfswLS2NDh06EB0d7ZwnOTmZnJwcfvnll4p9I14kIyODzMxMl30XERFBQkKCy76LjIyka9euznmSkpLw8/Nj+fLlznl69epFUFCQc57k5GQ2bdrEwYMHK+jdWG/JkiXUrVuXVq1aMXLkSPbv3++cpv14ZtnZ2QDUqlUL8NzfdFpamss6js9TVf/feup+PO79998nKiqK9u3bM3bsWPLz853TtB9PVykelOdp+/btw263u/wiAERHR7Nx40aLUnmfhIQEpk+fTqtWrdi9ezdPPvkkF198MevXryczM5OgoCAiIyNdlomOjiYzMxOAzMzMEvfx8Wm+6vh7L2nfnLzv6tat6zI9ICCAWrVquczTpEmT09ZxfFrNmjXLJb836du3L9dffz1NmjRh69atPProo1x55ZWkpaXh7++v/XgGDoeDv/3tb/Ts2ZP27dsDeOxv+kzz5OTkUFBQQGhoaHm8JUuUtB8BbrnlFho1akS9evX4+eefGTNmDJs2beLTTz8FtB9L4pNlRErnyiuvdP7csWNHEhISaNSoER999FGV+0OQyunmm292/tyhQwc6duxIs2bNWLJkCX369LEwmXcbNWoU69ev54cffrA6SqV2pv14xx13OH/u0KEDsbGx9OnTh61bt9KsWbOKjlkp+OTXNFFRUfj7+592lnhWVhYxMTEWpfJ+kZGRtGzZkvT0dGJiYigsLOTQoUMu85y8D2NiYkrcx8en+arj7/1sv38xMTHs2bPHZXpxcTEHDhzQ/j2Lpk2bEhUVRXp6OqD9WJK7776bL7/8ksWLF9OgQQPneE/9TZ9pnvDw8Cr1j5gz7ceSJCQkALj8Xmo/uvLJMhIUFER8fDypqanOcQ6Hg9TUVBITEy1M5t1yc3PZunUrsbGxxMfHExgY6LIPN23axM6dO537MDExkXXr1rl8GCxYsIDw8HDatm1b4fm9RZMmTYiJiXHZdzk5OSxfvtxl3x06dIhVq1Y551m0aBEOh8P5P7bExES+++47ioqKnPMsWLCAVq1aVcmvFkrjt99+Y//+/cTGxgLajyczxnD33Xcze/ZsFi1adNpXU576m05MTHRZx/F5qsr/W8+1H0uydu1aAJffS1/fj6ex+gxaq8yYMcMEBweb6dOnm19//dXccccdJjIy0uXsZl/3wAMPmCVLlpiMjAyzdOlSk5SUZKKiosyePXuMMccuA2zYsKFZtGiRWblypUlMTDSJiYnO5Y9fvnbFFVeYtWvXmvnz55s6der4xKW9hw8fNmvWrDFr1qwxgJk0aZJZs2aN2bFjhzHm2KW9kZGR5rPPPjM///yzGTBgQImX9nbp0sUsX77c/PDDD6ZFixYul6QeOnTIREdHm8GDB5v169ebGTNmmLCwsCp1SerZ9uPhw4fNgw8+aNLS0kxGRoZZuHChueCCC0yLFi3MkSNHnOvQfjxm5MiRJiIiwixZssTlktP8/HznPJ74mz5+SepDDz1kNmzYYCZPnlylLkk9135MT083Tz31lFm5cqXJyMgwn332mWnatKnp1auXcx3aj6fz2TJijDGvv/66adiwoQkKCjLdu3c3//vf/6yO5FVSUlJMbGysCQoKMvXr1zcpKSkmPT3dOb2goMDcddddpmbNmiYsLMxcd911Zvfu3S7r2L59u7nyyitNaGioiYqKMg888IApKiqq6LdS4RYvXmyA04ahQ4caY45d3jtu3DgTHR1tgoODTZ8+fcymTZtc1rF//34zcOBAU716dRMeHm6GDRtmDh8+7DLPTz/9ZC666CITHBxs6tevb5577rmKeosV4mz7MT8/31xxxRWmTp06JjAw0DRq1MiMGDHitH9QaD8eU9J+BMy0adOc83jqb3rx4sWmc+fOJigoyDRt2tRlG5Xdufbjzp07Ta9evUytWrVMcHCwad68uXnooYdc7jNijPbjqWzGGFNxx2FEREREXPnkOSMiIiLiPVRGRERExFIqIyIiImIplRERERGxlMqIiIiIWEplRERERCylMiIiIiKWUhkRERERS6mMiIiIiKVURkRERMRSKiMiIiJiKZURERERsdT/A29gNlVx6ewdAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "max_epochs = 10000 # Maximum number of epochs\n",
        "# Optimiser with step size = lr (learning rate)\n",
        "optimiser = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "# MSE loss\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "train_loss_list = list()\n",
        "val_loss_list = list()\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "  print(f\"Epoch: {epoch+1}/{max_epochs}\")\n",
        "  # Set ANN in training mode\n",
        "  net.train()\n",
        "  # Load data in minibatches\n",
        "  for batch, (x_train, y_train) in enumerate(customDataLoader_train):\n",
        "    # Forward pass\n",
        "    y_pred = net(x_train)\n",
        "    # Error measurement\n",
        "    loss_train = loss_fn(y_pred, y_train)\n",
        "    print(f\"Training: Batch {batch+1}/{len(customDataLoader_train)}, Loss: {loss_train.item()}\")\n",
        "    # Calculate gradient\n",
        "    loss_train.backward()\n",
        "    # Parameter update\n",
        "    optimiser.step()\n",
        "    # Reset the gradients\n",
        "    optimiser.zero_grad()\n",
        "  # Save training loss\n",
        "  train_loss_list.append(loss_train.item())\n",
        "\n",
        "  # Stop gradient tracking\n",
        "  with torch.no_grad():\n",
        "    # Set ANN for validation mode\n",
        "    net.eval()\n",
        "    # Load validation data\n",
        "    for batch, (x_val, y_val) in enumerate(customDataLoader_val):\n",
        "      # Forward pass\n",
        "      y_pred = net(x_val)\n",
        "      # Error measurement\n",
        "      loss_val = loss_fn(y_pred, y_val)\n",
        "      # Save validation loss\n",
        "      val_loss_list.append(loss_val.item())\n",
        "      print(f\"Validation: Batch {batch+1}/{len(customDataLoader_val)}, Loss: {loss_val.item()}\")\n",
        "    # Save best model\n",
        "    if epoch == 0:\n",
        "      min_val_loss = loss_val\n",
        "      torch.save(net, \"best_model.pth\")\n",
        "    if loss_val <= min_val_loss:\n",
        "      min_val_loss = loss_val\n",
        "      torch.save(net, \"best_model.pth\")\n",
        "    if loss_val > min_val_loss and epoch != 0:\n",
        "      print(f\"Epoch: {epoch+1}, Early stopping criteria invoked, stopping training\")\n",
        "      print(f\"Minimum validation loss: {min_val_loss}, Current loss: {loss_val}\")\n",
        "      net.load_state_dict(torch.load(\"best_model.pth\").state_dict())\n",
        "      break\n",
        "\n",
        "plt.plot(train_loss_list, \"r-\", label=\"Training loss\")\n",
        "plt.plot(val_loss_list, \"b-\", label=\"Validation loss\")\n",
        "plt.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFaOzQ6RC2jl"
      },
      "source": [
        "**Testing phase**\n",
        "\n",
        "Finally, we compare the predictions from the trained ANN and True value. We also compare the weights and bias with true values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "qvMvpPLDjzgt",
        "outputId": "08eee987-371a-4fb1-a8a3-8267a984184d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True w1: tensor([[0.8575, 0.2549],\n",
            "        [0.5864, 0.7975],\n",
            "        [0.4002, 0.0396]], device='cuda:0')\n",
            "True b1: tensor([[0.1713],\n",
            "        [0.6196],\n",
            "        [0.5936]], device='cuda:0')\n",
            "ANN w1: Parameter containing:\n",
            "tensor([[0.8575, 0.2549],\n",
            "        [0.5864, 0.7975],\n",
            "        [0.4002, 0.0396]], device='cuda:0', requires_grad=True)\n",
            "ANN b1: Parameter containing:\n",
            "tensor([0.1713, 0.6196, 0.5936], device='cuda:0', requires_grad=True)\n",
            "\n",
            " True: \n",
            " [[0.33934358 0.8629901  0.5946189 ]\n",
            " [0.7592675  0.9724872  0.7739415 ]\n",
            " [0.7263441  0.9789151  0.7539452 ]\n",
            " [0.6212493  0.82387877 0.734954  ]\n",
            " [0.71432376 0.9882895  0.74314374]\n",
            " [0.8571523  0.99461466 0.8043892 ]\n",
            " [0.6413591  0.9287359  0.7253761 ]\n",
            " [0.77907246 0.9988225  0.7617909 ]\n",
            " [0.67624545 0.98574036 0.7245797 ]\n",
            " [0.4323277  0.89868766 0.6311364 ]]\n",
            "\n",
            " Prediction: \n",
            " [[0.33934316 0.8629864  0.5946165 ]\n",
            " [0.7592665  0.972485   0.77393895]\n",
            " [0.72634315 0.97891307 0.7539426 ]\n",
            " [0.6212484  0.8238749  0.73495144]\n",
            " [0.71432287 0.9882879  0.7431411 ]\n",
            " [0.85715145 0.9946159  0.8043867 ]\n",
            " [0.64135826 0.9287328  0.7253735 ]\n",
            " [0.77907157 0.9988231  0.7617883 ]\n",
            " [0.67624456 0.9857387  0.72457707]\n",
            " [0.4323271  0.8986842  0.63113385]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nplt.plot(x_np[:, 0], y_np[:, 0], \"r*\", label=\"True x0y0\")\\nplt.plot(x_np[:, 0], y_pred[:, 0], \"bo\", label=\"Pred x0y0\")\\nplt.legend(loc=\"best\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "print(f\"True w1: {w1}\")\n",
        "print(f\"True b1: {b1}\")\n",
        "# print(f\"True w2: {w2}\")\n",
        "# print(f\"True b2: {b2}\")\n",
        "\n",
        "print(f\"ANN w1: {list(net.parameters())[0]}\")\n",
        "print(f\"ANN b1: {list(net.parameters())[1]}\")\n",
        "# print(f\"ANN w2: list(net.parameters())[2]\")\n",
        "# print(f\"ANN b2: list(net.parameters())[3]\")\n",
        "\n",
        "num_test_samples = 25\n",
        "x_test = (torch.rand(num_test_samples, sample_dim)).to(device=device).to(dtype=dtype)\n",
        "x_np = x_test.cpu().numpy()\n",
        "\n",
        "y_test = (torch.sin(torch.matmul(w1, x_test.T) + b1).T).to(device=device).to(dtype=dtype)\n",
        "y_np = y_test.cpu().numpy()\n",
        "y_pred = net(x_test)\n",
        "y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "print(f\"\\n True: \\n {y_np[:10, :3]}\")\n",
        "print(f\"\\n Prediction: \\n {y_pred[:10, :3]}\")\n",
        "\n",
        "'''\n",
        "plt.plot(x_np[:, 0], y_np[:, 0], \"r*\", label=\"True x0y0\")\n",
        "plt.plot(x_np[:, 0], y_pred[:, 0], \"bo\", label=\"Pred x0y0\")\n",
        "plt.legend(loc=\"best\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO73T7V0l0Jk"
      },
      "source": [
        "**Few tasks:**\n",
        "\n",
        "1. Use np.random.shuffle to shuffle data before splitting into training and validation\n",
        "2. Change input and output dimensions and observe the testing phase.\n",
        "3. Change data type to torch.float64 or torch.float16 and observe the training phase.\n",
        "4. Set activation function to identity and observe the predictions.\n",
        "5. How to find correct dimension of hidden layer and number of layers in real problems?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPp70stRTTNO8cAzbqwBdgS"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}